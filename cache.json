{"2024-10-18T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.14677v1","updated":"2024-10-18T17:59:57Z","published":"2024-10-18T17:59:57Z","title":"Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts","summary":"  The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld.\n","authors":["German Gritsai","Anastasia Voznyuk","Andrey Grabovoy","Yury Chekhovich"],"pdf_url":"https://arxiv.org/pdf/2410.14677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14676v1","updated":"2024-10-18T17:59:51Z","published":"2024-10-18T17:59:51Z","title":"SudoLM: Learning Access Control of Parametric Knowledge with\n  Authorization Alignment","summary":"  Existing preference alignment is a one-size-fits-all alignment mechanism,\nwhere the part of the large language model (LLM) parametric knowledge with\nnon-preferred features is uniformly blocked to all the users. However, this\npart of knowledge can be useful to advanced users whose expertise qualifies\nthem to handle these information. The one-size-fits-all alignment mechanism\nundermines LLM's utility for these qualified users. To address this problem, we\npropose SudoLM, a framework that lets LLMs learn access control over specific\nparametric knowledge for users with different credentials via authorization\nalignment. SudoLM allows authorized users to unlock their access to all the\nparametric knowledge with an assigned SUDO key while blocking access to\nnon-qualified users. Experiments on two application scenarios demonstrate that\nSudoLM effectively controls the user's access to the parametric knowledge and\nmaintains its general utility.\n","authors":["Qin Liu","Fei Wang","Chaowei Xiao","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14675v1","updated":"2024-10-18T17:59:47Z","published":"2024-10-18T17:59:47Z","title":"Enhancing Large Language Models' Situated Faithfulness to External\n  Contexts","summary":"  Large Language Models (LLMs) are often augmented with external information as\ncontexts, but this external information can sometimes be inaccurate or even\nintentionally misleading. We argue that robust LLMs should demonstrate situated\nfaithfulness, dynamically calibrating their trust in external information based\non their confidence in the internal knowledge and the external context. To\nbenchmark this capability, we evaluate LLMs across several QA datasets,\nincluding a newly created dataset called RedditQA featuring in-the-wild\nincorrect contexts sourced from Reddit posts. We show that when provided with\nboth correct and incorrect contexts, both open-source and proprietary models\ntend to overly rely on external information, regardless of its factual\naccuracy. To enhance situated faithfulness, we propose two approaches:\nSelf-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning\n(RCR). SCR enables models to self-access the confidence of external information\nrelative to their own internal knowledge to produce the most accurate answer.\nRCR, in contrast, extracts explicit confidence signals from the LLM and\ndetermines the final answer using predefined rules. Our results show that for\nLLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR\noutperforms RCR, achieving improvements of up to 24.2% over a direct input\naugmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR\noutperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct\nPreference Optimization (CR-DPO) method improves performance on both seen and\nunseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In\naddition to quantitative results, we offer insights into the relative strengths\nof SCR and RCR. Our findings highlight promising avenues for improving situated\nfaithfulness in LLMs. The data and code are released.\n","authors":["Yukun Huang","Sanxing Chen","Hongyi Cai","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2410.14675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14669v1","updated":"2024-10-18T17:58:21Z","published":"2024-10-18T17:58:21Z","title":"NaturalBench: Evaluating Vision-Language Models on Natural Adversarial\n  Samples","summary":"  Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\n$\\textbf{vision-centric}$ design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs.\n","authors":["Baiqi Li","Zhiqiu Lin","Wenxuan Peng","Jean de Dieu Nyandwi","Daniel Jiang","Zixian Ma","Simran Khanuja","Ranjay Krishna","Graham Neubig","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2410.14669v1.pdf","comment":"Accepted to NeurIPS 24; We open-source our dataset at:\n  https://huggingface.co/datasets/BaiqiL/NaturalBench; Project page at:\n  https://linzhiqiu.github.io/papers/naturalbench/"},{"id":"http://arxiv.org/abs/2410.14668v1","updated":"2024-10-18T17:57:40Z","published":"2024-10-18T17:57:40Z","title":"MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image\n  Description and Reasoning Steps","summary":"  Multimodal Chain of Thought (MCoT) is a popular prompting strategy for\nimproving the performance of multimodal large language models (MLLMs) across a\nrange of complex reasoning tasks. Despite its popularity, there is a notable\nabsence of automated methods for evaluating the quality of reasoning steps in\nMCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation\n(MiCEval), a framework designed to assess the correctness of reasoning chains\nby evaluating the quality of both the description and each reasoning step. The\nevaluation of the description component focuses on the accuracy of the image\ndescriptions, while the reasoning step evaluates the quality of each step as it\nis conditionally generated based on the preceding steps. MiCEval is built upon\na fine-grained dataset with annotations that rate each step according to\ncorrectness, relevance, and informativeness. Extensive experiments on four\nstate-of-the-art MLLMs show that step-wise evaluations using MiCEval align more\nclosely with human judgments compared to existing methods based on cosine\nsimilarity or fine-tuning approaches. MiCEval datasets and code can be found in\nhttps://github.com/alenai97/MiCEval.\n","authors":["Xiongtao Zhou","Jie He","Lanyu Chen","jingyu li","Haojing Chen","Victor Gutierrez Basulto","Jeff Z. Pan","Hanjie Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14668v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2410.14666v1","updated":"2024-10-18T17:56:11Z","published":"2024-10-18T17:56:11Z","title":"DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie\n  Character-Aware Discourse Graph","summary":"  Summarizing movie screenplays presents a unique set of challenges compared to\nstandard document summarization. Screenplays are not only lengthy, but also\nfeature a complex interplay of characters, dialogues, and scenes, with numerous\ndirect and subtle relationships and contextual nuances that are difficult for\nmachine learning models to accurately capture and comprehend. Recent attempts\nat screenplay summarization focus on fine-tuning transformer-based pre-trained\nmodels, but these models often fall short in capturing long-term dependencies\nand latent relationships, and frequently encounter the \"lost in the middle\"\nissue. To address these challenges, we introduce DiscoGraMS, a novel resource\nthat represents movie scripts as a movie character-aware discourse graph (CaD\nGraph). This approach is well-suited for various downstream tasks, such as\nsummarization, question-answering, and salience detection. The model aims to\npreserve all salient information, offering a more comprehensive and faithful\nrepresentation of the screenplay's content. We further explore a baseline\nmethod that combines the CaD Graph with the corresponding movie script through\na late fusion of graph and text modalities, and we present very initial\npromising results.\n","authors":["Maitreya Prafulla Chitale","Uday Bindal","Rajakrishnan Rajkumar","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2410.14666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06331v2","updated":"2024-10-18T17:53:46Z","published":"2024-10-08T20:12:11Z","title":"Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing","summary":"  The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper\nMLP layers, unlike single-hop tasks, which rely on earlier layers. This\ndistinction explains the poor performance of current methods in multi-hop\nqueries, as they primarily focus on editing shallow layers, leaving deeper\nlayers unchanged. To address this, we propose IFMET, a novel locate-then-edit\nKE approach designed to edit both shallow and deep MLP layers. IFMET employs\nmulti-hop editing prompts and supplementary sets to locate and modify knowledge\nacross different reasoning stages. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\neffectively overcoming the limitations of previous locate-then-edit methods.\n","authors":["Zhuoran Zhang","Yongxiang Li","Zijian Kan","Keyuan Cheng","Lijie Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06331v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.14651v1","updated":"2024-10-18T17:47:11Z","published":"2024-10-18T17:47:11Z","title":"Real-time Fake News from Adversarial Feedback","summary":"  We show that existing evaluations for fake news detection based on\nconventional sources, such as claims on fact-checking websites, result in an\nincreasing accuracy over time for LLM-based detectors -- even after their\nknowledge cutoffs. This suggests that recent popular political claims, which\nform the majority of fake news on such sources, are easily classified using\nsurface-level shallow patterns. Instead, we argue that a proper fake news\ndetection dataset should test a model's ability to reason factually about the\ncurrent world by retrieving and reading related evidence. To this end, we\ndevelop a novel pipeline that leverages natural language feedback from a\nRAG-based detector to iteratively modify real-time news into deceptive fake\nnews that challenges LLMs. Our iterative rewrite decreases the binary\nclassification AUC by an absolute 17.5 percent for a strong RAG GPT-4o\ndetector. Our experiments reveal the important role of RAG in both detecting\nand generating fake news, as retrieval-free LLM detectors are vulnerable to\nunseen events and adversarial attacks, while feedback from RAG detection helps\ndiscover more deceitful patterns in fake news.\n","authors":["Sanxing Chen","Yukun Huang","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2410.14651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14641v1","updated":"2024-10-18T17:41:19Z","published":"2024-10-18T17:41:19Z","title":"Distance between Relevant Information Pieces Causes Bias in Long-Context\n  LLMs","summary":"  Positional bias in large language models (LLMs) hinders their ability to\neffectively process long inputs. A prominent example is the \"lost in the\nmiddle\" phenomenon, where LLMs struggle to utilize relevant information\nsituated in the middle of the input. While prior research primarily focuses on\nsingle pieces of relevant information, real-world applications often involve\nmultiple relevant information pieces. To bridge this gap, we present\nLongPiBench, a benchmark designed to assess positional bias involving multiple\npieces of relevant information. Thorough experiments are conducted with five\ncommercial and six open-source models. These experiments reveal that while most\ncurrent models are robust against the \"lost in the middle\" issue, there exist\nsignificant biases related to the spacing of relevant information pieces. These\nfindings highlight the importance of evaluating and reducing positional biases\nto advance LLM's capabilities.\n","authors":["Runchu Tian","Yanghao Li","Yuepeng Fu","Siyang Deng","Qinyu Luo","Cheng Qian","Shuo Wang","Xin Cong","Zhong Zhang","Yesai Wu","Yankai Lin","Huadong Wang","Xiaojiang Liu"],"pdf_url":"https://arxiv.org/pdf/2410.14641v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2410.14635v1","updated":"2024-10-18T17:36:53Z","published":"2024-10-18T17:36:53Z","title":"GenEOL: Harnessing the Generative Power of LLMs for Training-Free\n  Sentence Embeddings","summary":"  Training-free embedding methods directly leverage pretrained large language\nmodels (LLMs) to embed text, bypassing the costly and complex procedure of\ncontrastive learning. Previous training-free embedding methods have mainly\nfocused on optimizing embedding prompts and have overlooked the benefits of\nutilizing the generative abilities of LLMs. We propose a novel method, GenEOL,\nwhich uses LLMs to generate diverse transformations of a sentence that preserve\nits meaning, and aggregates the resulting embeddings of these transformations\nto enhance the overall sentence embedding. GenEOL significantly outperforms the\nexisting training-free embedding methods by an average of 2.85 points across\nseveral LLMs on the sentence semantic text similarity (STS) benchmark. Our\nanalysis shows that GenEOL stabilizes representation quality across LLM layers\nand is robust to perturbations of embedding prompts. GenEOL also achieves\nnotable gains on multiple clustering, reranking and pair-classification tasks\nfrom the MTEB benchmark.\n","authors":["Raghuveer Thirukovalluru","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2410.14635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14632v1","updated":"2024-10-18T17:32:22Z","published":"2024-10-18T17:32:22Z","title":"Diverging Preferences: When do Annotators Disagree and do Models Know?","summary":"  We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training.\n","authors":["Michael JQ Zhang","Zhilin Wang","Jena D. Hwang","Yi Dong","Olivier Delalleau","Yejin Choi","Eunsol Choi","Xiang Ren","Valentina Pyatkin"],"pdf_url":"https://arxiv.org/pdf/2410.14632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07114v2","updated":"2024-10-18T17:30:04Z","published":"2024-09-19T19:48:31Z","title":"System 2 thinking in OpenAI's o1-preview model: Near-perfect performance\n  on a mathematics exam","summary":"  The processes underlying human cognition are often divided into System 1,\nwhich involves fast, intuitive thinking, and System 2, which involves slow,\ndeliberate reasoning. Previously, large language models were criticized for\nlacking the deeper, more analytical capabilities of System 2. In September\n2024, OpenAI introduced the o1 model series, designed to handle System 2-like\nreasoning. While OpenAI's benchmarks are promising, independent validation is\nstill needed. In this study, we tested the o1-preview model twice on the Dutch\n'Mathematics B' final exam. It scored a near-perfect 76 and 74 out of 76\npoints. For context, only 24 out of 16,414 students in the Netherlands achieved\na perfect score. By comparison, the GPT-4o model scored 66 and 62 out of 76,\nwell above the Dutch average of 40.63 points. Neither model had access to the\nexam figures. Since there was a risk of model contamination (i.e., the\nknowledge cutoff of o1-preview and GPT-4o was after the exam was published\nonline), we repeated the procedure with a new Mathematics B exam that was\npublished after the cutoff date. The results again indicated that o1-preview\nperformed strongly (97.8th percentile), which suggests that contamination was\nnot a factor. We also show that there is some variability in the output of\no1-preview, which means that sometimes there is 'luck' (the answer is correct)\nor 'bad luck' (the output has diverged into something that is incorrect). We\ndemonstrate that a self-consistency approach, where repeated prompts are given\nand the most common answer is selected, is a useful strategy for identifying\nthe correct answer. It is concluded that while OpenAI's new model series holds\ngreat potential, certain risks must be considered.\n","authors":["Joost de Winter","Dimitra Dodou","Yke Bauke Eisma"],"pdf_url":"https://arxiv.org/pdf/2410.07114v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14627v1","updated":"2024-10-18T17:29:56Z","published":"2024-10-18T17:29:56Z","title":"CELI: Controller-Embedded Language Model Interactions","summary":"  We introduce Controller-Embedded Language Model Interactions (CELI), a\nframework that integrates control logic directly within language model (LM)\nprompts, facilitating complex, multi-stage task execution. CELI addresses\nlimitations of existing prompt engineering and workflow optimization techniques\nby embedding control logic directly within the operational context of language\nmodels, enabling dynamic adaptation to evolving task requirements. Our\nframework transfers control from the traditional programming execution\nenvironment to the LMs, allowing them to autonomously manage computational\nworkflows while maintaining seamless interaction with external systems and\nfunctions. CELI supports arbitrary function calls with variable arguments,\nbridging the gap between LMs' adaptive reasoning capabilities and conventional\nsoftware paradigms' structured control mechanisms. To evaluate CELI's\nversatility and effectiveness, we conducted case studies in two distinct\ndomains: code generation (HumanEval benchmark) and multi-stage content\ngeneration (Wikipedia-style articles). The results demonstrate notable\nperformance improvements across a range of domains. CELI achieved a 4.9\npercentage point improvement over the best reported score of the baseline GPT-4\nmodel on the HumanEval code generation benchmark. In multi-stage content\ngeneration, 94.4% of CELI-produced Wikipedia-style articles met or exceeded\nfirst draft quality when optimally configured, with 44.4% achieving high\nquality. These outcomes underscore CELI's potential for optimizing AI-driven\nworkflows across diverse computational domains.\n","authors":["Jan-Samuel Wagner","Dave DeCaprio","Abishek Chiffon Muthu Raja","Jonathan M. Holman","Lauren K. Brady","Sky C. Cheung","Hosein Barzekar","Eric Yang","Mark Anthony Martinez II","David Soong","Sriram Sridhar","Han Si","Brandon W. Higgs","Hisham Hamadeh","Scott Ogden"],"pdf_url":"https://arxiv.org/pdf/2410.14627v1.pdf","comment":"26 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.14626v1","updated":"2024-10-18T17:27:38Z","published":"2024-10-18T17:27:38Z","title":"You Shall Know a Tool by the Traces it Leaves: The Predictability of\n  Sentiment Analysis Tools","summary":"  If sentiment analysis tools were valid classifiers, one would expect them to\nprovide comparable results for sentiment classification on different kinds of\ncorpora and for different languages. In line with results of previous studies\nwe show that sentiment analysis tools disagree on the same dataset. Going\nbeyond previous studies we show that the sentiment tool used for sentiment\nannotation can even be predicted from its outcome, revealing an algorithmic\nbias of sentiment analysis. Based on Twitter, Wikipedia and different news\ncorpora from the English, German and French languages, our classifiers separate\nsentiment tools with an averaged F1-score of 0.89 (for the English corpora). We\ntherefore warn against taking sentiment annotations as face value and argue for\nthe need of more and systematic NLP evaluation studies.\n","authors":["Daniel Baumartz","Mevlüt Bagci","Alexander Henlein","Maxim Konca","Andy Lücking","Alexander Mehler"],"pdf_url":"https://arxiv.org/pdf/2410.14626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10989v2","updated":"2024-10-18T17:21:17Z","published":"2024-10-14T18:17:01Z","title":"Liger Kernel: Efficient Triton Kernels for LLM Training","summary":"  Training Large Language Models (LLMs) efficiently at scale presents a\nformidable challenge, driven by their ever-increasing computational demands and\nthe need for enhanced performance. In this work, we introduce Liger-Kernel, an\nopen-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input\nchunking, our kernels achieve on average a 20% increase in training throughput\nand a 60% reduction in GPU memory usage for popular LLMs compared to\nHuggingFace implementations. In addition, Liger-Kernel is designed with\nmodularity, accessibility, and adaptability in mind, catering to both casual\nand expert users. Comprehensive benchmarks and integration tests are built in\nto ensure compatibility, performance, correctness, and convergence across\ndiverse computing environments and model architectures.\n  The source code is available under a permissive license at:\ngithub.com/linkedin/Liger-Kernel.\n","authors":["Pin-Lun Hsu","Yun Dai","Vignesh Kothapalli","Qingquan Song","Shao Tang","Siyu Zhu","Steven Shimizu","Shivam Sahni","Haowen Ning","Yanning Chen"],"pdf_url":"https://arxiv.org/pdf/2410.10989v2.pdf","comment":"17 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.02525v3","updated":"2024-10-18T17:18:24Z","published":"2024-10-03T14:33:34Z","title":"Contextual Document Embeddings","summary":"  Dense document embeddings are central to neural retrieval. The dominant\nparadigm is to train and construct embeddings by running encoders directly on\nindividual documents. In this work, we argue that these embeddings, while\neffective, are implicitly out-of-context for targeted use cases of retrieval,\nand that a contextualized document embedding should take into account both the\ndocument and neighboring documents in context - analogous to contextualized\nword embeddings. We propose two complementary methods for contextualized\ndocument embeddings: first, an alternative contrastive learning objective that\nexplicitly incorporates the document neighbors into the intra-batch contextual\nloss; second, a new contextual architecture that explicitly encodes neighbor\ndocument information into the encoded representation. Results show that both\nmethods achieve better performance than biencoders in several settings, with\ndifferences especially pronounced out-of-domain. We achieve state-of-the-art\nresults on the MTEB benchmark with no hard negative mining, score distillation,\ndataset-specific instructions, intra-GPU example-sharing, or extremely large\nbatch sizes. Our method can be applied to improve performance on any\ncontrastive learning dataset and any biencoder.\n","authors":["John X. Morris","Alexander M. Rush"],"pdf_url":"https://arxiv.org/pdf/2410.02525v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10101v2","updated":"2024-10-18T17:15:09Z","published":"2024-10-14T02:41:01Z","title":"Learning Linear Attention in Polynomial Time","summary":"  Previous research has explored the computational expressivity of Transformer\nmodels in simulating Boolean circuits or Turing machines. However, the\nlearnability of these simulators from observational data has remained an open\nquestion. Our study addresses this gap by providing the first polynomial-time\nlearnability results (specifically strong, agnostic PAC learning) for\nsingle-layer Transformers with linear attention. We show that linear attention\nmay be viewed as a linear predictor in a suitably defined RKHS. As a\nconsequence, the problem of learning any linear transformer may be converted\ninto the problem of learning an ordinary linear predictor in an expanded\nfeature space, and any such predictor may be converted back into a multiheaded\nlinear transformer. Moving to generalization, we show how to efficiently\nidentify training datasets for which every empirical risk minimizer is\nequivalent (up to trivial symmetries) to the linear Transformer that generated\nthe data, thereby guaranteeing the learned model will correctly generalize\nacross all inputs. Finally, we provide examples of computations expressible via\nlinear attention and therefore polynomial-time learnable, including associative\nmemories, finite automata, and a class of Universal Turing Machine (UTMs) with\npolynomially bounded computation histories. We empirically validate our\ntheoretical findings on three tasks: learning random linear attention networks,\nkey--value associations, and learning to execute finite automata. Our findings\nbridge a critical gap between theoretical expressivity and learnability of\nTransformers, and show that flexible and general models of computation are\nefficiently learnable.\n","authors":["Morris Yau","Ekin Akyürek","Jiayuan Mao","Joshua B. Tenenbaum","Stefanie Jegelka","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2410.10101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06402v2","updated":"2024-10-18T17:10:05Z","published":"2024-03-11T03:28:13Z","title":"One size doesn't fit all: Predicting the Number of Examples for\n  In-Context Learning","summary":"  In-context learning (ICL) refers to the process of adding a small number of\nlocalized examples (ones that are semantically similar to the input) from a\ntraining set of labelled data to an LLM's prompt with an objective to\neffectively control the generative process seeking to improve the downstream\ntask performance. Existing ICL approaches use an identical number of examples\n(a pre-configured hyper-parameter) for each data instance. Our work alleviates\nthe limitations of this 'one fits all' approach by dynamically predicting the\nnumber of examples for each data instance to be used in few-shot inference with\nLLMs. In particular, we employ a multi-label classifier, the parameters of\nwhich are fitted using a training set, where the label for each instance in the\ntraining set indicates if using a specific value of k (number of most similar\nexamples from 0 up to a maximum value) leads to correct k-shot downstream\npredictions. Our experiments on a number of text classification benchmarks show\nthat AICL substantially outperforms standard ICL by up to 17%.\n","authors":["Manish Chandra","Debasis Ganguly","Iadh Ounis"],"pdf_url":"https://arxiv.org/pdf/2403.06402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14609v1","updated":"2024-10-18T17:03:17Z","published":"2024-10-18T17:03:17Z","title":"DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual\n  Distillation in Conversational Search","summary":"  Conversational Search (CS) is the task of retrieving relevant documents from\na corpus within a conversational context, combining retrieval with\nconversational context modeling. With the explosion of Large Language Models\n(LLMs), the CS field has seen major improvements with LLMs rewriting user\nqueries, accounting for conversational context. However, engaging LLMs at\ninference time harms efficiency. Current methods address this by distilling\nembeddings from human-rewritten queries to learn the context modeling task.\nYet, these approaches predominantly focus on context modeling, and only treat\nthe contrastive component of the retrieval task within a\ndistillation-independent loss term. To address these limitations, we propose a\nnew distillation method, as a relaxation of the previous objective, unifying\nretrieval and context modeling. We relax the existing training objectives by\ndistilling similarity scores between conversations and documents, rather than\nrelying solely on representation learning. Our proposed distillation objective\nallows for more freedom in the representation space and leverages the\ncontrastive nature of document relevance. Through experiments on Learned Sparse\nRetrieval (LSR) across 5 CS datasets, our approach demonstrates substantial\nimprovements in both in-domain and out-of-domain retrieval performance,\noutperforming state-of-the-art with gains of up to 6 points in recall for\nout-of-domain datasets. Additionally, through the relaxation of the objective,\nwe propose a multi-teacher distillation, using multiple LLMs as teachers,\nyielding additional gains, and outperforming the teachers themselves in\nin-domain experiments. Finally, analysis of the sparsity of the models reveals\nthat our distillation allows for better control over the sparsity of the\ntrained models.\n","authors":["Simon Lupart","Mohammad Aliannejadi","Evangelos Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2410.14609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14596v1","updated":"2024-10-18T16:49:36Z","published":"2024-10-18T16:49:36Z","title":"Teaching Models to Balance Resisting and Accepting Persuasion","summary":"  Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Balanced Training (or PBT),\nwhich leverages multi-agent recursive dialogue trees to create data and trains\nmodels via preference optimization to accept persuasion when appropriate. PBT\nconsistently improves resistance to misinformation and resilience to being\nchallenged while also resulting in the best overall performance on holistic\ndata containing both positive and negative persuasion. Crucially, we show that\nPBT models are better teammates in multi-agent debates. We find that without\nPBT, pairs of stronger and weaker models have unstable performance, with the\norder in which the models present their answers determining whether the team\nobtains the stronger or weaker model's performance. PBT leads to better and\nmore stable results and less order dependence, with the stronger model\nconsistently pulling the weaker one up.\n","authors":["Elias Stengel-Eskin","Peter Hase","Mohit Bansal"],"pdf_url":"https://arxiv.org/pdf/2410.14596v1.pdf","comment":"Code: https://github.com/esteng/persuasion_balanced_training"},{"id":"http://arxiv.org/abs/2410.14594v1","updated":"2024-10-18T16:44:22Z","published":"2024-10-18T16:44:22Z","title":"Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and\n  Tool Knowledge Bases","summary":"  Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks\nlike secure database interactions and multi-agent code development. However,\nscaling tool capacity beyond agent reasoning or model limits remains a\nchallenge. In this paper, we address these challenges by introducing Toolshed\nKnowledge Bases, a tool knowledge base (vector database) designed to store\nenhanced tool representations and optimize tool selection for large-scale\ntool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a\nnovel ensemble of tool-applied advanced retrieval-augmented generation (RAG)\ntechniques across the pre-retrieval, intra-retrieval, and post-retrieval\nphases, without requiring model fine-tuning. During pre-retrieval, tool\ndocuments are enhanced with key information and stored in the Toolshed\nKnowledge Base. Intra-retrieval focuses on query planning and transformation to\nincrease retrieval accuracy. Post-retrieval refines the retrieved tool\ndocuments and enables self-reflection. Furthermore, by varying both the total\nnumber of tools (tool-M) an Agent has access to and the tool selection\nthreshold (top-k), we address trade-offs between retrieval accuracy, agent\nperformance, and token cost. Our approach achieves 46%, 56%, and 47% absolute\nimprovements on the ToolE single-tool, ToolE multi-tool and Seal-Tools\nbenchmark datasets, respectively (Recall@5).\n","authors":["Elias Lumer"],"pdf_url":"https://arxiv.org/pdf/2410.14594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13370v2","updated":"2024-10-18T16:44:05Z","published":"2024-04-20T13:15:27Z","title":"Movie101v2: Improved Movie Narration Benchmark","summary":"  Automatic movie narration aims to generate video-aligned plot descriptions to\nassist visually impaired audiences. Unlike standard video captioning, it\ninvolves not only describing key visual details but also inferring plots that\nunfold across multiple movie shots, presenting distinct and complex challenges.\nTo advance this field, we introduce Movie101v2, a large-scale, bilingual\ndataset with enhanced data quality specifically designed for movie narration.\nRevisiting the task, we propose breaking down the ultimate goal of automatic\nmovie narration into three progressive stages, offering a clear roadmap with\ncorresponding evaluation metrics. Based on our new benchmark, we baseline a\nrange of large vision-language models, including GPT-4V, and conduct an\nin-depth analysis of the challenges in narration generation. Our findings\nhighlight that achieving applicable movie narration generation is a fascinating\ngoal that requires significant research.\n","authors":["Zihao Yue","Yepeng Zhang","Ziheng Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2404.13370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13191v2","updated":"2024-10-18T16:42:01Z","published":"2024-10-17T03:38:29Z","title":"MCQG-SRefine: Multiple Choice Question Generation and Evaluation with\n  Iterative Self-Critique, Correction, and Comparison Feedback","summary":"  Automatic question generation (QG) is essential for AI and NLP, particularly\nin intelligent tutoring, dialogue systems, and fact verification. Generating\nmultiple-choice questions (MCQG) for professional exams, like the United States\nMedical Licensing Examination (USMLE), is particularly challenging, requiring\ndomain expertise and complex multi-hop reasoning for high-quality questions.\nHowever, current large language models (LLMs) like GPT-4 struggle with\nprofessional MCQG due to outdated knowledge, hallucination issues, and prompt\nsensitivity, resulting in unsatisfactory quality and difficulty. To address\nthese challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique\nand Correction) framework for converting medical cases into high-quality\nUSMLE-style questions. By integrating expert-driven prompt engineering with\niterative self-critique and self-correction feedback, MCQG-SRefine\nsignificantly enhances human expert satisfaction regarding both the quality and\ndifficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based\nautomatic metric to replace the complex and costly expert evaluation process,\nensuring reliable and expert-aligned assessments.\n","authors":["Zonghai Yao","Aditya Parashar","Huixue Zhou","Won Seok Jang","Feiyun Ouyang","Zhichao Yang","Hong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.13191v2.pdf","comment":"Equal contribution for the first two authors"},{"id":"http://arxiv.org/abs/2410.14589v1","updated":"2024-10-18T16:39:42Z","published":"2024-10-18T16:39:42Z","title":"Dialetto, ma Quanto Dialetto? Transcribing and Evaluating Dialects on a\n  Continuum","summary":"  There is increasing interest in looking at dialects in NLP. However, most\nwork to date still treats dialects as discrete categories. For instance,\nevaluative work in variation-oriented NLP for English often works with Indian\nEnglish or African-American Venacular English as homogeneous categories (Faisal\net al., 2024; Ziems et al., 2023), yet even within one variety there is\nsubstantial variation. We examine within-dialect variation and show that\nperformance critically varies within categories. We measure speech-to-text\nperformance on Italian dialects, and empirically observe a geographical\nperformance disparity. This disparity correlates substantially (-0.5) with\nlinguistic similarity to the highest performing dialect variety. We\ncross-examine our results against dialectometry methods, and interpret the\nperformance disparity to be due to a bias towards dialects that are more\nsimilar to the standard variety in the speech-to-text model examined. We\nadditionally leverage geostatistical methods to predict zero-shot performance\nat unseen sites, and find the incorporation of geographical information to\nsubstantially improve prediction performance, indicating there to be\ngeographical structure in the performance distribution.\n","authors":["Ryan Soh-Eun Shim","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2410.14589v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14582v1","updated":"2024-10-18T16:32:10Z","published":"2024-10-18T16:32:10Z","title":"Do LLMs estimate uncertainty well in instruction-following?","summary":"  Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.\n","authors":["Juyeon Heo","Miao Xiong","Christina Heinze-Deml","Jaya Narain"],"pdf_url":"https://arxiv.org/pdf/2410.14582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14581v1","updated":"2024-10-18T16:32:06Z","published":"2024-10-18T16:32:06Z","title":"Optimizing Attention with Mirror Descent: Generalized Max-Margin Token\n  Selection","summary":"  Attention mechanisms have revolutionized several domains of artificial\nintelligence, such as natural language processing and computer vision, by\nenabling models to selectively focus on relevant parts of the input data. While\nrecent work has characterized the optimization dynamics of gradient descent\n(GD) in attention-based models and the structural properties of its preferred\nsolutions, less is known about more general optimization algorithms such as\nmirror descent (MD). In this paper, we investigate the convergence properties\nand implicit biases of a family of MD algorithms tailored for softmax attention\nmechanisms, with the potential function chosen as the $p$-th power of the\n$\\ell_p$-norm. Specifically, we show that these algorithms converge in\ndirection to a generalized hard-margin SVM with an $\\ell_p$-norm objective when\napplied to a classification problem using a softmax attention model. Notably,\nour theoretical results reveal that the convergence rate is comparable to that\nof traditional GD in simpler models, despite the highly nonlinear and nonconvex\nnature of the present problem. Additionally, we delve into the joint\noptimization dynamics of the key-query matrix and the decoder, establishing\nconditions under which this complex joint optimization converges to their\nrespective hard-margin SVM solutions. Lastly, our numerical experiments on real\ndata demonstrate that MD algorithms improve generalization over standard GD and\nexcel in optimal token selection.\n","authors":["Aaron Alvarado Kristanto Julistiono","Davoud Ataee Tarzanagh","Navid Azizan"],"pdf_url":"https://arxiv.org/pdf/2410.14581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14578v1","updated":"2024-10-18T16:26:45Z","published":"2024-10-18T16:26:45Z","title":"Large Language Models Are Overparameterized Text Encoders","summary":"  Large language models (LLMs) demonstrate strong performance as text embedding\nmodels when finetuned with supervised contrastive training. However, their\nlarge size balloons inference time and memory requirements. In this paper, we\nshow that by pruning the last $p\\%$ layers of an LLM before supervised training\nfor only 1000 steps, we can achieve a proportional reduction in memory and\ninference time. We evaluate four different state-of-the-art LLMs on text\nembedding tasks and find that our method can prune up to 30\\% of layers with\nnegligible impact on performance and up to 80\\% with only a modest drop. With\nonly three lines of code, our method is easily implemented in any pipeline for\ntransforming LLMs to text encoders. We also propose $\\text{L}^3 \\text{Prune}$,\na novel layer-pruning strategy based on the model's initial loss that provides\ntwo optimal pruning configurations: a large variant with negligible performance\nloss and a small variant for resource-constrained settings. On average, the\nlarge variant prunes 21\\% of the parameters with a $-0.3$ performance drop, and\nthe small variant only suffers from a $-5.1$ decrease while pruning 74\\% of the\nmodel. We consider these results strong evidence that LLMs are\noverparameterized for text embedding tasks, and can be easily pruned.\n","authors":["Thennal D K","Tim Fischer","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2410.14578v1.pdf","comment":"8 pages of content + 1 for limitations and ethical considerations, 14\n  pages in total including references and appendix, 5+1 figures"},{"id":"http://arxiv.org/abs/2410.14574v1","updated":"2024-10-18T16:20:22Z","published":"2024-10-18T16:20:22Z","title":"MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts","summary":"  Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled\nscalability in deep learning. SMoE has the potential to exponentially increase\nparameter count while maintaining the efficiency of the model by only\nactivating a small subset of these parameters for a given sample. However, it\nhas been observed that SMoE suffers from unstable training and has difficulty\nadapting to new distributions, leading to the model's lack of robustness to\ndata contamination. To overcome these limitations, we first establish a\nconnection between the dynamics of the expert representations in SMoEs and\ngradient descent on a multi-objective optimization problem. Leveraging our\nframework, we then integrate momentum into SMoE and propose a new family of\nSMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate\nthat MomentumSMoE is more stable and robust than SMoE. In particular, we verify\nthe advantages of MomentumSMoE over SMoE on a variety of practical tasks\nincluding ImageNet-1K object recognition and WikiText-103 language modeling. We\ndemonstrate the applicability of MomentumSMoE to many types of SMoE models,\nincluding those in the Sparse MoE model for vision (V-MoE) and the Generalist\nLanguage Model (GLaM). We also show that other advanced momentum-based\noptimization methods, such as Adam, can be easily incorporated into the\nMomentumSMoE framework for designing new SMoE models with even better\nperformance, almost negligible additional computation cost, and simple\nimplementations.\n","authors":["Rachel S. Y. Teo","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.14574v1.pdf","comment":"10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https://github.com/rachtsy/MomentumSMoE"},{"id":"http://arxiv.org/abs/2410.14567v1","updated":"2024-10-18T16:11:29Z","published":"2024-10-18T16:11:29Z","title":"RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions","summary":"  Conversational AI agents use Retrieval Augmented Generation (RAG) to provide\nverifiable document-grounded responses to user inquiries. However, many natural\nquestions do not have good answers: about 25\\% contain false\nassumptions~\\cite{Yu2023:CREPE}, and over 50\\% are\nambiguous~\\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve\ntheir responses to confusing questions. This paper presents a novel synthetic\ndata generation method to efficiently create a diverse set of context-grounded\nconfusing questions from a given document corpus. We conduct an empirical\ncomparative evaluation of several large language models as RAG agents to\nmeasure the accuracy of confusion detection and appropriate response\ngeneration. We contribute a benchmark dataset to the public domain.\n","authors":["Zhiyuan Peng","Jinming Nian","Alexandre Evfimievski","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2410.14567v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2410.07400v2","updated":"2024-10-18T15:54:56Z","published":"2024-10-09T19:57:07Z","title":"Advocating Character Error Rate for Multilingual ASR Evaluation","summary":"  Automatic speech recognition (ASR) systems have traditionally been evaluated\nusing English datasets, with the word error rate (WER) serving as the\npredominant metric. WER's simplicity and ease of interpretation have\ncontributed to its widespread adoption, particularly for English. However, as\nASR systems expand to multilingual contexts, WER fails in various ways,\nparticularly with morphologically complex languages or those without clear word\nboundaries. Our work documents the limitations of WER as an evaluation metric\nand advocates for the character error rate (CER) as the primary metric in\nmultilingual ASR evaluation. We show that CER avoids many of the challenges WER\nfaces and exhibits greater consistency across writing systems. We support our\nproposition by conducting human evaluations of ASR transcriptions in three\nlanguages: Malayalam, English, and Arabic, which exhibit distinct morphological\ncharacteristics. We show that CER correlates more closely with human judgments\nthan WER, even for English. To facilitate further research, we release our\nhuman evaluation dataset for future benchmarking of ASR metrics. Our findings\nsuggest that CER should be prioritized, or at least supplemented, in\nmultilingual ASR evaluations to account for the varying linguistic\ncharacteristics of different languages.\n","authors":["Thennal D K","Jesin James","Deepa P Gopinath","Muhammed Ashraf K"],"pdf_url":"https://arxiv.org/pdf/2410.07400v2.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2409.15652v3","updated":"2024-10-18T15:45:39Z","published":"2024-09-24T01:29:24Z","title":"English offensive text detection using CNN based Bi-GRU model","summary":"  Over the years, the number of users of social media has increased\ndrastically. People frequently share their thoughts through social platforms,\nand this leads to an increase in hate content. In this virtual community,\nindividuals share their views, express their feelings, and post photos, videos,\nblogs, and more. Social networking sites like Facebook and Twitter provide\nplatforms to share vast amounts of content with a single click. However, these\nplatforms do not impose restrictions on the uploaded content, which may include\nabusive language and explicit images unsuitable for social media. To resolve\nthis issue, a new idea must be implemented to divide the inappropriate content.\nNumerous studies have been done to automate the process. In this paper, we\npropose a new Bi-GRU-CNN model to classify whether the text is offensive or\nnot. The combination of the Bi-GRU and CNN models outperforms the existing\nmodel.\n","authors":["Tonmoy Roy","Md Robiul Islam","Asif Ahammad Miazee","Anika Antara","Al Amin","Sunjim Hossain"],"pdf_url":"https://arxiv.org/pdf/2409.15652v3.pdf","comment":"5 pages and 6 figures"},{"id":"http://arxiv.org/abs/2405.20850v2","updated":"2024-10-18T15:43:02Z","published":"2024-05-31T14:33:07Z","title":"Improving Reward Models with Synthetic Critiques","summary":"  Reward models (RMs) play a critical role in aligning language models through\nthe process of reinforcement learning from human feedback. RMs are trained to\npredict a score reflecting human preference, which requires significant time\nand cost for human annotation. Additionally, RMs tend to quickly overfit on\nsuperficial features in the training set, hindering their generalization\nperformance on unseen distributions. We propose a novel approach using\nsynthetic natural language critiques generated by large language models to\nprovide additional feedback, evaluating aspects such as instruction following,\ncorrectness, and style. This offers richer signals and more robust features for\nRMs to assess and score on. We demonstrate that high-quality critiques improve\nthe performance and data efficiency of RMs initialized from different\npretrained models, reducing the reliance on costly human annotations.\nFurthermore, incorporating critiques improves both the interpretability and\nrobustness of RM training.\n","authors":["Zihuiwen Ye","Fraser Greenlee-Scott","Max Bartolo","Phil Blunsom","Jon Ander Campos","Matthias Gallé"],"pdf_url":"https://arxiv.org/pdf/2405.20850v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14917v2","updated":"2024-10-18T15:42:06Z","published":"2024-09-23T11:13:25Z","title":"With Ears to See and Eyes to Hear: Sound Symbolism Experiments with\n  Multimodal Large Language Models","summary":"  Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have\ndemonstrated aptitude as potential substitutes for human participants in\nexperiments testing psycholinguistic phenomena. However, an understudied\nquestion is to what extent models that only have access to vision and text\nmodalities are able to implicitly understand sound-based phenomena via abstract\nreasoning from orthography and imagery alone. To investigate this, we analyse\nthe ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise\na non-arbitrary link between sounds and concepts) as well as their ability to\n\"hear\" via the interplay of the language and vision modules of open and\nclosed-source multimodal models. We perform multiple experiments, including\nreplicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism\ntasks, and comparing human judgements of linguistic iconicity with that of\nLLMs. Our results show that VLMs demonstrate varying levels of agreement with\nhuman labels, and more task information may be required for VLMs versus their\nhuman counterparts for in silico experimentation. We additionally see through\nhigher maximum agreement levels that Magnitude Symbolism is an easier pattern\nfor VLMs to identify than Shape Symbolism, and that an understanding of\nlinguistic iconicity is highly dependent on model size.\n","authors":["Tyler Loakman","Yucheng Li","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2409.14917v2.pdf","comment":"Accepted to EMNLP 2024 (Camera Ready)"},{"id":"http://arxiv.org/abs/2410.14545v1","updated":"2024-10-18T15:40:48Z","published":"2024-10-18T15:40:48Z","title":"Tell me what I need to know: Exploring LLM-based (Personalized)\n  Abstractive Multi-Source Meeting Summarization","summary":"  Meeting summarization is crucial in digital communication, but existing\nsolutions struggle with salience identification to generate personalized,\nworkable summaries, and context understanding to fully comprehend the meetings'\ncontent. Previous attempts to address these issues by considering related\nsupplementary resources (e.g., presentation slides) alongside transcripts are\nhindered by models' limited context sizes and handling the additional\ncomplexities of the multi-source tasks, such as identifying relevant\ninformation in additional files and seamlessly aligning it with the meeting\ncontent. This work explores multi-source meeting summarization considering\nsupplementary materials through a three-stage large language model approach:\nidentifying transcript passages needing additional context, inferring relevant\ndetails from supplementary materials and inserting them into the transcript,\nand generating a summary from this enriched transcript. Our multi-source\napproach enhances model understanding, increasing summary relevance by ~9% and\nproducing more content-rich outputs. We introduce a personalization protocol\nthat extracts participant characteristics and tailors summaries accordingly,\nimproving informativeness by ~10%. This work further provides insights on\nperformance-cost trade-offs across four leading model families, including\nedge-device capable options. Our approach can be extended to similar complex\ngenerative tasks benefitting from additional resources and personalization,\nsuch as dialogue systems and action planning.\n","authors":["Frederic Kirstein","Terry Ruas","Robert Kratel","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2410.14545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02067v2","updated":"2024-10-18T15:39:15Z","published":"2024-07-02T08:55:41Z","title":"Crossroads of Continents: Automated Artifact Extraction for Cultural\n  Adaptation with Large Multimodal Models","summary":"  We present a comprehensive three-phase study to examine (1) the cultural\nunderstanding of Large Multimodal Models (LMMs) by introducing DalleStreet, a\nlarge-scale dataset generated by DALL-E 3 and validated by humans, containing\n9,935 images of 67 countries and 10 concept classes; (2) the underlying\nimplicit and potentially stereotypical cultural associations with a cultural\nartifact extraction task; and (3) an approach to adapt cultural representation\nin an image based on extracted associations using a modular pipeline,\nCultureAdapt. We find disparities in cultural understanding at geographic\nsub-region levels with both open-source (LLaVA) and closed-source (GPT-4V)\nmodels on DalleStreet and other existing benchmarks, which we try to understand\nusing over 18,000 artifacts that we identify in association to different\ncountries. Our findings reveal a nuanced picture of the cultural competence of\nLMMs, highlighting the need to develop culture-aware systems. Dataset and code\nare available at https://github.com/iamshnoo/crossroads\n","authors":["Anjishnu Mukherjee","Ziwei Zhu","Antonios Anastasopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.02067v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2404.11124v2","updated":"2024-10-18T15:34:41Z","published":"2024-04-17T07:15:07Z","title":"What's under the hood: Investigating Automatic Metrics on Meeting\n  Summarization","summary":"  Meeting summarization has become a critical task considering the increase in\nonline interactions. While new techniques are introduced regularly, their\nevaluation uses metrics not designed to capture meeting-specific errors,\nundermining effective evaluation. This paper investigates what the frequently\nused automatic metrics capture and which errors they mask by correlating\nautomatic metric scores with human evaluations across a broad error taxonomy.\nWe commence with a comprehensive literature review on English meeting\nsummarization to define key challenges like speaker dynamics and contextual\nturn-taking and error types such as missing information and linguistic\ninaccuracy, concepts previously loosely defined in the field. We examine the\nrelationship between characteristic challenges and errors by using annotated\ntranscripts and summaries from Transformer-based sequence-to-sequence and\nautoregressive models from the general summary QMSum dataset. Through\nexperimental validation, we find that different model architectures respond\nvariably to challenges in meeting transcripts, resulting in different\npronounced links between challenges and errors. Current default-used metrics\nstruggle to capture observable errors, showing weak to mid-correlations, while\na third of the correlations show trends of error masking. Only a subset reacts\naccurately to specific errors, while most correlations show either\nunresponsiveness or failure to reflect the error's impact on summary quality.\n","authors":["Frederic Kirstein","Jan Philip Wahle","Terry Ruas","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2404.11124v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12874v2","updated":"2024-10-18T15:26:55Z","published":"2024-10-14T18:11:53Z","title":"On Debiasing Text Embeddings Through Context Injection","summary":"  Current advances in Natural Language Processing (NLP) have made it\nincreasingly feasible to build applications leveraging textual data. Generally,\nthe core of these applications rely on having a good semantic representation of\ntext into vectors, via embedding models. However, it has been shown that these\nembeddings capture and perpetuate biases already present in text. While a few\ntechniques have been proposed to debias embeddings, they do not take advantage\nof the recent advances in context understanding of modern embedding models. In\nthis paper, we fill this gap by conducting a review of 19 embedding models by\nquantifying their biases and how well they respond to context injection as a\nmean of debiasing. We show that higher performing models are more prone to\ncapturing biases, but are also better at incorporating context. Surprisingly,\nwe find that while models can easily embed affirmative semantics, they fail at\nembedding neutral semantics. Finally, in a retrieval task, we show that biases\nin embeddings can lead to non-desirable outcomes. We use our new-found insights\nto design a simple algorithm for top $k$ retrieval, where $k$ is dynamically\nselected. We show that our algorithm is able to retrieve all relevant gendered\nand neutral chunks.\n","authors":["Thomas Uriot"],"pdf_url":"https://arxiv.org/pdf/2410.12874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13901v3","updated":"2024-10-18T15:25:44Z","published":"2024-03-20T18:13:17Z","title":"Train & Constrain: Phonologically Informed Tongue-Twister Generation\n  from Topics and Paraphrases","summary":"  Previous work in phonologically and phonetically grounded language generation\nhas mainly focused on domains such as puns and poetry. In this article, we\npresent new work on the generation of English tongue twisters - a form of\nlanguage that is required to be conditioned on a phoneme level to maximize\nsound overlap, while maintaining semantic consistency with an input topic or\nphrase and still being grammatically correct. We present TwisterLister, a\npipeline for generating phonologically informed tongue twisters from large\nlanguage models (LLMs) that we use to generate TwistList 2.0, the largest\nannotated dataset of tongue twisters to date, consisting of 17K+ examples from\na combination of human and LLM authors. Our generation pipeline involves the\nuse of a phonologically constrained vocabulary alongside LLM prompting to\ngenerate novel, non-derivative tongue twister examples. We additionally present\nthe results of automatic and human evaluation of smaller models trained on our\ngenerated dataset to demonstrate the extent to which phonologically motivated\nlanguage types can be generated without explicit injection of phonological\nknowledge. Additionally, we introduce a phoneme-aware constrained decoding\nmodule (PACD) that can be integrated into an autoregressive language model and\ndemonstrate that this method generates good quality tongue twisters both with\nand without fine-tuning the underlying language model. We also design and\nimplement a range of automatic metrics for the task of tongue twister\ngeneration that is phonologically motivated and captures the unique essence of\ntongue twisters, primarily based on phonemic edit distance (PED)\n","authors":["Tyler Loakman","Chen Tang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2403.13901v3.pdf","comment":"Accepted Final Version to Computational Linguistics"},{"id":"http://arxiv.org/abs/2406.11580v2","updated":"2024-10-18T15:20:43Z","published":"2024-06-17T14:20:47Z","title":"Error Span Annotation: A Balanced Approach for Human Evaluation of\n  Machine Translation","summary":"  High-quality Machine Translation (MT) evaluation relies heavily on human\njudgments. Comprehensive error classification methods, such as Multidimensional\nQuality Metrics (MQM), are expensive as they are time-consuming and can only be\ndone by experts, whose availability may be limited especially for low-resource\nlanguages. On the other hand, just assigning overall scores, like Direct\nAssessment (DA), is simpler and faster and can be done by translators of any\nlevel, but is less reliable. In this paper, we introduce Error Span Annotation\n(ESA), a human evaluation protocol which combines the continuous rating of DA\nwith the high-level error severity span marking of MQM. We validate ESA by\ncomparing it to MQM and DA for 12 MT systems and one human reference\ntranslation (English to German) from WMT23. The results show that ESA offers\nfaster and cheaper annotations than MQM at the same quality level, without the\nrequirement of expensive MQM experts.\n","authors":["Tom Kocmi","Vilém Zouhar","Eleftherios Avramidis","Roman Grundkiewicz","Marzena Karpinska","Maja Popović","Mrinmaya Sachan","Mariya Shmatova"],"pdf_url":"https://arxiv.org/pdf/2406.11580v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14516v1","updated":"2024-10-18T14:55:14Z","published":"2024-10-18T14:55:14Z","title":"Do LLMs \"know\" internally when they follow instructions?","summary":"  Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents.\n","authors":["Juyeon Heo","Christina Heinze-Deml","Oussama Elachqar","Shirley Ren","Udhay Nallasamy","Andy Miller","Kwan Ho Ryan Chan","Jaya Narain"],"pdf_url":"https://arxiv.org/pdf/2410.14516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14506v1","updated":"2024-10-18T14:38:37Z","published":"2024-10-18T14:38:37Z","title":"SignAttention: On the Interpretability of Transformer Models for Sign\n  Language Translation","summary":"  This paper presents the first comprehensive interpretability analysis of a\nTransformer-based Sign Language Translation (SLT) model, focusing on the\ntranslation from video-based Greek Sign Language to glosses and text.\nLeveraging the Greek Sign Language Dataset, we examine the attention mechanisms\nwithin the model to understand how it processes and aligns visual input with\nsequential glosses. Our analysis reveals that the model pays attention to\nclusters of frames rather than individual ones, with a diagonal alignment\npattern emerging between poses and glosses, which becomes less distinct as the\nnumber of glosses increases. We also explore the relative contributions of\ncross-attention and self-attention at each decoding step, finding that the\nmodel initially relies on video frames but shifts its focus to previously\npredicted tokens as the translation progresses. This work contributes to a\ndeeper understanding of SLT models, paving the way for the development of more\ntransparent and reliable translation systems essential for real-world\napplications.\n","authors":["Pedro Alejandro Dal Bianco","Oscar Agustín Stanchi","Facundo Manuel Quiroga","Franco Ronchetti","Enzo Ferrante"],"pdf_url":"https://arxiv.org/pdf/2410.14506v1.pdf","comment":"Accepted at IAI Workshop @ NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.14480v1","updated":"2024-10-18T14:03:52Z","published":"2024-10-18T14:03:52Z","title":"Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of\n  Language Models","summary":"  As large language models (LLMs) continue to advance, the need for precise and\nefficient evaluation metrics becomes more pressing. Traditional approaches,\nwhile informative, often face limitations in computational demands and\ninterpretability. In this paper, we introduce a novel hybrid evaluation method\nthat integrates two established techniques: entropy derived from covariance\nmatrices and the Matrix Nuclear Norm (MNN). Our method begins by normalizing\nhidden states from LLMs, then computes the covariance matrix and MNN from these\nrepresentations. We further calculate the entropy of the covariance matrix to\ncapture uncertainty and redundancy in the model's outputs. By combining these\nmetrics into a composite score, we offer a comprehensive evaluation framework\nthat balances accuracy with computational efficiency. Additionally, our\napproach allows for flexibility in adjusting the weightings between entropy and\nMNN, tailoring the evaluation for different objectives. Through a series of\nexperiments on various LLMs, we demonstrate the robustness and efficacy of our\nmethod, offering deeper insights into model performance. This work contributes\nto the ongoing development of LLM evaluation and opens avenues for future\ninnovations in model assessment techniques.\n","authors":["James Vo"],"pdf_url":"https://arxiv.org/pdf/2410.14480v1.pdf","comment":"The method is currently under experimentation"},{"id":"http://arxiv.org/abs/2410.09804v2","updated":"2024-10-18T14:03:05Z","published":"2024-10-13T11:15:38Z","title":"BlackDAN: A Black-Box Multi-Objective Approach for Effective and\n  Contextual Jailbreaking of Large Language Models","summary":"  While large language models (LLMs) exhibit remarkable capabilities across\nvarious tasks, they encounter potential security risks such as jailbreak\nattacks, which exploit vulnerabilities to bypass security measures and generate\nharmful outputs. Existing jailbreak strategies mainly focus on maximizing\nattack success rate (ASR), frequently neglecting other critical factors,\nincluding the relevance of the jailbreak response to the query and the level of\nstealthiness. This narrow focus on single objectives can result in ineffective\nattacks that either lack contextual relevance or are easily recognizable. In\nthis work, we introduce BlackDAN, an innovative black-box attack framework with\nmulti-objective optimization, aiming to generate high-quality prompts that\neffectively facilitate jailbreaking while maintaining contextual relevance and\nminimizing detectability. BlackDAN leverages Multiobjective Evolutionary\nAlgorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks\nacross multiple objectives including ASR, stealthiness, and semantic relevance.\nBy integrating mechanisms like mutation, crossover, and Pareto-dominance,\nBlackDAN provides a transparent and interpretable process for generating\njailbreaks. Furthermore, the framework allows customization based on user\npreferences, enabling the selection of prompts that balance harmfulness,\nrelevance, and other factors. Experimental results demonstrate that BlackDAN\noutperforms traditional single-objective methods, yielding higher success rates\nand improved robustness across various LLMs and multimodal LLMs, while ensuring\njailbreak responses are both relevant and less detectable.\n","authors":["Xinyuan Wang","Victor Shea-Jay Huang","Renmiao Chen","Hao Wang","Chengwei Pan","Lei Sha","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.09804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14346v2","updated":"2024-10-18T13:59:54Z","published":"2024-07-19T14:28:53Z","title":"Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals","summary":"  Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.\n","authors":["Akash Kumar Mohankumar","Gururaj K","Gagan Madan","Amit Singh"],"pdf_url":"https://arxiv.org/pdf/2407.14346v2.pdf","comment":"Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure"},{"id":"http://arxiv.org/abs/2406.13663v4","updated":"2024-10-18T13:16:57Z","published":"2024-06-19T16:10:26Z","title":"Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation","summary":"  Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.\n","authors":["Jirui Qi","Gabriele Sarti","Raquel Fernández","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2406.13663v4.pdf","comment":"Accepted by EMNLP 2024 Main Conference. Code and data released at\n  https://github.com/Betswish/MIRAGE"},{"id":"http://arxiv.org/abs/2405.11877v5","updated":"2024-10-18T13:03:05Z","published":"2024-05-20T08:41:15Z","title":"A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:\n  The First Romanian Natural Language Inference Corpus","summary":"  Natural language inference (NLI), the task of recognizing the entailment\nrelationship in sentence pairs, is an actively studied topic serving as a proxy\nfor natural language understanding. Despite the relevance of the task in\nbuilding conversational agents and improving text classification, machine\ntranslation and other NLP tasks, to the best of our knowledge, there is no\npublicly available NLI corpus for the Romanian language. To this end, we\nintroduce the first Romanian NLI corpus (RoNLI) comprising 58K training\nsentence pairs, which are obtained via distant supervision, and 6K validation\nand test sentence pairs, which are manually annotated with the correct labels.\nWe conduct experiments with multiple machine learning methods based on distant\nlearning, ranging from shallow models based on word embeddings to\ntransformer-based neural networks, to establish a set of competitive baselines.\nFurthermore, we improve on the best model by employing a new curriculum\nlearning strategy based on data cartography. Our dataset and code to reproduce\nthe baselines are available at https://github.com/Eduard6421/RONLI.\n","authors":["Eduard Poesina","Cornelia Caragea","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2405.11877v5.pdf","comment":"Accepted at ACL 2024 (Main)"},{"id":"http://arxiv.org/abs/2410.14442v1","updated":"2024-10-18T13:01:14Z","published":"2024-10-18T13:01:14Z","title":"A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference","summary":"  Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.\n","authors":["You Wu","Haoyi Wu","Kewei Tu"],"pdf_url":"https://arxiv.org/pdf/2410.14442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10996v2","updated":"2024-10-18T12:54:21Z","published":"2024-06-16T16:17:46Z","title":"Towards Lifelong Dialogue Agents via Relation-aware Memory Construction\n  and Timeline-augmented Response Generation","summary":"  To achieve lifelong human-agent interaction, dialogue agents need to\nconstantly memorize perceived information and properly retrieve it for response\ngeneration (RG). While prior work focuses on getting rid of outdated memories\nto improve retrieval quality, we argue that such memories provide rich,\nimportant contextual cues for RG (e.g., changes in user behaviors) in long-term\nconversations. We present Theanine, a framework for LLM-based lifelong dialogue\nagents. Theanine discards memory removal and manages large-scale memories by\nlinking them based on their temporal and cause-effect relation. Enabled by this\nlinking structure, Theanine augments RG with memory timelines - series of\nmemories representing the evolution or causality of relevant past events. Along\nwith Theanine, we introduce TeaFarm, a counterfactual-driven evaluation scheme,\naddressing the limitation of G-Eval and human efforts in measuring\nmemory-augmented dialogue agents. A supplementary video for Theanine and data\nfor TeaFarm are at https://huggingface.co/spaces/ResearcherScholar/Theanine.\n","authors":["Kai Tzu-iunn Ong","Namyoung Kim","Minju Gwak","Hyungjoo Chae","Taeyoon Kwon","Yohan Jo","Seung-won Hwang","Dongha Lee","Jinyoung Yeo"],"pdf_url":"https://arxiv.org/pdf/2406.10996v2.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2305.17819v3","updated":"2024-10-18T12:49:35Z","published":"2023-05-28T22:46:21Z","title":"Large Language Models, scientific knowledge and factuality: A framework\n  to streamline human expert evaluation","summary":"  The paper introduces a framework for the evaluation of the encoding of\nfactual scientific knowledge, designed to streamline the manual evaluation\nprocess typically conducted by domain experts. Inferring over and extracting\ninformation from Large Language Models (LLMs) trained on a large corpus of\nscientific literature can potentially define a step change in biomedical\ndiscovery, reducing the barriers for accessing and integrating existing medical\nevidence. This work explores the potential of LLMs for dialoguing with\nbiomedical background knowledge, using the context of antibiotic discovery. The\nframework involves of three evaluation steps, each assessing different aspects\nsequentially: fluency, prompt alignment, semantic coherence, factual knowledge,\nand specificity of the generated responses. By splitting these tasks between\nnon-experts and experts, the framework reduces the effort required from the\nlatter. The work provides a systematic assessment on the ability of eleven\nstate-of-the-art models LLMs, including ChatGPT, GPT-4 and Llama 2, in two\nprompting-based tasks: chemical compound definition generation and chemical\ncompound-fungus relation determination. Although recent models have improved in\nfluency, factual accuracy is still low and models are biased towards\nover-represented entities. The ability of LLMs to serve as biomedical knowledge\nbases is questioned, and the need for additional systematic evaluation\nframeworks is highlighted. While LLMs are currently not fit for purpose to be\nused as biomedical factual knowledge bases in a zero-shot setting, there is a\npromising emerging property in the direction of factuality as the models become\ndomain specialised, scale-up in size and level of human feedback.\n","authors":["Magdalena Wysocka","Oskar Wysocki","Maxime Delmas","Vincent Mutel","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2305.17819v3.pdf","comment":"Accepted at the Journal of Biomedical Informatics, Volume 158,\n  October 2024, 104724"},{"id":"http://arxiv.org/abs/2410.14425v1","updated":"2024-10-18T12:39:32Z","published":"2024-10-18T12:39:32Z","title":"Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge\n  Distillation","summary":"  Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct experiments on text classification tasks involving\nthree state-of-the-art language models and three different backdoor attack\nalgorithms. Our empirical results demonstrate the outstanding performance of\nW2SDefense in defending against backdoor attacks without compromising model\nperformance.\n","authors":["Shuai Zhao","Xiaobao Wu","Cong-Duy Nguyen","Meihuizi Jia","Yichao Feng","Luu Anh Tuan"],"pdf_url":"https://arxiv.org/pdf/2410.14425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10918v5","updated":"2024-10-18T12:27:07Z","published":"2024-06-16T12:46:40Z","title":"Multi-LLM QA with Embodied Exploration","summary":"  Large language models (LLMs) have grown in popularity due to their natural\nlanguage interface and pre trained knowledge, leading to rapidly increasing\nsuccess in question-answering (QA) tasks. More recently, multi-agent systems\nwith LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.\nIn these scenarios, the models may each answer the question and reach a\nconsensus or each model is specialized to answer different domain questions.\nHowever, most prior work dealing with Multi-LLM QA has focused on scenarios\nwhere the models are asked in a zero-shot manner or are given information\nsources to extract the answer. For question answering of an unknown\nenvironment, embodied exploration of the environment is first needed to answer\nthe question. This skill is necessary for personalizing embodied AI to\nenvironments such as households. There is a lack of insight into whether a\nMulti-LLM system can handle question-answering based on observations from\nembodied exploration. In this work, we address this gap by investigating the\nuse of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.\nMultiple LLM-based agents independently explore and then answer queries about a\nhousehold environment. We analyze different aggregation methods to generate a\nsingle, final answer for each query: debating, majority voting, and training a\ncentral answer module (CAM). Using CAM, we observe a $46\\%$ higher accuracy\ncompared against the other non-learning-based aggregation methods. We provide\ncode and the query dataset for further research.\n","authors":["Bhrij Patel","Vishnu Sashank Dorbala","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2406.10918v5.pdf","comment":"16 pages, 9 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2406.12950v2","updated":"2024-10-18T12:19:41Z","published":"2024-06-18T12:54:47Z","title":"MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular\n  Property Prediction","summary":"  Molecular property prediction (MPP) is a fundamental and crucial task in drug\ndiscovery. However, prior methods are limited by the requirement for a large\nnumber of labeled molecules and their restricted ability to generalize for\nunseen and new tasks, both of which are essential for real-world applications.\nTo address these challenges, we present MolecularGPT for few-shot MPP. From a\nperspective on instruction tuning, we fine-tune large language models (LLMs)\nbased on curated molecular instructions spanning over 1000 property prediction\ntasks. This enables building a versatile and specialized LLM that can be\nadapted to novel MPP tasks without any fine-tuning through zero- and few-shot\nin-context learning (ICL). MolecularGPT exhibits competitive in-context\nreasoning capabilities across 10 downstream evaluation datasets, setting new\nbenchmarks for few-shot molecular prediction tasks. More importantly, with just\ntwo-shot examples, MolecularGPT can outperform standard supervised graph neural\nnetwork methods on 4 out of 7 datasets. It also excels state-of-the-art LLM\nbaselines by up to 15.7% increase on classification accuracy and decrease of\n17.9 on regression metrics (e.g., RMSE) under zero-shot. This study\ndemonstrates the potential of LLMs as effective few-shot molecular property\npredictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.\n","authors":["Yuyan Liu","Sirui Ding","Sheng Zhou","Wenqi Fan","Qiaoyu Tan"],"pdf_url":"https://arxiv.org/pdf/2406.12950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14405v1","updated":"2024-10-18T12:08:07Z","published":"2024-10-18T12:08:07Z","title":"Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of\n  Language Models for Fact Completion","summary":"  Previous interpretations of language models (LMs) miss important distinctions\nin how these models process factual information. For example, given the query\n\"Astrid Lindgren was born in\" with the corresponding completion \"Sweden\", no\ndifference is made between whether the prediction was based on having the exact\nknowledge of the birthplace of the Swedish author or assuming that a person\nwith a Swedish-sounding name was born in Sweden. In this paper, we investigate\nfour different prediction scenarios for which the LM can be expected to show\ndistinct behaviors. These scenarios correspond to different levels of model\nreliability and types of information being processed - some being less\ndesirable for factual predictions. To facilitate precise interpretations of LMs\nfor fact completion, we propose a model-specific recipe called PrISM for\nconstructing datasets with examples of each scenario based on a set of\ndiagnostic criteria. We apply a popular interpretability method, causal tracing\n(CT), to the four prediction scenarios and find that while CT produces\ndifferent results for each scenario, aggregations over a set of mixed examples\nmay only represent the results from the scenario with the strongest measured\nsignal. In summary, we contribute tools for a more granular study of fact\ncompletion in language models and analyses that provide a more nuanced\nunderstanding of how LMs process fact-related queries.\n","authors":["Denitsa Saynova","Lovisa Hagström","Moa Johansson","Richard Johansson","Marco Kuhlmann"],"pdf_url":"https://arxiv.org/pdf/2410.14405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14399v1","updated":"2024-10-18T12:02:41Z","published":"2024-10-18T12:02:41Z","title":"SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic\n  Reasoning","summary":"  Syllogistic reasoning is crucial for Natural Language Inference (NLI). This\ncapability is particularly significant in specialized domains such as\nbiomedicine, where it can support automatic evidence interpretation and\nscientific discovery. This paper presents SylloBio-NLI, a novel framework that\nleverages external ontologies to systematically instantiate diverse syllogistic\narguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language\nModels (LLMs) on identifying valid conclusions and extracting supporting\nevidence across 28 syllogistic schemes instantiated with human genome pathways.\nExtensive experiments reveal that biomedical syllogistic reasoning is\nparticularly challenging for zero-shot LLMs, which achieve an average accuracy\nbetween 70% on generalized modus ponens and 23% on disjunctive syllogism. At\nthe same time, we found that few-shot prompting can boost the performance of\ndifferent LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper\nanalysis shows that both techniques exhibit high sensitivity to superficial\nlexical variations, highlighting a dependency between reliability, models'\narchitecture, and pre-training regime. Overall, our results indicate that,\nwhile in-context examples have the potential to elicit syllogistic reasoning in\nLLMs, existing models are still far from achieving the robustness and\nconsistency required for safe biomedical NLI applications.\n","authors":["Magdalena Wysocka","Danilo S. Carvalho","Oskar Wysocki","Marco Valentino","Andre Freitas"],"pdf_url":"https://arxiv.org/pdf/2410.14399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14395v1","updated":"2024-10-18T11:58:03Z","published":"2024-10-18T11:58:03Z","title":"Generative AI, Pragmatics, and Authenticity in Second Language Learning","summary":"  There are obvious benefits to integrating generative AI (artificial\nintelligence) into language learning and teaching. Those include using AI as a\nlanguage tutor, creating learning materials, or assessing learner output.\nHowever, due to how AI systems under-stand human language, based on a\nmathematical model using statistical probability, they lack the lived\nexperience to be able to use language with the same social aware-ness as\nhumans. Additionally, there are built-in linguistic and cultural biases based\non their training data which is mostly in English and predominantly from\nWestern sources. Those facts limit AI suitability for some language learning\ninteractions. Stud-ies have clearly shown that systems such as ChatGPT often do\nnot produce language that is pragmatically appropriate. The lack of linguistic\nand cultural authenticity has important implications for how AI is integrated\ninto second language acquisition as well as in instruction targeting\ndevelopment of intercultural communication compe-tence.\n","authors":["Robert Godwin-Jones`"],"pdf_url":"https://arxiv.org/pdf/2410.14395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14391v1","updated":"2024-10-18T11:52:10Z","published":"2024-10-18T11:52:10Z","title":"Analyzing Context Utilization of LLMs in Document-Level Translation","summary":"  Large language models (LLM) are increasingly strong contenders in machine\ntranslation. We study document-level translation, where some words cannot be\ntranslated without context from outside the sentence. We investigate the\nability of prominent LLMs to utilize context by analyzing models' robustness to\nperturbed and randomized document context. We find that LLMs' improved\ndocument-translation performance is not always reflected in pronoun translation\nperformance. We highlight the need for context-aware finetuning of LLMs with a\nfocus on relevant parts of the context to improve their reliability for\ndocument-level translation.\n","authors":["Wafaa Mohammed","Vlad Niculae"],"pdf_url":"https://arxiv.org/pdf/2410.14391v1.pdf","comment":"4 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.14387v1","updated":"2024-10-18T11:39:34Z","published":"2024-10-18T11:39:34Z","title":"How Do Multilingual Models Remember? Investigating Multilingual Factual\n  Recall Mechanisms","summary":"  Large Language Models (LLMs) store and retrieve vast amounts of factual\nknowledge acquired during pre-training. Prior research has localized and\nidentified mechanisms behind knowledge recall; however, it has primarily\nfocused on English monolingual models. The question of how these processes\ngeneralize to other languages and multilingual LLMs remains unexplored. In this\npaper, we address this gap by conducting a comprehensive analysis of two highly\nmultilingual LLMs. We assess the extent to which previously identified\ncomponents and mechanisms of factual recall in English apply to a multilingual\ncontext. Then, we examine when language plays a role in the recall process,\nuncovering evidence of language-independent and language-dependent mechanisms.\n","authors":["Constanza Fierro","Negar Foroutan","Desmond Elliott","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2410.14387v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14375v1","updated":"2024-10-18T11:06:23Z","published":"2024-10-18T11:06:23Z","title":"Fine-Tuning Pre-trained Language Models for Robust Causal Representation\n  Learning","summary":"  The fine-tuning of pre-trained language models (PLMs) has been shown to be\neffective across various domains. By using domain-specific supervised data, the\ngeneral-purpose representation derived from PLMs can be transformed into a\ndomain-specific representation. However, these methods often fail to generalize\nto out-of-domain (OOD) data due to their reliance on non-causal\nrepresentations, often described as spurious features. Existing methods either\nmake use of adjustments with strong assumptions about lack of hidden common\ncauses, or mitigate the effect of spurious features using multi-domain data. In\nthis work, we investigate how fine-tuned pre-trained language models aid\ngeneralizability from single-domain scenarios under mild assumptions, targeting\nmore general and practical real-world scenarios. We show that a robust\nrepresentation can be derived through a so-called causal front-door adjustment,\nbased on a decomposition assumption, using fine-tuned representations as a\nsource of data augmentation. Comprehensive experiments in both synthetic and\nreal-world settings demonstrate the superior generalizability of the proposed\nmethod compared to existing approaches. Our work thus sheds light on the domain\ngeneralization problem by introducing links between fine-tuning and causal\nmechanisms into representation learning.\n","authors":["Jialin Yu","Yuxiang Zhou","Yulan He","Nevin L. Zhang","Ricardo Silva"],"pdf_url":"https://arxiv.org/pdf/2410.14375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10203v3","updated":"2024-10-18T10:43:40Z","published":"2024-06-14T17:38:21Z","title":"A Fundamental Trade-off in Aligned Language Models and its Relation to\n  Sampling Adaptors","summary":"  The relationship between the quality of a string, as judged by a human\nreader, and its probability, $p(\\boldsymbol{y})$ under a language model\nundergirds the development of better language models. For example, many popular\nalgorithms for sampling from a language model have been conceived with the goal\nof manipulating $p(\\boldsymbol{y})$ to place higher probability on strings that\nhumans deem of high quality. In this article, we examine the\nprobability--quality relationship in language models explicitly aligned to\nhuman preferences, e.g., through reinforcement learning through human feedback.\nWe show that, when sampling corpora from an aligned language model, there\nexists a trade-off between the strings' average reward and average\nlog-likelihood under the prior language model, i.e., the same model before\nalignment with human preferences. We provide a formal treatment of this\nphenomenon and demonstrate how a choice of sampling adaptor allows for a\nselection of how much likelihood we exchange for the reward.\n","authors":["Naaman Tan","Josef Valvoda","Tianyu Liu","Anej Svete","Yanxia Qin","Kan Min-Yen","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2406.10203v3.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.14361v1","updated":"2024-10-18T10:40:47Z","published":"2024-10-18T10:40:47Z","title":"Efficiently Computing Susceptibility to Context in Language Models","summary":"  One strength of modern language models is their ability to incorporate\ninformation from a user-input context when answering queries. However, they are\nnot equally sensitive to the subtle changes to that context. To quantify this,\nDu et al. (2024) gives an information-theoretic metric to measure such\nsensitivity. Their metric, susceptibility, is defined as the degree to which\ncontexts can influence a model's response to a query at a distributional level.\nHowever, exactly computing susceptibility is difficult and, thus, Du et al.\n(2024) falls back on a Monte Carlo approximation. Due to the large number of\nsamples required, the Monte Carlo approximation is inefficient in practice. As\na faster alternative, we propose Fisher susceptibility, an efficient method to\nestimate the susceptibility based on Fisher information. Empirically, we\nvalidate that Fisher susceptibility is comparable to Monte Carlo estimated\nsusceptibility across a diverse set of query domains despite its being\n$70\\times$ faster. Exploiting the improved efficiency, we apply Fisher\nsusceptibility to analyze factors affecting the susceptibility of language\nmodels. We observe that larger models are as susceptible as smaller ones.\n","authors":["Tianyu Liu","Kevin Du","Mrinmaya Sachan","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2410.14361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11244v4","updated":"2024-10-18T10:21:31Z","published":"2023-10-17T13:12:32Z","title":"Entity Matching using Large Language Models","summary":"  Entity matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity. Entity matching is a central step in most data\nintegration pipelines. Many state-of-the-art entity matching methods rely on\npre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks\nof these models for entity matching are that (i) the models require significant\namounts of task-specific training data and (ii) the fine-tuned models are not\nrobust concerning out-of-distribution entities. This paper investigates using\ngenerative large language models (LLMs) as a less task-specific training\ndata-dependent and more robust alternative to PLM-based matchers. The study\ncovers hosted and open-source LLMs which can be run locally. We evaluate these\nmodels in a zero-shot scenario and a scenario where task-specific training data\nis available. We compare different prompt designs and the prompt sensitivity of\nthe models. We show that there is no single best prompt but that the prompt\nneeds to be tuned for each model/dataset combination. We further investigate\n(i) the selection of in-context demonstrations, (ii) the generation of matching\nrules, as well as (iii) fine-tuning LLMs using the same pool of training data.\nOur experiments show that the best LLMs require no or only a few training\nexamples to perform comparably to PLMs that were fine-tuned using thousands of\nexamples. LLM-based matchers further exhibit higher robustness to unseen\nentities. We show that GPT4 can generate structured explanations for matching\ndecisions and can automatically identify potential causes of matching errors by\nanalyzing explanations of wrong decisions. We demonstrate that the model can\ngenerate meaningful textual descriptions of the identified error classes, which\ncan help data engineers to improve entity matching pipelines.\n","authors":["Ralph Peeters","Aaron Steiner","Christian Bizer"],"pdf_url":"https://arxiv.org/pdf/2310.11244v4.pdf","comment":"Published in Proceedings of the 28th International Conference on\n  Extending Database Technology (EDBT), 25th March-28th March, 2025, ISBN\n  978-3-89318-098-1 on OpenProceedings.org"},{"id":"http://arxiv.org/abs/2409.19700v3","updated":"2024-10-18T10:15:29Z","published":"2024-09-29T13:16:37Z","title":"2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models","summary":"  Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines.\n","authors":["Jia-Nan Li","Jian Guan","Wei Wu","Zhengtao Yu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2409.19700v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10859v2","updated":"2024-10-18T10:02:03Z","published":"2024-10-07T13:46:06Z","title":"FAME: Towards Factual Multi-Task Model Editing","summary":"  Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.\n","authors":["Li Zeng","Yingyu Shan","Zeming Liu","Jiashu Yao","Yuhang Guo"],"pdf_url":"https://arxiv.org/pdf/2410.10859v2.pdf","comment":"9 pages, 3 figures. This paper has been accepted by EMNLP 2024"},{"id":"http://arxiv.org/abs/2302.08102v2","updated":"2024-10-18T09:58:45Z","published":"2023-02-16T06:01:31Z","title":"Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech\n  Recognition","summary":"  Visual Speech Recognition (VSR) aims to infer speech into text depending on\nlip movements alone. As it focuses on visual information to model the speech,\nits performance is inherently sensitive to personal lip appearances and\nmovements, and this makes the VSR models show degraded performance when they\nare applied to unseen speakers. In this paper, to remedy the performance\ndegradation of the VSR model on unseen speakers, we propose prompt tuning\nmethods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically,\nmotivated by recent advances in Natural Language Processing (NLP), we finetune\nprompts on adaptation data of target speakers instead of modifying the\npre-trained model parameters. Different from the previous prompt tuning methods\nmainly limited to Transformer variant architecture, we explore different types\nof prompts, the addition, the padding, and the concatenation form prompts that\ncan be applied to the VSR model which is composed of CNN and Transformer in\ngeneral. With the proposed prompt tuning, we show that the performance of the\npre-trained VSR model on unseen speakers can be largely improved by using a\nsmall amount of adaptation data (e.g., less than 5 minutes), even if the\npre-trained model is already developed with large speaker variations. Moreover,\nby analyzing the performance and parameters of different types of prompts, we\ninvestigate when the prompt tuning is preferred over the finetuning methods.\nThe effectiveness of the proposed method is evaluated on both word- and\nsentence-level VSR databases, LRW-ID and GRID.\n","authors":["Minsu Kim","Hyung-Il Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2302.08102v2.pdf","comment":"IEEE TPAMI"},{"id":"http://arxiv.org/abs/2410.13281v2","updated":"2024-10-18T09:50:41Z","published":"2024-10-17T07:15:15Z","title":"BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated\n  Bangla","summary":"  The proliferation of transliterated texts in digital spaces has emphasized\nthe need for detecting and classifying hate speech in languages beyond English,\nparticularly in low-resource languages. As online discourse can perpetuate\ndiscrimination based on target groups, e.g. gender, religion, and origin,\nmulti-label classification of hateful content can help in comprehending hate\nmotivation and enhance content moderation. While previous efforts have focused\non monolingual or binary hate classification tasks, no work has yet addressed\nthe challenge of multi-label hate speech classification in transliterated\nBangla. We introduce BanTH, the first multi-label transliterated Bangla hate\nspeech dataset comprising 37.3k samples. The samples are sourced from YouTube\ncomments, where each instance is labeled with one or more target groups,\nreflecting the regional demographic. We establish novel transformer\nencoder-based baselines by further pre-training on transliterated Bangla\ncorpus. We also propose a novel translation-based LLM prompting strategy for\ntransliterated text. Experiments reveal that our further pre-trained encoders\nare achieving state-of-the-art performance on the BanTH dataset, while our\ntranslation-based prompting outperforms other strategies in the zero-shot\nsetting. The introduction of BanTH not only fills a critical gap in hate speech\nresearch for Bangla but also sets the stage for future exploration into\ncode-mixed and multi-label classification challenges in underrepresented\nlanguages.\n","authors":["Fabiha Haider","Fariha Tanjim Shifat","Md Farhan Ishmam","Deeparghya Dutta Barua","Md Sakib Ul Rahman Sourove","Md Fahim","Md Farhad Alam"],"pdf_url":"https://arxiv.org/pdf/2410.13281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14335v1","updated":"2024-10-18T09:46:38Z","published":"2024-10-18T09:46:38Z","title":"Critical Questions Generation: Motivation and Challenges","summary":"  The development of Large Language Models (LLMs) has brought impressive\nperformances on mitigation strategies against misinformation, such as\ncounterargument generation. However, LLMs are still seriously hindered by\noutdated knowledge and by their tendency to generate hallucinated content. In\norder to circumvent these issues, we propose a new task, namely, Critical\nQuestions Generation, consisting of processing an argumentative text to\ngenerate the critical questions (CQs) raised by it. In argumentation theory CQs\nare tools designed to lay bare the blind spots of an argument by pointing at\nthe information it could be missing. Thus, instead of trying to deploy LLMs to\nproduce knowledgeable and relevant counterarguments, we use them to question\narguments, without requiring any external knowledge. Research on CQs Generation\nusing LLMs requires a reference dataset for large scale experimentation. Thus,\nin this work we investigate two complementary methods to create such a\nresource: (i) instantiating CQs templates as defined by Walton's argumentation\ntheory and (ii), using LLMs as CQs generators. By doing so, we contribute with\na procedure to establish what is a valid CQ and conclude that, while LLMs are\nreasonable CQ generators, they still have a wide margin for improvement in this\ntask.\n","authors":["Blanca Calvo Figueras","Rodrigo Agerri"],"pdf_url":"https://arxiv.org/pdf/2410.14335v1.pdf","comment":"14 pages, 3 figures, 7 tables, to be published in the 28th Conference\n  on Computational Natural Language Learning (CoNLL 2024)"},{"id":"http://arxiv.org/abs/2410.11677v2","updated":"2024-10-18T09:41:53Z","published":"2024-10-15T15:14:22Z","title":"Understanding Likelihood Over-optimisation in Direct Alignment\n  Algorithms","summary":"  Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation\n(DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives\nto online Reinforcement Learning from Human Feedback (RLHF) algorithms such as\nProximal Policy Optimisation (PPO) for aligning language models to human\npreferences, without the need for explicit reward modelling. These methods\ngenerally aim to increase the likelihood of generating better (preferred)\ncompletions while discouraging worse (non-preferred) ones, while staying close\nto the original model's behaviour. In this work, we explore the relationship\nbetween completion likelihood and model performance in state-of-the-art DAAs,\nand identify a critical issue of likelihood over-optimisation. Contrary to\nexpectations, we find that higher likelihood of better completions and larger\nmargins between better and worse completion likelihoods do not necessarily lead\nto better performance, and may even degrade it. Our analysis reveals that while\nhigher likelihood correlates with better memorisation of factual knowledge\npatterns, a slightly lower completion likelihood tends to improve output\ndiversity, thus leading to better generalisation to unseen scenarios. Moreover,\nwe identify two key indicators that signal when over-optimised output diversity\nbegins to harm performance: Decreasing Entropy over Top-k Tokens and\nDiminishing Top-k Probability Mass. Our experimental results validate that\nthese indicators are reliable signs of declining performance under different\nregularisations, helping prevent over-optimisation and improve alignment with\nhuman preferences.\n","authors":["Zhengyan Shi","Sander Land","Acyr Locatelli","Matthieu Geist","Max Bartolo"],"pdf_url":"https://arxiv.org/pdf/2410.11677v2.pdf","comment":"Preprint Version"},{"id":"http://arxiv.org/abs/2403.05902v2","updated":"2024-10-18T09:39:14Z","published":"2024-03-09T12:46:53Z","title":"MaiBaam Annotation Guidelines","summary":"  This document provides the annotation guidelines for MaiBaam, a Bavarian\ncorpus manually annotated with part-of-speech (POS) tags, syntactic\ndependencies, and German lemmas. MaiBaam belongs to the Universal Dependencies\n(UD) project, and our annotations elaborate on the general and German UD\nversion 2 guidelines. In this document, we detail how to preprocess and\ntokenize Bavarian data, provide an overview of the POS tags and dependencies we\nuse, explain annotation decisions that would also apply to closely related\nlanguages like German, and lastly we introduce and motivate decisions that are\nspecific to Bavarian grammar.\n","authors":["Verena Blaschke","Barbara Kovačić","Siyao Peng","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2403.05902v2.pdf","comment":"Updated for UD v2.15 (German lemmas added)"},{"id":"http://arxiv.org/abs/2410.10291v2","updated":"2024-10-18T09:26:46Z","published":"2024-10-14T08:45:35Z","title":"Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal\n  Perspective","summary":"  Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench .\n","authors":["Xiangru Zhu","Penglei Sun","Yaoxian Song","Yanghua Xiao","Zhixu Li","Chengyu Wang","Jun Huang","Bei Yang","Xiaoxiao Xu"],"pdf_url":"https://arxiv.org/pdf/2410.10291v2.pdf","comment":"The only change in the current version update is the replacement of\n  the template with a more precise one"},{"id":"http://arxiv.org/abs/2410.14309v1","updated":"2024-10-18T09:15:35Z","published":"2024-10-18T09:15:35Z","title":"LoGU: Long-form Generation with Uncertainty Expressions","summary":"  While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses.\n","authors":["Ruihan Yang","Caiqi Zhang","Zhisong Zhang","Xinting Huang","Sen Yang","Nigel Collier","Dong Yu","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2410.14309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05672v2","updated":"2024-10-18T09:12:45Z","published":"2023-06-09T05:04:13Z","title":"I run as fast as a rabbit, can you? A Multilingual Simile Dialogue\n  Dataset","summary":"  A simile is a figure of speech that compares two different things (called the\ntenor and the vehicle) via shared properties. The tenor and the vehicle are\nusually connected with comparator words such as \"like\" or \"as\". The simile\nphenomena are unique and complex in a real-life dialogue scene where the tenor\nand the vehicle can be verbal phrases or sentences, mentioned by different\nspeakers, exist in different sentences, or occur in reversed order. However,\nthe current simile research usually focuses on similes in a triplet tuple\n(tenor, property, vehicle) or a single sentence where the tenor and vehicle are\nusually entities or noun phrases, which could not reflect complex simile\nphenomena in real scenarios. In this paper, we propose a novel and high-quality\nmultilingual simile dialogue (MSD) dataset to facilitate the study of complex\nsimile phenomena. The MSD is the largest manually annotated simile data\n($\\sim$20K) and it contains both English and Chinese data. Meanwhile, the MSD\ndata can also be used on dialogue tasks to test the ability of dialogue systems\nwhen using similes. We design 3 simile tasks (recognition, interpretation, and\ngeneration) and 2 dialogue tasks (retrieval and generation) with MSD. For each\ntask, we provide experimental results from strong pre-trained or\nstate-of-the-art models. The experiments demonstrate the challenge of MSD and\nwe have released the data/code on GitHub.\n","authors":["Longxuan Ma","Weinan Zhang","Shuhan Zhou","Churui Sun","Changxin Ke","Ting Liu"],"pdf_url":"https://arxiv.org/pdf/2306.05672v2.pdf","comment":"13 Pages, 1 Figure, 12 Tables, ACL 2023 findings"},{"id":"http://arxiv.org/abs/2406.13632v3","updated":"2024-10-18T09:07:53Z","published":"2024-06-19T15:28:29Z","title":"Can Few-shot Work in Long-Context? Recycling the Context to Generate\n  Demonstrations","summary":"  Despite recent advancements in Large Language Models (LLMs), their\nperformance on tasks involving long contexts remains sub-optimal. In-Context\nLearning (ICL) with few-shot examples may be an appealing solution to enhance\nLLM performance in this scenario; However, na\\\"ively adding ICL examples with\nlong context introduces challenges, including substantial token overhead added\nfor each few-shot example and context mismatch between the demonstrations and\nthe target query. In this work, we propose to automatically generate few-shot\nexamples for long context QA tasks by recycling contexts. Specifically, given a\nlong input context (1-3k tokens) and a query, we generate additional\nquery-output pairs from the given context as few-shot examples, while\nintroducing the context only once. This ensures that the demonstrations are\nleveraging the same context as the target query while only adding a small\nnumber of tokens to the prompt. We further enhance each demonstration by\ninstructing the model to explicitly identify the relevant paragraphs before the\nanswer, which improves performance while providing fine-grained attribution to\nthe answer source. We apply our method on multiple LLMs and obtain substantial\nimprovements (+16 absolute points on average across models) on various QA\ndatasets with long context, especially when the answer lies within the middle\nof the context. Surprisingly, despite introducing only single-hop ICL examples,\nLLMs also successfully generalize to multi-hop long-context QA using our\napproach.\n","authors":["Arie Cattan","Alon Jacovi","Alex Fabrikant","Jonathan Herzig","Roee Aharoni","Hannah Rashkin","Dror Marcus","Avinatan Hassidim","Yossi Matias","Idan Szpektor","Avi Caciularu"],"pdf_url":"https://arxiv.org/pdf/2406.13632v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09075v3","updated":"2024-10-18T09:02:46Z","published":"2023-12-14T16:10:56Z","title":"Towards Verifiable Text Generation with Evolving Memory and\n  Self-Reflection","summary":"  Despite the remarkable ability of large language models (LLMs) in language\ncomprehension and generation, they often suffer from producing factually\nincorrect information, also known as hallucination. A promising solution to\nthis issue is verifiable text generation, which prompts LLMs to generate\ncontent with citations for accuracy verification. However, verifiable text\ngeneration is non-trivial due to the focus-shifting phenomenon, the intricate\nreasoning needed to align the claim with correct citations, and the dilemma\nbetween the precision and breadth of retrieved documents. In this paper, we\npresent VTG, an innovative framework for Verifiable Text Generation with\nevolving memory and self-reflection. VTG introduces evolving long short-term\nmemory to retain both valuable documents and recent documents. A two-tier\nverifier equipped with an evidence finder is proposed to rethink and reflect on\nthe relationship between the claim and citations. Furthermore, active retrieval\nand diverse query generation are utilized to enhance both the precision and\nbreadth of the retrieved documents. We conduct extensive experiments on five\ndatasets across three knowledge-intensive tasks and the results reveal that VTG\nsignificantly outperforms baselines.\n","authors":["Hao Sun","Hengyi Cai","Bo Wang","Yingyan Hou","Xiaochi Wei","Shuaiqiang Wang","Yan Zhang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2312.09075v3.pdf","comment":"EMNLP 2024 Main Conference"},{"id":"http://arxiv.org/abs/2410.13824v2","updated":"2024-10-18T09:01:01Z","published":"2024-10-17T17:48:54Z","title":"Harnessing Webpage UIs for Text-Rich Visual Understanding","summary":"  Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.\n","authors":["Junpeng Liu","Tianyue Ou","Yifan Song","Yuxiao Qu","Wai Lam","Chenyan Xiong","Wenhu Chen","Graham Neubig","Xiang Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12013v2","updated":"2024-10-18T08:57:30Z","published":"2024-06-26T12:33:34Z","title":"Dating ancient manuscripts using radiocarbon and AI-based writing style\n  analysis","summary":"  Determining the chronology of ancient handwritten manuscripts is essential\nfor reconstructing the evolution of ideas. For the Dead Sea Scrolls, this is\nparticularly important. However, there is an almost complete lack of\ndate-bearing manuscripts evenly distributed across the timeline and written in\nsimilar scripts available for palaeographic comparison. Here, we present Enoch,\na state-of-the-art AI-based date-prediction model, trained on the basis of new\nradiocarbon-dated samples of the scrolls. Enoch uses established\nhandwriting-style descriptors and applies Bayesian ridge regression. The\nchallenge of this study is that the number of radiocarbon-dated manuscripts is\nsmall, while current machine learning requires an abundance of training data.\nWe show that by using combined angular and allographic writing style feature\nvectors and applying Bayesian ridge regression, Enoch could predict the\nradiocarbon-based dates from style, supported by leave-one-out validation, with\nvaried MAEs of 27.9 to 30.7 years relative to the radiocarbon dating. Enoch was\nthen used to estimate the dates of 135 unseen manuscripts, revealing that 79\nper cent of the samples were considered 'realistic' upon palaeographic post-hoc\nevaluation. We present a new chronology of the scrolls. The radiocarbon ranges\nand Enoch's style-based predictions are often older than the traditionally\nassumed palaeographic estimates. In the range of 300-50 BCE, Enoch's date\nprediction provides an improved granularity. The study is in line with current\ndevelopments in multimodal machine-learning techniques, and the methods can be\nused for date prediction in other partially-dated manuscript collections. This\nresearch shows how Enoch's quantitative, probability-based approach can be a\ntool for palaeographers and historians, re-dating ancient Jewish key texts and\ncontributing to current debates on Jewish and Christian origins.\n","authors":["Mladen Popović","Maruf A. Dhali","Lambert Schomaker","Johannes van der Plicht","Kaare Lund Rasmussen","Jacopo La Nasa","Ilaria Degano","Maria Perla Colombini","Eibert Tigchelaar"],"pdf_url":"https://arxiv.org/pdf/2407.12013v2.pdf","comment":"16 pages of main article, 103 pages of supplementary materials; the\n  first version of this article is originally prepared in July 2023 after the\n  completion of all the experiments"},{"id":"http://arxiv.org/abs/2310.14626v2","updated":"2024-10-18T08:56:18Z","published":"2023-10-23T07:00:51Z","title":"Conversational Recommender System and Large Language Model Are Made for\n  Each Other in E-commerce Pre-sales Dialogue","summary":"  E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases.\n","authors":["Yuanxing Liu","Wei-Nan Zhang","Yifan Chen","Yuchi Zhang","Haopeng Bai","Fan Feng","Hengbin Cui","Yongbin Li","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2310.14626v2.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2405.20680v4","updated":"2024-10-18T08:54:37Z","published":"2024-05-31T08:22:49Z","title":"Unraveling and Mitigating Retriever Inconsistencies in\n  Retrieval-Augmented Large Language Models","summary":"  Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their\nsuperiority in terms of factuality, they do not consistently outperform the\noriginal retrieval-free Language Models (LMs). Our experiments reveal that this\nexample-level performance inconsistency exists not only between\nretrieval-augmented and retrieval-free LM but also among different retrievers.\nTo understand this phenomenon, we investigate the degeneration behavior of\nRALMs and theoretically decompose it into four categories. Further analysis\nbased on our decomposition reveals that the innate difference in knowledge\nsources and the unpredictable degeneration of the reader model contribute most\nto the inconsistency. Drawing from our analysis, we introduce Ensemble of\nRetrievers (EoR), a trainable framework that can adaptively retrieve from\ndifferent knowledge sources and effectively decrease unpredictable reader\nerrors. Our experiments on Open Domain Question Answering show that EoR\nsubstantially improves performance over the RALM with a single retriever by\nconsiderably reducing inconsistent behaviors.\n","authors":["Mingda Li","Xinyu Li","Yifan Chen","Wenfeng Xuan","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.20680v4.pdf","comment":"ACL 2024 (findings)"},{"id":"http://arxiv.org/abs/2406.15053v2","updated":"2024-10-18T08:51:55Z","published":"2024-06-21T11:00:38Z","title":"PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement\n  on Multilingual and Multi-Cultural Data","summary":"  Evaluation of multilingual Large Language Models (LLMs) is challenging due to\na variety of factors -- the lack of benchmarks with sufficient linguistic\ndiversity, contamination of popular benchmarks into LLM pre-training data and\nthe lack of local, cultural nuances in translated benchmarks. In this work, we\nstudy human and LLM-based evaluation in a multilingual, multi-cultural setting.\nWe evaluate 30 models across 10 Indic languages by conducting 90K human\nevaluations and 30K LLM-based evaluations and find that models such as GPT-4o\nand Llama-3 70B consistently perform best for most Indic languages. We build\nleaderboards for two evaluation settings - pairwise comparison and direct\nassessment and analyze the agreement between humans and LLMs. We find that\nhumans and LLMs agree fairly well in the pairwise setting but the agreement\ndrops for direct assessment evaluation especially for languages such as Bengali\nand Odia. We also check for various biases in human and LLM-based evaluation\nand find evidence of self-bias in the GPT-based evaluator. Our work presents a\nsignificant step towards scaling up multilingual evaluation of LLMs.\n","authors":["Ishaan Watts","Varun Gumma","Aditya Yadavalli","Vivek Seshadri","Manohar Swaminathan","Sunayana Sitaram"],"pdf_url":"https://arxiv.org/pdf/2406.15053v2.pdf","comment":"Accepted to EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.14289v1","updated":"2024-10-18T08:49:24Z","published":"2024-10-18T08:49:24Z","title":"SwaQuAD-24: QA Benchmark Dataset in Swahili","summary":"  This paper proposes the creation of a Swahili Question Answering (QA)\nbenchmark dataset, aimed at addressing the underrepresentation of Swahili in\nnatural language processing (NLP). Drawing from established benchmarks like\nSQuAD, GLUE, KenSwQuAD, and KLUE, the dataset will focus on providing\nhigh-quality, annotated question-answer pairs that capture the linguistic\ndiversity and complexity of Swahili. The dataset is designed to support a\nvariety of applications, including machine translation, information retrieval,\nand social services like healthcare chatbots. Ethical considerations, such as\ndata privacy, bias mitigation, and inclusivity, are central to the dataset\ndevelopment. Additionally, the paper outlines future expansion plans to include\ndomain-specific content, multimodal integration, and broader crowdsourcing\nefforts. The Swahili QA dataset aims to foster technological innovation in East\nAfrica and provide an essential resource for NLP research and applications in\nlow-resource languages.\n","authors":["Alfred Malengo Kondoro"],"pdf_url":"https://arxiv.org/pdf/2410.14289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14276v1","updated":"2024-10-18T08:31:22Z","published":"2024-10-18T08:31:22Z","title":"EcomEdit: An Automated E-commerce Knowledge Editing Framework for\n  Enhanced Product and Purchase Intention Understanding","summary":"  Knowledge Editing (KE) aims to correct and update factual information in\nLarge Language Models (LLMs) to ensure accuracy and relevance without\ncomputationally expensive fine-tuning. Though it has been proven effective in\nseveral domains, limited work has focused on its application within the\ne-commerce sector. However, there are naturally occurring scenarios that make\nKE necessary in this domain, such as the timely updating of product features\nand trending purchase intentions by customers, which necessitate further\nexploration. In this paper, we pioneer the application of KE in the e-commerce\ndomain by presenting ECOMEDIT, an automated e-commerce knowledge editing\nframework tailored for e-commerce-related knowledge and tasks. Our framework\nleverages more powerful LLMs as judges to enable automatic knowledge conflict\ndetection and incorporates conceptualization to enhance the semantic coverage\nof the knowledge to be edited. Through extensive experiments, we demonstrate\nthe effectiveness of ECOMEDIT in improving LLMs' understanding of product\ndescriptions and purchase intentions. We also show that LLMs, after our\nediting, can achieve stronger performance on downstream e-commerce tasks.\n","authors":["Ching Ming Samuel Lau","Weiqi Wang","Haochen Shi","Baixuan Xu","Jiaxin Bai","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2410.14276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14273v1","updated":"2024-10-18T08:27:02Z","published":"2024-10-18T08:27:02Z","title":"REEF: Representation Encoding Fingerprints for Large Language Models","summary":"  Protecting the intellectual property of open-source Large Language Models\n(LLMs) is very important, because training LLMs costs extensive computational\nresources and data. Therefore, model owners and third parties need to identify\nwhether a suspect model is a subsequent development of the victim model. To\nthis end, we propose a training-free REEF to identify the relationship between\nthe suspect and victim models from the perspective of LLMs' feature\nrepresentations. Specifically, REEF computes and compares the centered kernel\nalignment similarity between the representations of a suspect model and a\nvictim model on the same samples. This training-free REEF does not impair the\nmodel's general capabilities and is robust to sequential fine-tuning, pruning,\nmodel merging, and permutations. In this way, REEF provides a simple and\neffective way for third parties and models' owners to protect LLMs'\nintellectual property together. The code is available at\nhttps://github.com/tmylla/REEF.\n","authors":["Jie Zhang","Dongrui Liu","Chen Qian","Linfeng Zhang","Yong Liu","Yu Qiao","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2410.14273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14268v1","updated":"2024-10-18T08:22:07Z","published":"2024-10-18T08:22:07Z","title":"MoDification: Mixture of Depths Made Easy","summary":"  Long-context efficiency has recently become a trending topic in serving large\nlanguage models (LLMs). And mixture of depths (MoD) is proposed as a perfect\nfit to bring down both latency and memory. In this paper, however, we discover\nthat MoD can barely transform existing LLMs without costly training over an\nextensive number of tokens. To enable the transformations from any LLMs to MoD\nones, we showcase top-k operator in MoD should be promoted to threshold-p\noperator, and refinement to architecture and data should also be crafted along.\nAll these designs form our method termed MoDification. Through a comprehensive\nset of experiments covering model scales from 3B to 70B, we exhibit\nMoDification strikes an excellent balance between efficiency and effectiveness.\nMoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in\nmemory compared to original LLMs especially in long-context applications.\n","authors":["Chen Zhang","Meizhi Zhong","Qimeng Wang","Xuantao Lu","Zheyu Ye","Chengqiang Lu","Yan Gao","Yao Hu","Kehai Chen","Min Zhang","Dawei Song"],"pdf_url":"https://arxiv.org/pdf/2410.14268v1.pdf","comment":"12 pages, 9 figures, 5 tables, work in progress"},{"id":"http://arxiv.org/abs/2406.06572v2","updated":"2024-10-18T08:20:38Z","published":"2024-06-03T17:07:46Z","title":"Graph Neural Network Enhanced Retrieval for Question Answering of LLMs","summary":"  Retrieval augmented generation has revolutionized large language model (LLM)\noutputs by providing factual supports. Nevertheless, it struggles to capture\nall the necessary knowledge for complex reasoning questions. Existing retrieval\nmethods typically divide reference documents into passages, treating them in\nisolation. These passages, however, are often interrelated, such as passages\nthat are contiguous or share the same keywords. Therefore, it is crucial to\nrecognize such relatedness for enhancing the retrieval process. In this paper,\nwe propose a novel retrieval method, called GNN-Ret, which leverages graph\nneural networks (GNNs) to enhance retrieval by exploiting the relatedness\nbetween passages. Specifically, we first construct a graph of passages by\nconnecting passages that are structure-related or keyword-related. A graph\nneural network (GNN) is then leveraged to exploit the relationships between\npassages and improve the retrieval of supporting passages. Furthermore, we\nextend our method to handle multi-hop reasoning questions using a recurrent\ngraph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates\nthe graphs of passages from previous steps, thereby enhancing the retrieval of\nsupporting passages. Extensive experiments on benchmark datasets demonstrate\nthat GNN-Ret achieves higher accuracy for question answering with a single\nquery of LLMs than strong baselines that require multiple queries, and RGNN-Ret\nfurther improves accuracy and achieves state-of-the-art performance, with up to\n10.4% accuracy improvement on the 2WikiMQA dataset.\n","authors":["Zijian Li","Qingyan Guo","Jiawei Shao","Lei Song","Jiang Bian","Jun Zhang","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06572v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.14262v1","updated":"2024-10-18T08:18:18Z","published":"2024-10-18T08:18:18Z","title":"Good Parenting is all you need -- Multi-agentic LLM Hallucination\n  Mitigation","summary":"  This study explores the ability of Large Language Model (LLM) agents to\ndetect and correct hallucinations in AI-generated content. A primary agent was\ntasked with creating a blog about a fictional Danish artist named Flipfloppidy,\nwhich was then reviewed by another agent for factual inaccuracies. Most LLMs\nhallucinated the existence of this artist. Across 4,900 test runs involving\nvarious combinations of primary and reviewing agents, advanced AI models such\nas Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in\nidentifying hallucinations and successfully revised outputs in 85% to 100% of\ncases following feedback. These findings underscore the potential of advanced\nAI models to significantly enhance the accuracy and reliability of generated\ncontent, providing a promising approach to improving AI workflow orchestration.\n","authors":[" Edward"," Kwartler","Matthew Berman","Alan Aqrawi"],"pdf_url":"https://arxiv.org/pdf/2410.14262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14259v1","updated":"2024-10-18T08:14:10Z","published":"2024-10-18T08:14:10Z","title":"Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via\n  Role Recognition and Involvement Measurement","summary":"  The rapid development of large language models (LLMs), like ChatGPT, has\nresulted in the widespread presence of LLM-generated content on social media\nplatforms, raising concerns about misinformation, data biases, and privacy\nviolations, which can undermine trust in online discourse. While detecting\nLLM-generated content is crucial for mitigating these risks, current methods\noften focus on binary classification, failing to address the complexities of\nreal-world scenarios like human-AI collaboration. To move beyond binary\nclassification and address these challenges, we propose a new paradigm for\ndetecting LLM-generated content. This approach introduces two novel tasks: LLM\nRole Recognition (LLM-RR), a multi-class classification task that identifies\nspecific roles of LLM in content generation, and LLM Influence Measurement\n(LLM-IM), a regression task that quantifies the extent of LLM involvement in\ncontent creation. To support these tasks, we propose LLMDetect, a benchmark\ndesigned to evaluate detectors' performance on these new tasks. LLMDetect\nincludes the Hybrid News Detection Corpus (HNDC) for training detectors, as\nwell as DetectEval, a comprehensive evaluation suite that considers five\ndistinct cross-context variations and multi-intensity variations within the\nsame LLM role. This allows for a thorough assessment of detectors'\ngeneralization and robustness across diverse contexts. Our empirical validation\nof 10 baseline detection methods demonstrates that fine-tuned PLM-based models\nconsistently outperform others on both tasks, while advanced LLMs face\nchallenges in accurately detecting their own generated content. Our\nexperimental results and analysis offer insights for developing more effective\ndetection models for LLM-generated content. This research enhances the\nunderstanding of LLM-generated content and establishes a foundation for more\nnuanced detection methodologies.\n","authors":["Zihao Cheng","Li Zhou","Feng Jiang","Benyou Wang","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2410.14259v1.pdf","comment":"Social Media, Large Language Models, LLM-generated Text Detection,\n  AI-assisted News Detection"},{"id":"http://arxiv.org/abs/2410.14255v1","updated":"2024-10-18T08:04:36Z","published":"2024-10-18T08:04:36Z","title":"Nova: An Iterative Planning and Search Approach to Enhance Novelty and\n  Diversity of LLM Generated Ideas","summary":"  Scientific innovation is pivotal for humanity, and harnessing large language\nmodels (LLMs) to generate research ideas could transform discovery. However,\nexisting LLMs often produce simplistic and repetitive suggestions due to their\nlimited ability in acquiring external knowledge for innovation. To address this\nproblem, we introduce an enhanced planning and search methodology designed to\nboost the creative potential of LLM-based systems. Our approach involves an\niterative process to purposely plan the retrieval of external knowledge,\nprogressively enriching the idea generation with broader and deeper insights.\nValidation through automated and human assessments indicates that our framework\nsubstantially elevates the quality of generated ideas, particularly in novelty\nand diversity. The number of unique novel ideas produced by our framework is\n3.4 times higher than without it. Moreover, our method outperforms the current\nstate-of-the-art, generating at least 2.5 times more top-rated ideas based on\n170 seed papers in a Swiss Tournament evaluation.\n","authors":["Xiang Hu","Hongyu Fu","Jinge Wang","Yifeng Wang","Zhikun Li","Renjun Xu","Yu Lu","Yaochu Jin","Lili Pan","Zhenzhong Lan"],"pdf_url":"https://arxiv.org/pdf/2410.14255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17524v4","updated":"2024-10-18T08:03:02Z","published":"2024-04-26T16:41:00Z","title":"On the Use of Large Language Models to Generate Capability Ontologies","summary":"  Capability ontologies are increasingly used to model functionalities of\nsystems or machines. The creation of such ontological models with all\nproperties and constraints of capabilities is very complex and can only be done\nby ontology experts. However, Large Language Models (LLMs) have shown that they\ncan generate machine-interpretable models from natural language text input and\nthus support engineers / ontology experts. Therefore, this paper investigates\nhow LLMs can be used to create capability ontologies. We present a study with a\nseries of experiments in which capabilities with varying complexities are\ngenerated using different prompting techniques and with different LLMs. Errors\nin the generated ontologies are recorded and compared. To analyze the quality\nof the generated ontologies, a semi-automated approach based on RDF syntax\nchecking, OWL reasoning, and SHACL constraints is used. The results of this\nstudy are very promising because even for complex capabilities, the generated\nontologies are almost free of errors.\n","authors":["Luis Miguel Vieira da Silva","Aljosha Köcher","Felix Gehlhoff","Alexander Fay"],"pdf_url":"https://arxiv.org/pdf/2404.17524v4.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2410.14251v1","updated":"2024-10-18T08:01:39Z","published":"2024-10-18T08:01:39Z","title":"Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation","summary":"  Post-training is essential for enabling large language models (LLMs) to\nfollow human instructions. Inspired by the recent success of using LLMs to\nsimulate human society, we leverage multi-agent simulation to automatically\ngenerate diverse text-based scenarios, capturing a wide range of real-world\nhuman needs. We propose MATRIX, a multi-agent simulator that creates realistic\nand scalable scenarios. Leveraging these outputs, we introduce a novel\nscenario-driven instruction generator MATRIX-Gen for controllable and highly\nrealistic data synthesis. Extensive experiments demonstrate that our framework\neffectively generates both general and domain-specific data. Notably, on\nAlpacaEval 2 and Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on\ndatasets synthesized by MATRIX-Gen with just 20K instruction-response pairs,\noutperforms Meta's Llama-3-8B-Instruct model, which was trained on over 10M\npairs; see our project at https://github.com/ShuoTang123/MATRIX-Gen.\n","authors":["Shuo Tang","Xianghe Pang","Zexi Liu","Bohan Tang","Rui Ye","Xiaowen Dong","Yanfeng Wang","Siheng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14248v1","updated":"2024-10-18T07:52:22Z","published":"2024-10-18T07:52:22Z","title":"Addressing Blind Guessing: Calibration of Selection Bias in\n  Multiple-Choice Question Answering by Video Language Models","summary":"  Evaluating Video Language Models (VLMs) is a challenging task. Due to its\ntransparency, Multiple-Choice Question Answering (MCQA) is widely used to\nmeasure the performance of these models through accuracy. However, existing\nMCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to\nselection bias, when models disproportionately favor certain answer options\nbased on positional patterns observed during training. In this work, we conduct\na comprehensive empirical analysis of several VLM architectures across major\ndatasets designed to assess complex video-focused reasoning. We identify where\nthe bias is most pronounced and demonstrate to what extent model responses\nreflect genuine understanding of video content and related questions, as\nopposed to reliance on arbitrary patterns or superficial cues, such as answer\nposition. By decomposing the MCQA task and adapting fairness bias metrics to\nVLMs, we introduce a post-processing calibration technique BOLD to balance this\nbias. Our results show that reducing selection bias improves not only debiasing\nmetrics but also overall model performance, including Accuracy and F1 Mean\nscore. Our method, by suppressing \"blind guessing\", offers a more cost- and\ntime-effective approach to mitigating selection bias compared to existing\ntechniques. This study represents the first focused investigation of selection\nbias in video-to-text LLM-powered models.\n","authors":["Olga Loginova","Oleksandr Bezrukov","Alexey Kravets"],"pdf_url":"https://arxiv.org/pdf/2410.14248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14236v1","updated":"2024-10-18T07:36:57Z","published":"2024-10-18T07:36:57Z","title":"A Novel Method to Metigate Demographic and Expert Bias in ICD Coding\n  with Causal Inference","summary":"  ICD(International Classification of Diseases) coding involves assigning ICD\ncodes to patients visit based on their medical notes. Considering ICD coding as\na multi-label text classification task, researchers have developed\nsophisticated methods. Despite progress, these models often suffer from label\nimbalance and may develop spurious correlations with demographic factors.\nAdditionally, while human coders assign ICD codes, the inclusion of irrelevant\ninformation from unrelated experts introduces biases. To combat these issues,\nwe propose a novel method to mitigate Demographic and Expert biases in ICD\ncoding through Causal Inference (DECI). We provide a novel causality-based\ninterpretation in ICD Coding that models make predictions by three distinct\npathways. And based counterfactual reasoning, DECI mitigate demographic and\nexpert biases. Experimental results show that DECI outperforms state-of-the-art\nmodels, offering a significant advancement in accurate and unbiased ICD coding.\n","authors":["Bin Zhang","Junli Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07962v2","updated":"2024-10-18T07:34:39Z","published":"2024-06-12T07:41:44Z","title":"Toward a Method to Generate Capability Ontologies from Natural Language\n  Descriptions","summary":"  To achieve a flexible and adaptable system, capability ontologies are\nincreasingly leveraged to describe functions in a machine-interpretable way.\nHowever, modeling such complex ontological descriptions is still a manual and\nerror-prone task that requires a significant amount of effort and ontology\nexpertise. This contribution presents an innovative method to automate\ncapability ontology modeling using Large Language Models (LLMs), which have\nproven to be well suited for such tasks. Our approach requires only a natural\nlanguage description of a capability, which is then automatically inserted into\na predefined prompt using a few-shot prompting technique. After prompting an\nLLM, the resulting capability ontology is automatically verified through\nvarious steps in a loop with the LLM to check the overall correctness of the\ncapability ontology. First, a syntax check is performed, then a check for\ncontradictions, and finally a check for hallucinations and missing ontology\nelements. Our method greatly reduces manual effort, as only the initial natural\nlanguage description and a final human review and possible correction are\nnecessary, thereby streamlining the capability ontology generation process.\n","authors":["Luis Miguel Vieira da Silva","Aljosha Köcher","Felix Gehlhoff","Alexander Fay"],"pdf_url":"https://arxiv.org/pdf/2406.07962v2.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2410.14235v1","updated":"2024-10-18T07:34:21Z","published":"2024-10-18T07:34:21Z","title":"Towards Robust Knowledge Representations in Multilingual LLMs for\n  Equivalence and Inheritance based Consistent Reasoning","summary":"  Reasoning and linguistic skills form the cornerstone of human intelligence,\nfacilitating problem-solving and decision-making. Recent advances in Large\nLanguage Models (LLMs) have led to impressive linguistic capabilities and\nemergent reasoning behaviors, fueling widespread adoption across application\ndomains. However, LLMs still struggle with complex reasoning tasks,\nhighlighting their systemic limitations. In this work, we focus on evaluating\nwhether LLMs have the requisite representations to reason using two\nfoundational relationships: \"equivalence\" and \"inheritance\". We introduce novel\ntasks and benchmarks spanning six languages and observe that current SOTA LLMs\noften produce conflicting answers to the same questions across languages in\n17.3-57.5% of cases and violate inheritance constraints in up to 37.2% cases.\nTo enhance consistency across languages, we propose novel \"Compositional\nRepresentations\" where tokens are represented as composition of equivalent\ntokens across languages, with resulting conflict reduction (up to -4.7%)\nindicating benefits of shared LLM representations.\n","authors":["Gaurav Arora","Srujana Merugu","Shreya Jain","Vaibhav Saxena"],"pdf_url":"https://arxiv.org/pdf/2410.14235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14231v1","updated":"2024-10-18T07:25:00Z","published":"2024-10-18T07:25:00Z","title":"Unveiling Large Language Models Generated Texts: A Multi-Level\n  Fine-Grained Detection Framework","summary":"  Large language models (LLMs) have transformed human writing by enhancing\ngrammar correction, content expansion, and stylistic refinement. However, their\nwidespread use raises concerns about authorship, originality, and ethics, even\npotentially threatening scholarly integrity. Existing detection methods, which\nmainly rely on single-feature analysis and binary classification, often fail to\neffectively identify LLM-generated text in academic contexts. To address these\nchallenges, we propose a novel Multi-level Fine-grained Detection (MFD)\nframework that detects LLM-generated text by integrating low-level structural,\nhigh-level semantic, and deep-level linguistic features, while conducting\nsentence-level evaluations of lexicon, grammar, and syntax for comprehensive\nanalysis. To improve detection of subtle differences in LLM-generated text and\nenhance robustness against paraphrasing, we apply two mainstream evasion\ntechniques to rewrite the text. These variations, along with original texts,\nare used to train a text encoder via contrastive learning, extracting\nhigh-level semantic features of sentence to boost detection generalization.\nFurthermore, we leverage advanced LLM to analyze the entire text and extract\ndeep-level linguistic features, enhancing the model's ability to capture\ncomplex patterns and nuances while effectively incorporating contextual\ninformation. Extensive experiments on public datasets show that the MFD model\noutperforms existing methods, achieving an MAE of 0.1346 and an accuracy of\n88.56%. Our research provides institutions and publishers with an effective\nmechanism to detect LLM-generated text, mitigating risks of compromised\nauthorship. Educators and editors can use the model's predictions to refine\nverification and plagiarism prevention protocols, ensuring adherence to\nstandards.\n","authors":["Zhen Tao","Zhiyu Li","Runyu Chen","Dinghao Xi","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2410.14231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14225v1","updated":"2024-10-18T07:14:54Z","published":"2024-10-18T07:14:54Z","title":"Few-Shot Joint Multimodal Entity-Relation Extraction via\n  Knowledge-Enhanced Cross-modal Prompt Model","summary":"  Joint Multimodal Entity-Relation Extraction (JMERE) is a challenging task\nthat aims to extract entities and their relations from text-image pairs in\nsocial media posts. Existing methods for JMERE require large amounts of labeled\ndata. However, gathering and annotating fine-grained multimodal data for JMERE\nposes significant challenges. Initially, we construct diverse and comprehensive\nmultimodal few-shot datasets fitted to the original data distribution. To\naddress the insufficient information in the few-shot setting, we introduce the\n\\textbf{K}nowledge-\\textbf{E}nhanced \\textbf{C}ross-modal \\textbf{P}rompt\n\\textbf{M}odel (KECPM) for JMERE. This method can effectively address the\nproblem of insufficient information in the few-shot setting by guiding a large\nlanguage model to generate supplementary background knowledge. Our proposed\nmethod comprises two stages: (1) a knowledge ingestion stage that dynamically\nformulates prompts based on semantic similarity guide ChatGPT generating\nrelevant knowledge and employs self-reflection to refine the knowledge; (2) a\nknowledge-enhanced language model stage that merges the auxiliary knowledge\nwith the original input and utilizes a transformer-based model to align with\nJMERE's required output format. We extensively evaluate our approach on a\nfew-shot dataset derived from the JMERE dataset, demonstrating its superiority\nover strong baselines in terms of both micro and macro F$_1$ scores.\nAdditionally, we present qualitative analyses and case studies to elucidate the\neffectiveness of our model.\n","authors":["Li Yuan","Yi Cai","Junsheng Huang"],"pdf_url":"https://arxiv.org/pdf/2410.14225v1.pdf","comment":"accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2409.09785v3","updated":"2024-10-18T07:11:35Z","published":"2024-09-15T16:32:49Z","title":"Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition","summary":"  Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.\n","authors":["Chao-Han Huck Yang","Taejin Park","Yuan Gong","Yuanchao Li","Zhehuai Chen","Yen-Ting Lin","Chen Chen","Yuchen Hu","Kunal Dhawan","Piotr Żelasko","Chao Zhang","Yun-Nung Chen","Yu Tsao","Jagadeesh Balam","Boris Ginsburg","Sabato Marco Siniscalchi","Eng Siong Chng","Peter Bell","Catherine Lai","Shinji Watanabe","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2409.09785v3.pdf","comment":"IEEE SLT 2024. The initial draft version has been done in December\n  2023. Post-ASR Text Processing and Understanding Community and LlaMA-7B\n  pre-training correction model:\n  https://huggingface.co/GenSEC-LLM/SLT-Task1-Llama2-7b-HyPo-baseline"},{"id":"http://arxiv.org/abs/2410.09421v2","updated":"2024-10-18T07:10:38Z","published":"2024-10-12T07:56:47Z","title":"VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language\n  Models Alignment","summary":"  As large vision-language models (LVLMs) evolve rapidly, the demand for\nhigh-quality and diverse data to align these models becomes increasingly\ncrucial. However, the creation of such data with human supervision proves\ncostly and time-intensive. In this paper, we investigate the efficacy of AI\nfeedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the\nfirst large-scale vision-language feedback dataset, comprising over 82K\nmulti-modal instructions and comprehensive rationales generated by\noff-the-shelf models without human annotations. To evaluate the effectiveness\nof AI feedback for vision-language alignment, we train Silkie, an LVLM\nfine-tuned via direct preference optimization on VLFeedback. Silkie showcases\nexceptional performance regarding helpfulness, visual faithfulness, and safety\nmetrics. It outperforms its base model by 6.9\\% and 9.5\\% in perception and\ncognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits\nenhanced resilience against red-teaming attacks. Furthermore, our analysis\nunderscores the advantage of AI feedback, particularly in fostering preference\ndiversity to deliver more comprehensive improvements. Our dataset, training\ncode and models are available at https://vlf-silkie.github.io.\n","authors":["Lei Li","Zhihui Xie","Mukai Li","Shunian Chen","Peiyi Wang","Liang Chen","Yazheng Yang","Benyou Wang","Lingpeng Kong","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2410.09421v2.pdf","comment":"EMNLP 2024 Main Conference camera-ready version (fixed small typos).\n  This article supersedes arXiv:2312.10665"},{"id":"http://arxiv.org/abs/2403.04808v3","updated":"2024-10-18T07:05:57Z","published":"2024-03-06T10:55:30Z","title":"WaterMax: breaking the LLM watermark detectability-robustness-quality\n  trade-off","summary":"  Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.\n","authors":["Eva Giboulot","Teddy Furon"],"pdf_url":"https://arxiv.org/pdf/2403.04808v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14024v4","updated":"2024-10-18T06:59:24Z","published":"2024-06-20T06:42:27Z","title":"LLM Critics Help Catch Bugs in Mathematics: Towards a Better\n  Mathematical Verifier with Natural Language Feedback","summary":"  In recent progress, mathematical verifiers have achieved success in\nmathematical reasoning tasks by validating the correctness of solutions\ngenerated by policy models. However, existing verifiers are trained with binary\nclassification labels, which are not informative enough for the model to\naccurately assess the solutions. To mitigate the aforementioned insufficiency\nof binary labels, we introduce step-wise natural language feedback as rationale\nlabels, that is, the correctness of each step and the detailed explanations. In\nthis paper, we propose Math-Minos, a natural language feedback-enhanced\nverifier by constructing automatically generated training data and a two-stage\ntraining paradigm for effective training and efficient inference. Our\nexperiments reveal that a small set of natural language feedback can\nsignificantly boost the performance of the verifier in both verification and\nreinforcement learning. We have released the code and data for further\nexploration.\n","authors":["Bofei Gao","Zefan Cai","Runxin Xu","Peiyi Wang","Ce Zheng","Runji Lin","Keming Lu","Dayiheng Liu","Chang Zhou","Wen Xiao","Junjie Hu","Tianyu Liu","Baobao Chang"],"pdf_url":"https://arxiv.org/pdf/2406.14024v4.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2410.14211v1","updated":"2024-10-18T06:57:19Z","published":"2024-10-18T06:57:19Z","title":"Paths-over-Graph: Knowledge Graph Enpowered Large Language Model\n  Reasoning","summary":"  Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%.\n","authors":["Xingyu Tan","Xiaoyang Wang","Qing Liu","Xiwei Xu","Xin Yuan","Wenjie Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19740v2","updated":"2024-10-18T06:57:08Z","published":"2024-05-30T06:38:32Z","title":"PertEval: Unveiling Real Knowledge Capacity of LLMs with\n  Knowledge-Invariant Perturbations","summary":"  Expert-designed close-ended benchmarks are indispensable in assessing the\nknowledge capacity of large language models (LLMs). Despite their widespread\nuse, concerns have mounted regarding their reliability due to limited test\nscenarios and an unavoidable risk of data contamination. To rectify this, we\npresent PertEval, a toolkit devised for in-depth probing of LLMs' knowledge\ncapacity through \\textbf{knowledge-invariant perturbations}. These\nperturbations employ human-like restatement techniques to generate on-the-fly\ntest samples from static benchmarks, meticulously retaining knowledge-critical\ncontent while altering irrelevant details. Our toolkit further includes a suite\nof \\textbf{response consistency analyses} that compare performance on raw vs.\nperturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six\nrepresentative LLMs are re-evaluated using PertEval. Results reveal\nsignificantly inflated performance of the LLMs on raw benchmarks, including an\nabsolute 25.8% overestimation for GPT-4. Additionally, through a nuanced\nresponse pattern analysis, we discover that PertEval retains LLMs' uncertainty\nto specious knowledge, and reveals their potential rote memorization to correct\noptions which leads to overestimated performance. We also find that the\ndetailed response consistency analyses by PertEval could illuminate various\nweaknesses in existing LLMs' knowledge mastery and guide the development of\nrefinement. Our findings provide insights for advancing more robust and\ngenuinely knowledgeable LLMs. Our code is available at\n\\url{https://github.com/aigc-apps/PertEval}.\n","authors":["Jiatong Li","Renjun Hu","Kunzhe Huang","Yan Zhuang","Qi Liu","Mengxiao Zhu","Xing Shi","Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2405.19740v2.pdf","comment":"Accepted by NeurIPS '24 D&B Spotlight; 28 pages, 15 figures, 14\n  tables"},{"id":"http://arxiv.org/abs/2403.01976v5","updated":"2024-10-18T06:52:17Z","published":"2024-03-04T12:19:28Z","title":"SciAssess: Benchmarking LLM Proficiency in Scientific Literature\n  Analysis","summary":"  Recent breakthroughs in Large Language Models (LLMs) have revolutionized\nscientific literature analysis. However, existing benchmarks fail to adequately\nevaluate the proficiency of LLMs in this domain, particularly in scenarios\nrequiring higher-level abilities beyond mere memorization and the handling of\nmultimodal data. In response to this gap, we introduce SciAssess, a benchmark\nspecifically designed for the comprehensive evaluation of LLMs in scientific\nliterature analysis. It aims to thoroughly assess the efficacy of LLMs by\nevaluating their capabilities in Memorization (L1), Comprehension (L2), and\nAnalysis \\& Reasoning (L3). It encompasses a variety of tasks drawn from\ndiverse scientific fields, including biology, chemistry, material, and\nmedicine. To ensure the reliability of SciAssess, rigorous quality control\nmeasures have been implemented, ensuring accuracy, anonymization, and\ncompliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting\ntheir strengths and areas for improvement. We hope this evaluation supports the\nongoing development of LLM applications in scientific literature analysis.\nSciAssess and its resources are available at\n\\url{https://github.com/sci-assess/SciAssess}.\n","authors":["Hengxing Cai","Xiaochen Cai","Junhan Chang","Sihang Li","Lin Yao","Changxin Wang","Zhifeng Gao","Hongshuai Wang","Yongge Li","Mujie Lin","Shuwen Yang","Jiankun Wang","Mingjun Xu","Jin Huang","Xi Fang","Jiaxi Zhuang","Yuqi Yin","Yaqi Li","Changhong Chen","Zheng Cheng","Zifeng Zhao","Linfeng Zhang","Guolin Ke"],"pdf_url":"https://arxiv.org/pdf/2403.01976v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14208v1","updated":"2024-10-18T06:50:15Z","published":"2024-10-18T06:50:15Z","title":"Montessori-Instruct: Generate Influential Training Data Tailored for\n  Student Learning","summary":"  Synthetic data has been widely used to train large language models, but their\ngenerative nature inevitably introduces noisy, non-informative, and misleading\nlearning signals. In this paper, we propose Montessori-Instruct, a novel data\nsynthesis framework that tailors the data synthesis ability of the teacher\nlanguage model toward the student language model's learning process.\nSpecifically, we utilize local data influence of synthetic training data points\non students to characterize students' learning preferences. Then, we train the\nteacher model with Direct Preference Optimization (DPO) to generate synthetic\ndata tailored toward student learning preferences. Experiments with\nLlama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and\nMT-Bench demonstrate that Montessori-Instruct significantly outperforms\nstandard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also\nbeats data synthesized by a stronger teacher model, GPT-4o. Further analysis\nconfirms the benefits of teacher's learning to generate more influential\ntraining data in the student's improved learning, the advantages of local data\ninfluence in accurately measuring student preferences, and the robustness of\nMontessori-Instruct across different student models. Our code and data are\nopen-sourced at https://github.com/cxcscmu/Montessori-Instruct.\n","authors":["Xiaochuan Li","Zichun Yu","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.14208v1.pdf","comment":"Codes and data are open-sourced at\n  https://github.com/cxcscmu/Montessori-Instruct"},{"id":"http://arxiv.org/abs/2410.14204v1","updated":"2024-10-18T06:38:22Z","published":"2024-10-18T06:38:22Z","title":"MediTOD: An English Dialogue Dataset for Medical History Taking with\n  Comprehensive Annotations","summary":"  Medical task-oriented dialogue systems can assist doctors by collecting\npatient medical history, aiding in diagnosis, or guiding treatment selection,\nthereby reducing doctor burnout and expanding access to medical services.\nHowever, doctor-patient dialogue datasets are not readily available, primarily\ndue to privacy regulations. Moreover, existing datasets lack comprehensive\nannotations involving medical slots and their different attributes, such as\nsymptoms and their onset, progression, and severity. These comprehensive\nannotations are crucial for accurate diagnosis. Finally, most existing datasets\nare non-English, limiting their utility for the larger research community.\n  In response, we introduce MediTOD, a new dataset of doctor-patient dialogues\nin English for the medical history-taking task. Collaborating with doctors, we\ndevise a questionnaire-based labeling scheme tailored to the medical domain.\nThen, medical professionals create the dataset with high-quality comprehensive\nannotations, capturing medical slots and their attributes. We establish\nbenchmarks in supervised and few-shot settings on MediTOD for natural language\nunderstanding, policy learning, and natural language generation subtasks,\nevaluating models from both TOD and biomedical domains. We make MediTOD\npublicly available for future research.\n","authors":["Vishal Vivek Saley","Goonjan Saha","Rocktim Jyoti Das","Dinesh Raghu"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2410.14204v1.pdf","comment":"EMNLP2024 Camera Ready Version"},{"id":"http://arxiv.org/abs/2410.14202v1","updated":"2024-10-18T06:35:17Z","published":"2024-10-18T06:35:17Z","title":"Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay\n  Scoring with Rationale Generated by LLMs","summary":"  Existing automated essay scoring (AES) has solely relied on essay text\nwithout using explanatory rationales for the scores, thereby forgoing an\nopportunity to capture the specific aspects evaluated by rubric indicators in a\nfine-grained manner. This paper introduces Rationale-based Multiple Trait\nScoring (RMTS), a novel approach for multi-trait essay scoring that integrates\nprompt-engineering-based large language models (LLMs) with a fine-tuning-based\nessay scoring model using a smaller large language model (S-LLM). RMTS uses an\nLLM-based trait-wise rationale generation system where a separate LLM agent\ngenerates trait-specific rationales based on rubric guidelines, which the\nscoring model uses to accurately predict multi-trait scores. Extensive\nexperiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,\nshow that RMTS significantly outperforms state-of-the-art models and vanilla\nS-LLMs in trait-specific scoring. By assisting quantitative assessment with\nfine-grained qualitative rationales, RMTS enhances the trait-wise reliability,\nproviding partial explanations about essays.\n","authors":["SeongYeub Chu","JongWoo Kim","Bryan Wong","MunYong Yi"],"pdf_url":"https://arxiv.org/pdf/2410.14202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14200v1","updated":"2024-10-18T06:31:40Z","published":"2024-10-18T06:31:40Z","title":"E3D-GPT: Enhanced 3D Visual Foundation for Medical Vision-Language Model","summary":"  The development of 3D medical vision-language models holds significant\npotential for disease diagnosis and patient treatment. However, compared to 2D\nmedical images, 3D medical images, such as CT scans, face challenges related to\nlimited training data and high dimension, which severely restrict the progress\nof 3D medical vision-language models. To address these issues, we collect a\nlarge amount of unlabeled 3D CT data and utilize self-supervised learning to\nconstruct a 3D visual foundation model for extracting 3D visual features. Then,\nwe apply 3D spatial convolutions to aggregate and project high-level image\nfeatures, reducing computational complexity while preserving spatial\ninformation. We also construct two instruction-tuning datasets based on BIMCV-R\nand CT-RATE to fine-tune the 3D vision-language model. Our model demonstrates\nsuperior performance compared to existing methods in report generation, visual\nquestion answering, and disease diagnosis. Code and data will be made publicly\navailable soon.\n","authors":["Haoran Lai","Zihang Jiang","Qingsong Yao","Rongsheng Wang","Zhiyang He","Xiaodong Tao","Wei Wei","Weifu Lv","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.14200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14198v1","updated":"2024-10-18T06:25:27Z","published":"2024-10-18T06:25:27Z","title":"Supervised Chain of Thought","summary":"  Large Language Models (LLMs) have revolutionized natural language processing\nand hold immense potential for advancing Artificial Intelligence. However, the\ncore architecture of most mainstream LLMs -- the Transformer -- has inherent\nlimitations in computational depth, rendering them theoretically incapable of\nsolving many reasoning tasks that demand increasingly deep computations. Chain\nof Thought (CoT) prompting has emerged as a technique to address these\narchitectural limitations, as evidenced by several theoretical studies. It\noffers a promising approach to solving complex reasoning tasks that were\npreviously beyond the capabilities of these models. Despite its successes, CoT\nand its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a\n\"one-prompt-for-all\" approach, using a single prompt structure (e.g., \"think\nstep by step\") for a wide range of tasks -- from counting and sorting to\nsolving mathematical and algorithmic problems. This approach poses significant\nchallenges for models to generate the correct reasoning steps, as the model\nmust navigate through a vast prompt template space to find the appropriate\ntemplate for each task. In this work, we build upon previous theoretical\nanalyses of CoT to demonstrate how the one-prompt-for-all approach can\nnegatively affect the computability of LLMs. We partition the solution search\nspace into two: the prompt space and the answer space. Our findings show that\ntask-specific supervision is essential for navigating the prompt space\naccurately and achieving optimal performance. Through experiments with\nstate-of-the-art LLMs, we reveal a gap in reasoning performance when\nsupervision is applied versus when it is not.\n","authors":["Xiang Zhang","Dujian Ding"],"pdf_url":"https://arxiv.org/pdf/2410.14198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06833v2","updated":"2024-10-18T06:19:22Z","published":"2024-04-10T08:49:27Z","title":"Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural\n  Knowledge","summary":"  Recent studies have highlighted the presence of cultural biases in Large\nLanguage Models (LLMs), yet often lack a robust methodology to dissect these\nphenomena comprehensively. Our work aims to bridge this gap by delving into the\nFood domain, a universally relevant yet culturally diverse aspect of human\nlife. We introduce FmLAMA, a multilingual dataset centered on food-related\ncultural facts and variations in food practices. We analyze LLMs across various\narchitectures and configurations, evaluating their performance in both\nmonolingual and multilingual settings. By leveraging templates in six different\nlanguages, we investigate how LLMs interact with language-specific and cultural\nknowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias\ntowards food knowledge prevalent in the United States; (2) Incorporating\nrelevant cultural context significantly improves LLMs' ability to access\ncultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is\nhighly dependent on the interplay between the probing language, the specific\nmodel architecture, and the cultural context in question. This research\nunderscores the complexity of integrating cultural understanding into LLMs and\nemphasizes the importance of culturally diverse datasets to mitigate biases and\nenhance model performance across different cultural domains.\n","authors":["Li Zhou","Taelin Karidi","Wanlong Liu","Nicolas Garneau","Yong Cao","Wenyu Chen","Haizhou Li","Daniel Hershcovich"],"pdf_url":"https://arxiv.org/pdf/2404.06833v2.pdf","comment":"cultural bias analysis, cultural knowledge probing, large language\n  models, cultural NLP"},{"id":"http://arxiv.org/abs/2405.15585v3","updated":"2024-10-18T06:14:50Z","published":"2024-05-24T14:13:54Z","title":"Synergizing In-context Learning with Hints for End-to-end Task-oriented\n  Dialog Systems","summary":"  End-to-end Task-Oriented Dialog (TOD) systems typically require extensive\ntraining datasets to perform well. In contrast, large language model (LLM)\nbased TOD systems can excel even with limited data due to their ability to\nlearn tasks through in-context exemplars. However, these models lack alignment\nwith the style of responses in training data and often generate comprehensive\nresponses, making it difficult for users to grasp the information quickly. In\nresponse, we propose SyncTOD that synergizes LLMs with task-specific hints to\nimprove alignment in low-data settings. SyncTOD employs small auxiliary models\nto provide hints and select exemplars for in-context prompts. With ChatGPT,\nSyncTOD achieves superior performance compared to LLM-based baselines and SoTA\nmodels in low-data settings, while retaining competitive performance in\nfull-data settings.\n","authors":["Vishal Vivek Saley","Rocktim Jyoti Das","Dinesh Raghu"," Mausam"],"pdf_url":"https://arxiv.org/pdf/2405.15585v3.pdf","comment":"EMNLP2024 Camera-Ready Version"},{"id":"http://arxiv.org/abs/2410.14194v1","updated":"2024-10-18T06:09:41Z","published":"2024-10-18T06:09:41Z","title":"Speciesism in Natural Language Processing Research","summary":"  Natural Language Processing (NLP) research on AI Safety and social bias in AI\nhas focused on safety for humans and social bias against human minorities.\nHowever, some AI ethicists have argued that the moral significance of nonhuman\nanimals has been ignored in AI research. Therefore, the purpose of this study\nis to investigate whether there is speciesism, i.e., discrimination against\nnonhuman animals, in NLP research. First, we explain why nonhuman animals are\nrelevant in NLP research. Next, we survey the findings of existing research on\nspeciesism in NLP researchers, data, and models and further investigate this\nproblem in this study. The findings of this study suggest that speciesism\nexists within researchers, data, and models, respectively. Specifically, our\nsurvey and experiments show that (a) among NLP researchers, even those who\nstudy social bias in AI, do not recognize speciesism or speciesist bias; (b)\namong NLP data, speciesist bias is inherent in the data annotated in the\ndatasets used to evaluate NLP models; (c) OpenAI GPTs, recent NLP models,\nexhibit speciesist bias by default. Finally, we discuss how we can reduce\nspeciesism in NLP research.\n","authors":["Masashi Takeshita","Rafal Rzepka"],"pdf_url":"https://arxiv.org/pdf/2410.14194v1.pdf","comment":"This article is a preprint and has not been peer-reviewed. The\n  postprint has been accepted for publication in AI and Ethics. Please cite the\n  final version of the article once it is published"},{"id":"http://arxiv.org/abs/2410.04422v4","updated":"2024-10-18T06:09:31Z","published":"2024-10-06T09:29:19Z","title":"Hyper-multi-step: The Truth Behind Difficult Long-context Tasks","summary":"  Long-context language models (LCLM), characterized by their extensive context\nwindow, is becoming increasingly popular. Meanwhile, many long-context\nbenchmarks present challenging tasks that even the most advanced LCLMs struggle\nto complete. However, the underlying sources of various challenging\nlong-context tasks have seldom been studied. To bridge this gap, we conduct\nexperiments to indicate their difficulty stems primarily from two basic issues:\n\"multi-matching retrieval,\" which requires the simultaneous retrieval of\nmultiple items, and \"logic-based retrieval,\" which necessitates logical\njudgment within retrieval criteria. These two problems, while seemingly\nstraightforward, actually exceed the capabilities of LCLMs because they are\nproven to be hyper-multi-step (demanding numerous steps to solve) in nature.\nThis finding could explain why LLMs struggle with more advanced long-context\ntasks, providing a more accurate perspective for rethinking solutions for them.\n","authors":["Yijiong Yu","Ma Xiufa","Fang Jianwei","Zhi Xu","Su Guangyao","Wang Jiancheng","Yongfeng Huang","Zhixiao Qi","Wei Wang","Weifeng Liu","Ran Chen","Ji Pei"],"pdf_url":"https://arxiv.org/pdf/2410.04422v4.pdf","comment":"Our code is publicly available at\n  https://github.com/yuyijiong/hard_retrieval_for_llm and the datasets is at\n  https://huggingface.co/datasets/yuyijiong/difficult_retrieval"},{"id":"http://arxiv.org/abs/2410.14184v1","updated":"2024-10-18T05:31:13Z","published":"2024-10-18T05:31:13Z","title":"MetaAlign: Align Large Language Models with Diverse Preferences during\n  Inference Time","summary":"  Large Language Models (LLMs) acquire extensive knowledge and remarkable\nabilities from extensive text corpora, making them powerful tools for various\napplications. To make LLMs more usable, aligning them with human preferences is\nessential. Existing alignment techniques, such as Reinforcement Learning from\nHuman Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed\npredefined preferences directly within the model's parameters. These methods,\nhowever, often result in a static alignment that can not account for the\ndiversity of human preferences in practical applications. In response to this\nchallenge, we propose an effective method, \\textbf{MetaAlign}, which aims to\nhelp LLMs dynamically align with various explicit or implicit preferences\nspecified at inference time. Experimental results show that LLMs optimized on\nour meticulously constructed MetaAlign Dataset can effectively align with any\npreferences specified at the inference stage, validating the feasibility of\nMetaAlign. We hope that our work can provide some insights into the alignment\nof language models.\n","authors":["Mozhi Zhang","Pengyu Wang","Chenkun Tan","Mianqiu Huang","Dong Zhang","Yaqian Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.14184v1.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.00131v2","updated":"2024-10-18T05:22:02Z","published":"2024-09-30T18:12:18Z","title":"Fisher Information-based Efficient Curriculum Federated Learning with\n  Large Language Models","summary":"  As a promising paradigm to collaboratively train models with decentralized\ndata, Federated Learning (FL) can be exploited to fine-tune Large Language\nModels (LLMs). While LLMs correspond to huge size, the scale of the training\ndata significantly increases, which leads to tremendous amounts of computation\nand communication costs. The training data is generally non-Independent and\nIdentically Distributed (non-IID), which requires adaptive data processing\nwithin each device. Although Low Rank Adaptation (LoRA) can significantly\nreduce the scale of parameters to update in the fine-tuning process, it still\ntakes unaffordable time to transfer the low-rank parameters of all the layers\nin LLMs. In this paper, we propose a Fisher Information-based Efficient\nCurriculum Federated Learning framework (FibecFed) with two novel methods,\ni.e., adaptive federated curriculum learning and efficient sparse parameter\nupdate. First, we propose a fisher information-based method to adaptively\nsample data within each device to improve the effectiveness of the FL\nfine-tuning process. Second, we dynamically select the proper layers for global\naggregation and sparse parameters for local update with LoRA so as to improve\nthe efficiency of the FL fine-tuning process. Extensive experimental results\nbased on 10 datasets demonstrate that FibecFed yields excellent performance (up\nto 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61%\nfaster) compared with 17 baseline approaches).\n","authors":["Ji Liu","Jiaxiang Ren","Ruoming Jin","Zijie Zhang","Yang Zhou","Patrick Valduriez","Dejing Dou"],"pdf_url":"https://arxiv.org/pdf/2410.00131v2.pdf","comment":"27 pages, 8 figures, 14 tables, to appear in EMNLP 2024"},{"id":"http://arxiv.org/abs/2410.14182v1","updated":"2024-10-18T05:21:05Z","published":"2024-10-18T05:21:05Z","title":"LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs","summary":"  Laboratory accidents pose significant risks to human life and property,\nunderscoring the importance of robust safety protocols. Despite advancements in\nsafety training, laboratory personnel may still unknowingly engage in unsafe\npractices. With the increasing reliance on large language models (LLMs) for\nguidance in various fields, including laboratory settings, there is a growing\nconcern about their reliability in critical safety-related decision-making.\nUnlike trained human researchers, LLMs lack formal lab safety education,\nraising questions about their ability to provide safe and accurate guidance.\nExisting research on LLM trustworthiness primarily focuses on issues such as\nethical compliance, truthfulness, and fairness but fails to fully cover\nsafety-critical real-world applications, like lab safety. To address this gap,\nwe propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive\nevaluation framework based on a new taxonomy aligned with Occupational Safety\nand Health Administration (OSHA) protocols. This benchmark includes 765\nmultiple-choice questions verified by human experts, assessing LLMs and vision\nlanguage models (VLMs) performance in lab safety contexts. Our evaluations\ndemonstrate that while GPT-4o outperforms human participants, it is still prone\nto critical errors, highlighting the risks of relying on LLMs in\nsafety-critical environments. Our findings emphasize the need for specialized\nbenchmarks to accurately assess the trustworthiness of LLMs in real-world\nsafety applications.\n","authors":["Yujun Zhou","Jingdong Yang","Kehan Guo","Pin-Yu Chen","Tian Gao","Werner Geyer","Nuno Moniz","Nitesh V Chawla","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14182v1.pdf","comment":"50 pages, 19 figures"},{"id":"http://arxiv.org/abs/2410.14180v1","updated":"2024-10-18T05:16:39Z","published":"2024-10-18T05:16:39Z","title":"XForecast: Evaluating Natural Language Explanations for Time Series\n  Forecasting","summary":"  Time series forecasting aids decision-making, especially for stakeholders who\nrely on accurate predictions, making it very important to understand and\nexplain these models to ensure informed decisions. Traditional explainable AI\n(XAI) methods, which underline feature or temporal importance, often require\nexpert knowledge. In contrast, natural language explanations (NLEs) are more\naccessible to laypeople. However, evaluating forecast NLEs is difficult due to\nthe complex causal relationships in time series data. To address this, we\nintroduce two new performance metrics based on simulatability, assessing how\nwell a human surrogate can predict model forecasts using the explanations.\nExperiments show these metrics differentiate good from poor explanations and\nalign with human judgments. Utilizing these metrics, we further evaluate the\nability of state-of-the-art large language models (LLMs) to generate\nexplanations for time series data, finding that numerical reasoning, rather\nthan model size, is the main factor influencing explanation quality.\n","authors":["Taha Aksu","Chenghao Liu","Amrita Saha","Sarah Tan","Caiming Xiong","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2410.14180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14179v1","updated":"2024-10-18T05:15:50Z","published":"2024-10-18T05:15:50Z","title":"MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart\n  Problems","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nabilities across various tasks, including visual question answering and chart\ncomprehension, yet existing benchmarks for chart-related tasks fall short in\ncapturing the complexity of real-world multi-chart scenarios. Current\nbenchmarks primarily focus on single-chart tasks, neglecting the multi-hop\nreasoning required to extract and integrate information from multiple charts,\nwhich is essential in practical applications. To fill this gap, we introduce\nMultiChartQA, a benchmark that evaluates MLLMs' capabilities in four key areas:\ndirect question answering, parallel question answering, comparative reasoning,\nand sequential reasoning. Our evaluation of a wide range of MLLMs reveals\nsignificant performance gaps compared to humans. These results highlight the\nchallenges in multi-chart comprehension and the potential of MultiChartQA to\ndrive advancements in this field. Our code and data are available at\nhttps://github.com/Zivenzhu/Multi-chart-QA\n","authors":["Zifeng Zhu","Mengzhao Jia","Zhihan Zhang","Lang Li","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.14179v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.10270v2","updated":"2024-10-18T05:07:38Z","published":"2024-10-14T08:21:25Z","title":"QUIS: Question-guided Insights Generation for Automated Exploratory Data\n  Analysis","summary":"  Discovering meaningful insights from a large dataset, known as Exploratory\nData Analysis (EDA), is a challenging task that requires thorough exploration\nand analysis of the data. Automated Data Exploration (ADE) systems use\ngoal-oriented methods with Large Language Models and Reinforcement Learning\ntowards full automation. However, these methods require human involvement to\nanticipate goals that may limit insight extraction, while fully automated\nsystems demand significant computational resources and retraining for new\ndatasets. We introduce QUIS, a fully automated EDA system that operates in two\nstages: insight generation (ISGen) driven by question generation (QUGen). The\nQUGen module generates questions in iterations, refining them from previous\niterations to enhance coverage without human intervention or manually curated\nexamples. The ISGen module analyzes data to produce multiple relevant insights\nin response to each question, requiring no prior training and enabling QUIS to\nadapt to new datasets.\n","authors":["Abhijit Manatkar","Ashlesha Akella","Parthivi Gupta","Krishnasuri Narayanam"],"pdf_url":"https://arxiv.org/pdf/2410.10270v2.pdf","comment":"Accepted for ENLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2408.15545v3","updated":"2024-10-18T05:04:53Z","published":"2024-08-28T05:41:52Z","title":"SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding","summary":"  Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.\n","authors":["Sihang Li","Jin Huang","Jiaxi Zhuang","Yaorui Shi","Xiaochen Cai","Mingjun Xu","Xiang Wang","Linfeng Zhang","Guolin Ke","Hengxing Cai"],"pdf_url":"https://arxiv.org/pdf/2408.15545v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13276v2","updated":"2024-10-18T05:01:11Z","published":"2024-10-17T07:07:09Z","title":"SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs","summary":"  Attention is the cornerstone of modern Large Language Models (LLMs). Yet its\nquadratic complexity limits the efficiency and scalability of LLMs, especially\nfor those with a long-context window. A promising approach addressing this\nlimitation is to leverage the sparsity in attention. However, existing\nsparsity-based solutions predominantly rely on predefined patterns or\nheuristics to approximate sparsity. This practice falls short to fully capture\nthe dynamic nature of attention sparsity in language-based tasks. This paper\nargues that attention sparsity should be learned rather than predefined. To\nthis end, we design SeerAttention, a new Attention mechanism that augments the\nconventional attention with a learnable gate that adaptively selects\nsignificant blocks in an attention map and deems the rest blocks sparse. Such\nblock-level sparsity effectively balances accuracy and speedup. To enable\nefficient learning of the gating network, we develop a customized\nFlashAttention implementation that extracts the block-level ground truth of\nattention map with minimum overhead. SeerAttention not only applies to\npost-training, but also excels in long-context fine-tuning. Our results show\nthat at post-training stages, SeerAttention significantly outperforms\nstate-of-the-art static or heuristic-based sparse attention methods, while also\nbeing more versatile and flexible to adapt to varying context lengths and\nsparsity ratios. When applied to long-context fine-tuning with YaRN,\nSeerAttention can achieve a remarkable 90% sparsity ratio at a 32k context\nlength with minimal perplexity loss, offering a 5.67x speedup over\nFlashAttention-2.\n","authors":["Yizhao Gao","Zhichen Zeng","Dayou Du","Shijie Cao","Hayden Kwok-Hay So","Ting Cao","Fan Yang","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05541v2","updated":"2024-10-18T04:52:38Z","published":"2024-08-10T12:44:49Z","title":"P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for\n  data pruning in LLM Training","summary":"  In the rapidly advancing field of Large Language Models (LLMs), effectively\nleveraging existing datasets during fine-tuning to maximize the model's\npotential is of paramount importance. This paper introduces P3, an adaptive\nframework aimed at optimizing the task-specific fine-tuning process through\niterative data pruning. P3 consists of three key components: (1) Policy-driven\nDifficulty Measurement, which dynamically assesses data difficulty based on the\nmodel's real-time performance, replacing static metrics with adaptable\nevaluations; (2) Pace-Adaptive Selection, leveraging self-paced learning to\nprogressively introduce more challenging data, thereby enhancing model\ncapability; (3) Diversity Promotion, incorporating Determinantal Point Process\n(DPP) to ensure data diversity across epochs, enriching the learning process.\nWe validate P3 on the reasoning scenarios, APPS and MATH, demonstrating\nsignificant improvements over traditional data pruning methods. By advancing\ndynamic data selection and utilization strategies, P3 contributes both a\ntheoretical framework and concrete approach to fully exploit existing data for\nLLMs' performance improvement, offering utility across diverse tasks.\n","authors":["Yingxuan Yang","Huayi Wang","Muning Wen","Xiaoyun Mo","Qiuying Peng","Jun Wang","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.05541v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14862v4","updated":"2024-10-18T04:39:35Z","published":"2024-06-21T04:39:03Z","title":"LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models","summary":"  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\n\\textit{LatentExplainer}, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\n\\textit{LatentExplainer} tackles three main challenges: inferring the meaning\nof latent variables, aligning explanations with inductive biases, and handling\nvarying degrees of explainability. Our approach perturbs latent variables,\ninterpreting changes in generated data, and uses multi-modal large language\nmodels (MLLMs) to produce human-understandable explanations. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations for\nlatent variables. The results highlight the effectiveness of incorporating\ninductive biases and uncertainty quantification, significantly enhancing model\ninterpretability.\n","authors":["Mengdan Zhu","Raasikh Kanjiani","Jiahui Lu","Andrew Choi","Qirui Ye","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.14862v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15820v2","updated":"2024-10-18T04:38:47Z","published":"2024-09-24T07:34:50Z","title":"Supervised Fine-Tuning Achieve Rapid Task Adaption Via Alternating\n  Attention Head Activation Patterns","summary":"  LLMs' performance on complex tasks is still unsatisfactory. A key issue is\nthat presently LLMs learn in a data-driven schema, while the instructions about\nthese complex tasks are both scarce and hard to collect or construct. On the\ncontrary, a prominent phenomenon is that LLMs can learn rather fast on simpler\ntasks with adequate prior knowledge captured during pretraining stage. Thus, if\nthe prerequisite and mechanism of such rapid generalization could be\nelucidated, it could enhance the efficiency and effectiveness of the LLM's\nability to learn complex tasks. Thus, in this paper, we employ a gradient-based\nmethod, to dissect the process that the SFT process adapts LLMs to downstream\ntasks via the perspective of attention patterns. We find that: (1) LLMs\nselectively activate task-specific attention heads during SFT; (2) activation\npatterns for complex tasks are combinations of basic task patterns; and (3)\nchanges in a few parameters can significantly impact activation patterns after\nSFT on a small number of samples.Based on these insights, experiments are\nconducted to actually enhance the efficiency and effectiveness of SFT.\n","authors":["Yang Zhao","Li Du","Xiao Ding","Kai Xiong","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2409.15820v2.pdf","comment":"in review"},{"id":"http://arxiv.org/abs/2407.00902v2","updated":"2024-10-18T04:37:33Z","published":"2024-07-01T01:57:21Z","title":"From Introspection to Best Practices: Principled Analysis of\n  Demonstrations in Multimodal In-Context Learning","summary":"  Motivated by in-context learning (ICL) capabilities of Large Language models\n(LLMs), multimodal LLMs with additional visual modality are also exhibited with\nsimilar ICL abilities when multiple image-text pairs are provided as\ndemonstrations. However, relatively less work has been done to investigate the\nprinciples behind how and why multimodal ICL works. We conduct a systematic and\nprincipled evaluation of multimodal ICL for models of different scales on a\nbroad spectrum of new yet critical tasks. Through perturbations over different\nmodality information, we show that modalities matter differently across tasks\nin multimodal ICL. Guided by task-specific modality impact, we recommend\nmodality-driven demonstration strategies to boost ICL performance. We also find\nthat models may follow inductive biases from multimodal ICL even if they are\nrarely seen in or contradict semantic priors from pretraining data. Our\nprincipled analysis provides a comprehensive way of understanding the role of\ndemonstrations in multimodal in-context learning, and sheds light on\neffectively improving multimodal ICL on a wide range of tasks.\n","authors":["Nan Xu","Fei Wang","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.00902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15349v2","updated":"2024-10-18T04:32:49Z","published":"2024-05-24T08:42:40Z","title":"Everything is Editable: Extend Knowledge Editing to Unstructured Data in\n  Large Language Models","summary":"  Recent knowledge editing methods have primarily focused on modifying\nstructured knowledge in large language models. However, this task setting\noverlooks the fact that a significant portion of real-world knowledge is stored\nin an unstructured format, characterized by long-form content, noise, and a\ncomplex yet comprehensive nature. Techniques like local layer key-value storage\nand term-driven optimization, as used in previous methods like MEMIT, are not\neffective for handling unstructured knowledge. To address these challenges, we\npropose a novel Unstructured Knowledge Editing method, namely UnKE, which\nextends previous assumptions in the layer dimension and token dimension.\nFirstly, in the layer dimension, we propose non-local block key-value storage\nto replace local layer key-value storage, increasing the representation ability\nof key-value pairs and incorporating attention layer knowledge. Secondly, in\nthe token dimension, we replace term-driven optimization with cause-driven\noptimization, which edits the last token directly while preserving context,\navoiding the need to locate terms and preventing the loss of context\ninformation. Results on newly proposed unstructured knowledge editing dataset\n(UnKEBench) and traditional structured datasets demonstrate that UnKE achieves\nremarkable performance, surpassing strong baselines. In addition, UnKE has\nrobust batch editing and sequential editing capabilities.\n","authors":["Jingcheng Deng","Zihao Wei","Liang Pang","Hanxing Ding","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.15349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14166v1","updated":"2024-10-18T04:17:16Z","published":"2024-10-18T04:17:16Z","title":"LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with\n  Simple Word-based Counting Problems","summary":"  Interestingly, LLMs yet struggle with some basic tasks that humans find\ntrivial to handle, e.g., counting the number of character r's in the word\n\"strawberry\". There are several popular conjectures (e.g., tokenization,\narchitecture and training data) regarding the reason for deficiency of LLMs in\nsimple word-based counting problems, sharing the similar belief that such\nfailure stems from model pretraining hence probably inevitable during\ndeployment. In this paper, we carefully design multiple evaluation settings to\ninvestigate validity of prevalent conjectures. Meanwhile, we measure\ntransferability of advanced mathematical and coding reasoning capabilities from\nspecialized LLMs to simple counting tasks. Although specialized LLMs suffer\nfrom counting problems as well, we find conjectures about inherent deficiency\nof LLMs invalid and further seek opportunities to elicit knowledge and\ncapabilities from LLMs that are beneficial to counting tasks. Compared with\nstrategies such as finetuning and in-context learning that are commonly adopted\nto enhance performance on new or challenging tasks, we show that engaging\nreasoning is the most robust and efficient way to help LLMs better perceive\ntasks with more accurate responses.\n  We hope our conjecture validation design could provide insights into the\nstudy of future critical failure modes of LLMs. Based on challenges in\ntransferring advanced capabilities to much simpler tasks, we call for more\nattention to model capability acquisition and evaluation. We also highlight the\nimportance of cultivating consciousness of \"reasoning before responding\" during\nmodel pretraining.\n","authors":["Nan Xu","Xuezhe Ma"],"pdf_url":"https://arxiv.org/pdf/2410.14166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14165v1","updated":"2024-10-18T04:13:51Z","published":"2024-10-18T04:13:51Z","title":"Automated Genre-Aware Article Scoring and Feedback Using Large Language\n  Models","summary":"  This paper focuses on the development of an advanced intelligent article\nscoring system that not only assesses the overall quality of written work but\nalso offers detailed feature-based scoring tailored to various article genres.\nBy integrating the pre-trained BERT model with the large language model\nChat-GPT, the system gains a deep understanding of both the content and\nstructure of the text, enabling it to provide a thorough evaluation along with\ntargeted suggestions for improvement. Experimental results demonstrate that\nthis system outperforms traditional scoring methods across multiple public\ndatasets, particularly in feature-based assessments, offering a more accurate\nreflection of the quality of different article types. Moreover, the system\ngenerates personalized feedback to assist users in enhancing their writing\nskills, underscoring the potential and practical value of automated scoring\ntechnologies in educational contexts.\n","authors":["Chihang Wang","Yuxin Dong","Zhenhong Zhang","Ruotong Wang","Shuo Wang","Jiajing Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13170v2","updated":"2024-10-18T04:13:05Z","published":"2024-06-19T02:53:39Z","title":"Amphista: Bi-directional Multi-head Decoding for Accelerating LLM\n  Inference","summary":"  Large Language Models (LLMs) inherently use autoregressive decoding, which\nlacks parallelism in inference and results in significantly slow inference\nspeed. While methods such as Medusa constructs parallelized heads, they lack\nadequate information interaction across different prediction positions. To\novercome this limitation, we introduce Amphista, an enhanced speculative\ndecoding framework that builds upon Medusa. Specifically, Amphista models an\nAuto-embedding Block capable of parallel inference, incorporating\nbi-directional attention to enable interaction between different drafting\nheads. Additionally, Amphista integrates Staged Adaptation Layers, which ensure\na seamless transition of semantic information from the target model's\nautoregressive inference to the drafting heads' non-autoregressive inference,\neffectively achieving paradigm shift and feature fusion. Experimental results\non Vicuna models using MT-Bench and Spec-Bench demonstrate that Amphista\nachieves substantial acceleration while maintaining generation quality. On\nMT-Bench, Amphista delivers up to 2.75$\\times$ speedup over vanilla\nautoregressive decoding and 1.40$\\times$ over Medusa on Vicuna 33B in\nwall-clock time.\n","authors":["Zeping Li","Xinlong Yang","Ziheng Gao","Ji Liu","Guanchen Li","Zhuang Liu","Dong Li","Jinzhang Peng","Lu Tian","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2406.13170v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16710v4","updated":"2024-10-18T04:02:31Z","published":"2024-04-25T16:20:23Z","title":"LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding","summary":"  We present LayerSkip, an end-to-end solution to speed-up inference of large\nlanguage models (LLMs). First, during training we apply layer dropout, with low\ndropout rates for earlier layers and higher dropout rates for later layers, and\nan early exit loss where all transformer layers share the same exit. Second,\nduring inference, we show that this training recipe increases the accuracy of\nearly exit at earlier layers, without adding any auxiliary layers or modules to\nthe model. Third, we present a novel self-speculative decoding solution where\nwe exit at early layers and verify and correct with remaining layers of the\nmodel. Our proposed self-speculative decoding approach has less memory\nfootprint than other speculative decoding approaches and benefits from shared\ncompute and activations of the draft and verification stages. We run\nexperiments on different Llama model sizes on different types of training:\npretraining from scratch, continual pretraining, finetuning on specific data\ndomain, and finetuning on specific task. We implement our inference solution\nand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x\non coding, and 2.0x on TOPv2 semantic parsing task. We open source our code and\ncheckpoints at https://github.com/facebookresearch/LayerSkip.\n","authors":["Mostafa Elhoushi","Akshat Shrivastava","Diana Liskovich","Basil Hosmer","Bram Wasti","Liangzhen Lai","Anas Mahmoud","Bilge Acun","Saurabh Agarwal","Ahmed Roman","Ahmed A Aly","Beidi Chen","Carole-Jean Wu"],"pdf_url":"https://arxiv.org/pdf/2404.16710v4.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2410.14157v1","updated":"2024-10-18T03:48:53Z","published":"2024-10-18T03:48:53Z","title":"Beyond Autoregression: Discrete Diffusion for Complex Reasoning and\n  Planning","summary":"  Autoregressive language models, despite their impressive capabilities,\nstruggle with complex reasoning and long-term planning tasks. We introduce\ndiscrete diffusion models as a novel solution to these challenges. Through the\nlens of subgoal imbalance, we demonstrate how diffusion models effectively\nlearn difficult subgoals that elude autoregressive approaches. We propose\nMulti-granularity Diffusion Modeling (MDM), which prioritizes subgoals based on\ndifficulty during learning. On complex tasks like Countdown, Sudoku, and\nBoolean Satisfiability Problems, MDM significantly outperforms autoregressive\nmodels without using search techniques. For instance, MDM achieves 91.5\\% and\n100\\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\\% and\n20.7\\% for autoregressive models. Our work highlights the potential of\ndiffusion-based approaches in advancing AI capabilities for sophisticated\nlanguage understanding and problem-solving tasks.\n","authors":["Jiacheng Ye","Jiahui Gao","Shansan Gong","Lin Zheng","Xin Jiang","Zhenguo Li","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2410.14157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14155v1","updated":"2024-10-18T03:45:42Z","published":"2024-10-18T03:45:42Z","title":"Towards Faithful Natural Language Explanations: A Study Using Activation\n  Patching in Large Language Models","summary":"  Large Language Models (LLMs) are capable of generating persuasive Natural\nLanguage Explanations (NLEs) to justify their answers. However, the\nfaithfulness of these explanations should not be readily trusted at face value.\nRecent studies have proposed various methods to measure the faithfulness of\nNLEs, typically by inserting perturbations at the explanation or feature level.\nWe argue that these approaches are neither comprehensive nor correctly designed\naccording to the established definition of faithfulness. Moreover, we highlight\nthe risks of grounding faithfulness findings on out-of-distribution samples. In\nthis work, we leverage a causal mediation technique called activation patching,\nto measure the faithfulness of an explanation towards supporting the explained\nanswer. Our proposed metric, Causal Faithfulness quantifies the consistency of\ncausal attributions between explanations and the corresponding model outputs as\nthe indicator of faithfulness. We experimented across models varying from 2B to\n27B parameters and found that models that underwent alignment tuning tend to\nproduce more faithful and plausible explanations. We find that Causal\nFaithfulness is a promising improvement over existing faithfulness tests by\ntaking into account the model's internal computations and avoiding out of\ndistribution concerns that could otherwise undermine the validity of\nfaithfulness assessments. We release the code in\n\\url{https://github.com/wj210/Causal-Faithfulness}\n","authors":["Wei Jie Yeo","Ranjan Satapthy","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2410.14155v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.14152v1","updated":"2024-10-18T03:43:42Z","published":"2024-10-18T03:43:42Z","title":"SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy\n  with LLM-based Agent","summary":"  Public scarce resource allocation plays a crucial role in economics as it\ndirectly influences the efficiency and equity in society. Traditional studies\nincluding theoretical model-based, empirical study-based and simulation-based\nmethods encounter limitations due to the idealized assumption of complete\ninformation and individual rationality, as well as constraints posed by limited\navailable data. In this work, we propose an innovative framework, SRAP-Agent\n(Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based\nAgent), which integrates Large Language Models (LLMs) into economic\nsimulations, aiming to bridge the gap between theoretical models and real-world\ndynamics. Using public housing allocation scenarios as a case study, we conduct\nextensive policy simulation experiments to verify the feasibility and\neffectiveness of the SRAP-Agent and employ the Policy Optimization Algorithm\nwith certain optimization objectives. The source code can be found in\nhttps://github.com/jijiarui-cather/SRAPAgent_Framework\n","authors":["Jiarui Ji","Yang Li","Hongtao Liu","Zhicheng Du","Zhewei Wei","Weiran Shen","Qi Qi","Yankai Lin"],"pdf_url":"https://arxiv.org/pdf/2410.14152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14150v1","updated":"2024-10-18T03:40:45Z","published":"2024-10-18T03:40:45Z","title":"Utilizing Large Language Models for Event Deconstruction to Enhance\n  Multimodal Aspect-Based Sentiment Analysis","summary":"  With the rapid development of the internet, the richness of User-Generated\nContentcontinues to increase, making Multimodal Aspect-Based Sentiment Analysis\n(MABSA) a research hotspot. Existing studies have achieved certain results in\nMABSA, but they have not effectively addressed the analytical challenges in\nscenarios where multiple entities and sentiments coexist. This paper\ninnovatively introduces Large Language Models (LLMs) for event decomposition\nand proposes a reinforcement learning framework for Multimodal Aspect-based\nSentiment Analysis (MABSA-RL) framework. This framework decomposes the original\ntext into a set of events using LLMs, reducing the complexity of analysis,\nintroducing reinforcement learning to optimize model parameters. Experimental\nresults show that MABSA-RL outperforms existing advanced methods on two\nbenchmark datasets. This paper provides a new research perspective and method\nfor multimodal aspect-level sentiment analysis.\n","authors":["Xiaoyong Huang","Heli Sun","Qunshu Gao","Wenjie Huang","Ruichen Cao"],"pdf_url":"https://arxiv.org/pdf/2410.14150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12151v2","updated":"2024-10-18T03:36:03Z","published":"2024-08-22T06:40:32Z","title":"A Tighter Complexity Analysis of SparseGPT","summary":"  In this work, we improved the analysis of the running time of SparseGPT\n[Frantar, Alistarh ICML 2023] from $O(d^{3})$ to $O(d^{\\omega} + d^{2+a+o(1)} +\nd^{1+\\omega(1,1,a)-a})$ for any $a \\in [0, 1]$, where $\\omega$ is the exponent\nof matrix multiplication. In particular, for the current $\\omega \\approx 2.371$\n[Alman, Duan, Williams, Xu, Xu, Zhou 2024], our running time boils down to\n$O(d^{2.53})$. This running time is due to the analysis of the lazy update\nbehavior in iterative maintenance problems such as [Deng, Song, Weinstein 2022;\nBrand, Song, Zhou ICML 2024].\n","authors":["Xiaoyu Li","Yingyu Liang","Zhenmei Shi","Zhao Song"],"pdf_url":"https://arxiv.org/pdf/2408.12151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14148v1","updated":"2024-10-18T03:34:32Z","published":"2024-10-18T03:34:32Z","title":"Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment","summary":"  The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.\n","authors":["Chenhang Cui","An Zhang","Yiyang Zhou","Zhaorun Chen","Gelei Deng","Huaxiu Yao","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.14148v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2402.13606v3","updated":"2024-10-18T03:34:12Z","published":"2024-02-21T08:20:06Z","title":"A Comprehensive Study of Multilingual Confidence Estimation on Large\n  Language Models","summary":"  The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks.\n","authors":["Boyang Xue","Hongru Wang","Rui Wang","Sheng Wang","Zezhong Wang","Yiming Du","Bin Liang","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2402.13606v3.pdf","comment":"Comments: n pages; Previously this version appeared as\n  arXiv:2410.12478 which was submitted as a new work by accident"},{"id":"http://arxiv.org/abs/2410.14145v1","updated":"2024-10-18T03:33:18Z","published":"2024-10-18T03:33:18Z","title":"CAPE: A Chinese Dataset for Appraisal-based Emotional Generation using\n  Large Language Models","summary":"  Generating emotionally appropriate responses in conversations with large\nlanguage models presents a significant challenge due to the complexities of\nhuman emotions and cognitive processes, which remain largely underexplored in\ntheir critical role in social interactions. In this study, we introduce a\ntwo-stage automatic data generation framework to create CAPE, a Chinese dataset\nnamed Cognitive Appraisal theory-based Emotional corpus. This corpus\nfacilitates the generation of dialogues with contextually appropriate emotional\nresponses by accounting for diverse personal and situational factors. We\npropose two tasks utilizing this dataset: emotion prediction and next utterance\nprediction. Both automated and human evaluations demonstrate that agents\ntrained on our dataset can deliver responses that are more aligned with human\nemotional expressions. Our study shows the potential for advancing emotional\nexpression in conversational agents, paving the way for more nuanced and\nmeaningful human-computer interactions.\n","authors":["June M. Liu","He Cao","Renliang Sun","Rui Wang","Yu Li","Jiaxing Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14144v1","updated":"2024-10-18T03:32:00Z","published":"2024-10-18T03:32:00Z","title":"A Lightweight Multi Aspect Controlled Text Generation Solution For Large\n  Language Models","summary":"  Large language models (LLMs) show remarkable abilities with instruction\ntuning. However, they fail to achieve ideal tasks when lacking high-quality\ninstruction tuning data on target tasks. Multi-Aspect Controllable Text\nGeneration (MCTG) is a representative task for this dilemma, where aspect\ndatasets are usually biased and correlated. Existing work exploits additional\nmodel structures and strategies for solutions, limiting adaptability to LLMs.\nTo activate MCTG ability of LLMs, we propose a lightweight MCTG pipeline based\non data augmentation. We analyze bias and correlations in traditional datasets,\nand address these concerns with augmented control attributes and sentences.\nAugmented datasets are feasible for instruction tuning. In our experiments,\nLLMs perform better in MCTG after data augmentation, with a 20% accuracy rise\nand less aspect correlations.\n","authors":["Chenyang Zhang","Jiayi Lin","Haibo Tong","Bingxuan Hou","Dongyu Zhang","Jialin Li","Junli Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02052v3","updated":"2024-10-18T03:27:37Z","published":"2024-10-02T21:42:35Z","title":"ExACT: Teaching AI Agents to Explore with Reflective-MCTS and\n  Exploratory Learning","summary":"  Autonomous agents have demonstrated significant potential in automating\ncomplex multistep decision-making tasks. However, even state-of-the-art\nvision-language models (VLMs), such as GPT-4o, still fall short of human-level\nperformance, particularly in intricate web environments and long-horizon tasks.\nTo address these limitations, we present ExACT, an approach to combine\ntest-time search and self-learning to build o1-like models for agentic\napplications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a\nnovel test time algorithm designed to enhance AI agents' ability to explore\ndecision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating\ncontrastive reflection, allowing agents to learn from past interactions and\ndynamically improve their search efficiency; and 2) using multi-agent debate\nfor reliable state evaluation. Next, we introduce Exploratory Learning, a novel\nlearning strategy to teach agents to search at inference time without relying\non any external search algorithms. On the challenging VisualWebArena benchmark,\nour GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across\nvarious tasks compared to the previous state-of-the-art. Additionally, we show\nthat the knowledge and experience gained from test-time search can be\neffectively transferred back to GPT-4o via fine-tuning. After Exploratory\nLearning, GPT-4o 1) demonstrates the ability to explore the environment,\nevaluate a state, and backtrack to viable ones when it detects that the current\nstate cannot lead to success, and 2) matches 87% of R-MCTS's performance while\nusing significantly less compute. Notably, our work demonstrates the compute\nscaling properties in both training - data collection with R-MCTS - and testing\ntime. These results suggest a promising research direction to enhance VLMs'\ncapabilities for agentic applications via test-time search and self-learning.\n","authors":["Xiao Yu","Baolin Peng","Vineeth Vajipey","Hao Cheng","Michel Galley","Jianfeng Gao","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02052v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14141v1","updated":"2024-10-18T03:26:06Z","published":"2024-10-18T03:26:06Z","title":"Coherence-Driven Multimodal Safety Dialogue with Active Learning for\n  Embodied Agents","summary":"  When assisting people in daily tasks, robots need to accurately interpret\nvisual cues and respond effectively in diverse safety-critical situations, such\nas sharp objects on the floor. In this context, we present M-CoDAL, a\nmultimodal-dialogue system specifically designed for embodied agents to better\nunderstand and communicate in safety-critical situations. The system leverages\ndiscourse coherence relations to enhance its contextual understanding and\ncommunication abilities. To train this system, we introduce a novel\nclustering-based active learning mechanism that utilizes an external Large\nLanguage Model (LLM) to identify informative instances. Our approach is\nevaluated using a newly created multimodal dataset comprising 1K safety\nviolations extracted from 2K Reddit images. These violations are annotated\nusing a Large Multimodal Model (LMM) and verified by human annotators. Results\nwith this dataset demonstrate that our approach improves resolution of safety\nsituations, user sentiment, as well as safety of the conversation. Next, we\ndeploy our dialogue system on a Hello Robot Stretch robot and conduct a\nwithin-subject user study with real-world participants. In the study,\nparticipants role-play two safety scenarios with different levels of severity\nwith the robot and receive interventions from our model and a baseline system\npowered by OpenAI's ChatGPT. The study results corroborate and extend the\nfindings from automated evaluation, showing that our proposed system is more\npersuasive and competent in a real-world embodied agent setting.\n","authors":["Sabit Hassan","Hye-Young Chung","Xiang Zhi Tan","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2410.14141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12478v2","updated":"2024-10-18T03:19:51Z","published":"2024-10-16T11:46:55Z","title":"MlingConf: A Comprehensive Study of Multilingual Confidence Estimation\n  on Large Language Models","summary":"  The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluate high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy on LS tasks.\n","authors":["Boyang Xue","Hongru Wang","Rui Wang","Sheng Wang","Zezhong Wang","Yiming Du","Bin Liang","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2410.12478v2.pdf","comment":"Comments: This work was intended as a replacement of arXiv:2402.13606\n  and any subsequent updates will appear there"},{"id":"http://arxiv.org/abs/2410.04717v3","updated":"2024-10-18T03:18:50Z","published":"2024-10-07T03:15:11Z","title":"$\\textbf{Only-IF}$:Revealing the Decisive Effect of Instruction\n  Diversity on Generalization","summary":"  Understanding and accurately following instructions is critical for large\nlanguage models (LLMs) to be effective across diverse tasks. In this work, we\nrigorously examine the key factors that enable models to generalize to unseen\ninstructions, providing insights to guide the collection of data for\ninstruction-tuning. Through controlled experiments, inspired by the\nTuring-complete Markov algorithm, we demonstrate that such generalization\n$\\textbf{only emerges}$ when training data is diversified enough across\nsemantic domains. Our findings also reveal that merely diversifying within\nlimited domains fails to ensure robust generalization. In contrast,\ncross-domain data diversification, even under constrained data budgets,\nsignificantly enhances a model's adaptability. We further extend our analysis\nto real-world scenarios, including fine-tuning of\n$\\textit{$\\textbf{specialist}$}$ and $\\textit{$\\textbf{generalist}$}$ models.\nIn both cases, we demonstrate that 1) better performance can be achieved by\nincreasing the diversity of an established dataset while keeping the data size\nconstant, and 2) when scaling up the data, diversifying the semantics of\ninstructions is more effective than simply increasing the quantity of similar\ndata. Our research provides important insights for dataset collation,\nparticularly when optimizing model performance by expanding training data for\nboth specialist and generalist scenarios. We show that careful consideration of\ndata diversification is key: training specialist models with data extending\nbeyond their core domain leads to significant performance improvements, while\ngeneralist models benefit from diverse data mixtures that enhance their overall\ninstruction-following capabilities across a wide range of applications. Our\nresults highlight the critical role of strategic diversification and offer\nclear guidelines for improving data quality.\n","authors":["Dylan Zhang","Justin Wang","Francois Charton"],"pdf_url":"https://arxiv.org/pdf/2410.04717v3.pdf","comment":"Fix formatting issues"},{"id":"http://arxiv.org/abs/2410.13804v2","updated":"2024-10-18T03:15:21Z","published":"2024-10-17T17:41:15Z","title":"BenTo: Benchmark Task Reduction with In-Context Transferability","summary":"  Evaluating large language models (LLMs) is costly: it requires the generation\nand examination of LLM outputs on a large-scale benchmark of various tasks.\nThis paper investigates how to efficiently reduce the tasks used to benchmark\nLLMs without affecting the evaluation quality. Our study reveals that task\ntransferability and relevance provide critical information to identify the most\nrepresentative subset of tasks via optimizing a facility location function. We\npropose a practically efficient metric for estimating the transferability\nbetween two tasks via in-context learning (ICL). By analyzing the pairwise\ntransferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or\nFLAN) to 5% while inducing only a <4% difference to the evaluation on the\noriginal benchmark. Compared to prior works, our method is training-free,\ngradient-free, and highly efficient requiring ICL only.\n","authors":["Hongyu Zhao","Ming Li","Lichao Sun","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.13804v2.pdf","comment":"https://github.com/tianyi-lab/bento"},{"id":"http://arxiv.org/abs/2409.03258v2","updated":"2024-10-18T03:11:28Z","published":"2024-09-05T05:34:16Z","title":"GraphInsight: Unlocking Insights in Large Language Models for Graph\n  Structure Understanding","summary":"  Although Large Language Models (LLMs) have demonstrated potential in\nprocessing graphs, they struggle with comprehending graphical structure\ninformation through prompts of graph description sequences, especially as the\ngraph size increases. We attribute this challenge to the uneven memory\nperformance of LLMs across different positions in graph description sequences,\nknown as ''positional biases''. To address this, we propose GraphInsight, a\nnovel framework aimed at improving LLMs' comprehension of both macro- and\nmicro-level graphical information. GraphInsight is grounded in two key\nstrategies: 1) placing critical graphical information in positions where LLMs\nexhibit stronger memory performance, and 2) investigating a lightweight\nexternal knowledge base for regions with weaker memory performance, inspired by\nretrieval-augmented generation (RAG). Moreover, GraphInsight explores\nintegrating these two strategies into LLM agent processes for composite graph\ntasks that require multi-step reasoning. Extensive empirical studies on\nbenchmarks with a wide range of evaluation tasks show that GraphInsight\nsignificantly outperforms all other graph description methods (e.g., prompting\ntechniques and reordering strategies) in understanding graph structures of\nvarying sizes.\n","authors":["Yukun Cao","Shuo Han","Zengyi Gao","Zezhong Ding","Xike Xie","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.03258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13960v3","updated":"2024-10-18T03:10:13Z","published":"2024-06-20T03:02:38Z","title":"AutoPal: Autonomous Adaptation to Users for Personal AI Companionship","summary":"  Previous research has demonstrated the potential of AI agents to act as\ncompanions that can provide constant emotional support for humans. In this\npaper, we emphasize the necessity of autonomous adaptation in personal AI\ncompanionship, an underexplored yet promising direction. Such adaptability is\ncrucial as it can facilitate more tailored interactions with users and allow\nthe agent to evolve in response to users' changing needs. However, imbuing\nagents with autonomous adaptability presents unique challenges, including\nidentifying optimal adaptations to meet users' expectations and ensuring a\nsmooth transition during the adaptation process. To address them, we devise a\nhierarchical framework, AutoPal, that enables controllable and authentic\nadjustments to the agent's persona based on user interactions. A\npersonamatching dataset is constructed to facilitate the learning of optimal\npersona adaptations. Extensive experiments demonstrate the effectiveness of\nAutoPal and highlight the importance of autonomous adaptability in AI\ncompanionship.\n","authors":["Yi Cheng","Wenge Liu","Kaishuai Xu","Wenjun Hou","Yi Ouyang","Chak Tou Leong","Xian Wu","Yefeng Zheng"],"pdf_url":"https://arxiv.org/pdf/2406.13960v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07825v3","updated":"2024-10-18T03:06:39Z","published":"2024-03-12T17:04:28Z","title":"Efficiently Quantifying and Mitigating Ripple Effects in Model Editing","summary":"  Large Language Models have revolutionized numerous tasks with their\nremarkable efficacy. However, editing these models, crucial for rectifying\noutdated or erroneous information, often leads to a complex issue known as the\nripple effect in the hidden space. While difficult to detect, this effect can\nsignificantly impede the efficacy of model editing tasks and deteriorate model\nperformance. This paper addresses this scientific challenge by proposing a\nnovel evaluation methodology, Graphical Impact Evaluation(GIE), which\nquantitatively evaluates the adaptations of the model and the subsequent impact\nof editing. Furthermore, we introduce the Selective Impact Revision(SIR), a\nmodel editing method designed to mitigate this ripple effect. Our comprehensive\nevaluations reveal that the ripple effect in the hidden space is a significant\nissue in all current model editing methods. However, our proposed methods, GIE\nand SIR, effectively identify and alleviate this issue, contributing to the\nadvancement of LLM editing techniques.\n","authors":["Jianchen Wang","Zhouhong Gu","Xiaoxuan Zhu","Lin Zhang","Haoning Ye","Zhuozhi Xiong","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.07825v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13408v2","updated":"2024-10-18T03:05:01Z","published":"2024-10-17T10:14:52Z","title":"MoR: Mixture of Ranks for Low-Rank Adaptation Tuning","summary":"  Low-Rank Adaptation (LoRA) drives research to align its performance with full\nfine-tuning. However, significant challenges remain: (1) Simply increasing the\nrank size of LoRA does not effectively capture high-rank information, which\nleads to a performance bottleneck.(2) MoE-style LoRA methods substantially\nincrease parameters and inference latency, contradicting the goals of efficient\nfine-tuning and ease of application. To address these challenges, we introduce\nMixture of Ranks (MoR), which learns rank-specific information for different\ntasks based on input and efficiently integrates multi-rank information. We\nfirstly propose a new framework that equates the integration of multiple LoRAs\nto expanding the rank of LoRA. Moreover, we hypothesize that low-rank LoRA\nalready captures sufficient intrinsic information, and MoR can derive high-rank\ninformation through mathematical transformations of the low-rank components.\nThus, MoR can reduces the learning difficulty of LoRA and enhances its\nmulti-task capabilities. MoR achieves impressive results, with MoR delivering a\n1.31\\% performance improvement while using only 93.93\\% of the parameters\ncompared to baseline methods.\n","authors":["Chuanyu Tang","Yilong Chen","Zhenyu Zhang","Junyuan Shang","Wenyuan Zhang","Yong Huang","Tingwen Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13408v2.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.12841v2","updated":"2024-10-18T03:03:01Z","published":"2024-10-09T17:33:15Z","title":"UniAutoML: A Human-Centered Framework for Unified Discriminative and\n  Generative AutoML with Large Language Models","summary":"  Automated Machine Learning (AutoML) has simplified complex ML processes such\nas data pre-processing, model selection, and hyper-parameter searching.\nHowever, traditional AutoML frameworks focus solely on discriminative tasks,\noften falling short in tackling AutoML for generative models. Additionally,\nthese frameworks lack interpretability and user engagement during the training\nprocess, primarily due to the absence of human-centered design. It leads to a\nlack of transparency in final decision-making and limited user control,\npotentially reducing trust and adoption of AutoML methods. To address these\nlimitations, we introduce UniAutoML, a human-centered AutoML framework that\nleverages Large Language Models (LLMs) to unify AutoML for both discriminative\n(e.g., Transformers and CNNs for classification or regression tasks) and\ngenerative tasks (e.g., fine-tuning diffusion models or LLMs). The\nhuman-centered design of UniAutoML innovatively features a conversational user\ninterface (CUI) that facilitates natural language interactions, providing users\nwith real-time guidance, feedback, and progress updates for better\ninterpretability. This design enhances transparency and user control throughout\nthe AutoML training process, allowing users to seamlessly break down or modify\nthe model being trained. To mitigate potential risks associated with LLM\ngenerated content, UniAutoML incorporates a safety guardline that filters\ninputs and censors outputs. We evaluated UniAutoML's performance and usability\nthrough experiments on eight diverse datasets and user studies involving 25\nparticipants, demonstrating that UniAutoML not only enhances performance but\nalso improves user control and trust. Our human-centered design bridges the gap\nbetween AutoML capabilities and user understanding, making ML more accessible\nto a broader audience.\n","authors":["Jiayi Guo","Zan Chen","Yingrui Ji","Liyun Zhang","Daqin Luo","Zhigang Li","Yiqin Shen"],"pdf_url":"https://arxiv.org/pdf/2410.12841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14132v1","updated":"2024-10-18T03:00:03Z","published":"2024-10-18T03:00:03Z","title":"ViConsFormer: Constituting Meaningful Phrases of Scene Texts using\n  Transformer-based Method in Vietnamese Text-based Visual Question Answering","summary":"  Text-based VQA is a challenging task that requires machines to use scene\ntexts in given images to yield the most appropriate answer for the given\nquestion. The main challenge of text-based VQA is exploiting the meaning and\ninformation from scene texts. Recent studies tackled this challenge by\nconsidering the spatial information of scene texts in images via embedding 2D\ncoordinates of their bounding boxes. In this study, we follow the definition of\nmeaning from linguistics to introduce a novel method that effectively exploits\nthe information from scene texts written in Vietnamese. Experimental results\nshow that our proposed method obtains state-of-the-art results on two\nlarge-scale Vietnamese Text-based VQA datasets. The implementation can be found\nat this link.\n","authors":["Nghia Hieu Nguyen","Tho Thanh Quan","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.14132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12847v2","updated":"2024-10-18T02:56:32Z","published":"2024-10-10T07:48:53Z","title":"ACCEPT: Adaptive Codebook for Composite and Efficient Prompt Tuning","summary":"  Prompt Tuning has been a popular Parameter-Efficient Fine-Tuning method\nattributed to its remarkable performance with few updated parameters on various\nlarge-scale pretrained Language Models (PLMs). Traditionally, each prompt has\nbeen considered indivisible and updated independently, leading the parameters\nincrease proportionally as prompt length grows. To address this issue, we\npropose Adaptive Codebook for Composite and Efficient Prompt Tuning (ACCEPT).\nIn our method, we refer to the concept of product quantization (PQ), allowing\nall soft prompts to share a set of learnable codebook vectors in each subspace,\nwith each prompt differentiated by a set of adaptive weights. We achieve the\nsuperior performance on 17 diverse natural language tasks including natural\nlanguage understanding (NLU) and question answering (QA) tasks by tuning only\n0.3% of parameters of the PLMs. Our approach also excels in few-shot and large\nmodel settings, highlighting its significant potential.\n","authors":["Yu-Chen Lin","Wei-Hua Li","Jun-Cheng Chen","Chu-Song Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12847v2.pdf","comment":"EMNLP Findings 2024"},{"id":"http://arxiv.org/abs/2406.05213v2","updated":"2024-10-18T02:55:27Z","published":"2024-06-07T18:54:40Z","title":"On Subjective Uncertainty Quantification and Calibration in Natural\n  Language Generation","summary":"  Applications of large language models often involve the generation of\nfree-form responses, in which case uncertainty quantification becomes\nchallenging. This is due to the need to identify task-specific uncertainties\n(e.g., about the semantics) which appears difficult to define in general cases.\nThis work addresses these challenges from a perspective of Bayesian decision\ntheory, starting from the assumption that our utility is characterized by a\nsimilarity measure that compares a generated response with a hypothetical true\nresponse. We discuss how this assumption enables principled quantification of\nthe model's subjective uncertainty and its calibration. We further derive a\nmeasure for epistemic uncertainty, based on a missing data perspective and its\ncharacterization as an excess risk. The proposed methods can be applied to\nblack-box language models. We illustrate the methods on question answering and\nmachine translation tasks. Our experiments provide a principled evaluation of\ntask-specific calibration, and demonstrate that epistemic uncertainty offers a\npromising deferral strategy for efficient data acquisition in in-context\nlearning.\n","authors":["Ziyu Wang","Chris Holmes"],"pdf_url":"https://arxiv.org/pdf/2406.05213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13166v2","updated":"2024-10-18T02:53:14Z","published":"2024-10-17T02:47:10Z","title":"An Evolved Universal Transformer Memory","summary":"  Prior methods propose to offset the escalating costs of modern foundation\nmodels by dropping specific parts of their contexts with hand-designed rules,\nwhile attempting to preserve their original performance. We overcome this\ntrade-off with Neural Attention Memory Models (NAMMs), introducing a learned\nnetwork for memory management that improves both the performance and efficiency\nof transformers. We evolve NAMMs atop pre-trained transformers to provide\ndifferent latent contexts focusing on the most relevant information for\nindividual layers and attention heads. NAMMs are universally applicable to any\nmodel using self-attention as they condition exclusively on the values in the\nproduced attention matrices. Learning NAMMs on a small set of problems, we\nachieve substantial performance improvements across multiple long-context\nbenchmarks while cutting the model's input contexts up to a fraction of the\noriginal sizes. We show the generality of our conditioning enables zero-shot\ntransfer of NAMMs trained only on language to entirely new transformer\narchitectures even across input modalities, with their benefits carrying over\nto vision and reinforcement learning.\n","authors":["Edoardo Cetin","Qi Sun","Tianyu Zhao","Yujin Tang"],"pdf_url":"https://arxiv.org/pdf/2410.13166v2.pdf","comment":"29 pages, 14 figures. Preprint, under submission. Source code is\n  available at https://github.com/SakanaAI/evo-memory"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.14672v1","updated":"2024-10-18T17:59:04Z","published":"2024-10-18T17:59:04Z","title":"BiGR: Harnessing Binary Latent Codes for Image Generation and Improved\n  Visual Representation Capabilities","summary":"  We introduce BiGR, a novel conditional image generation model using compact\nbinary latent codes for generative training, focusing on enhancing both\ngeneration and representation capabilities. BiGR is the first conditional\ngenerative model that unifies generation and discrimination within the same\nframework. BiGR features a binary tokenizer, a masked modeling mechanism, and a\nbinary transcoder for binary code prediction. Additionally, we introduce a\nnovel entropy-ordered sampling method to enable efficient image generation.\nExtensive experiments validate BiGR's superior performance in generation\nquality, as measured by FID-50k, and representation capabilities, as evidenced\nby linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization\nacross various vision tasks, enabling applications such as image inpainting,\noutpainting, editing, interpolation, and enrichment, without the need for\nstructural modifications. Our findings suggest that BiGR unifies generative and\ndiscriminative tasks effectively, paving the way for further advancements in\nthe field.\n","authors":["Shaozhe Hao","Xuantong Liu","Xianbiao Qi","Shihao Zhao","Bojia Zi","Rong Xiao","Kai Han","Kwan-Yee K. Wong"],"pdf_url":"https://arxiv.org/pdf/2410.14672v1.pdf","comment":"Project page: https://haoosz.github.io/BiGR"},{"id":"http://arxiv.org/abs/2410.14669v1","updated":"2024-10-18T17:58:21Z","published":"2024-10-18T17:58:21Z","title":"NaturalBench: Evaluating Vision-Language Models on Natural Adversarial\n  Samples","summary":"  Vision-language models (VLMs) have made significant progress in recent\nvisual-question-answering (VQA) benchmarks that evaluate complex\nvisio-linguistic reasoning. However, are these models truly effective? In this\nwork, we show that VLMs still struggle with natural images and questions that\nhumans can easily answer, which we term natural adversarial samples. We also\nfind it surprisingly easy to generate these VQA samples from natural image-text\ncorpora using off-the-shelf models like CLIP and ChatGPT. We propose a\nsemi-automated approach to collect a new benchmark, NaturalBench, for reliably\nevaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a\n$\\textbf{vision-centric}$ design by pairing each question with two images that\nyield different answers, preventing blind solutions from answering without\nusing the images. This makes NaturalBench more challenging than previous\nbenchmarks that can be solved with commonsense priors. We evaluate 53\nstate-of-the-art VLMs on NaturalBench, showing that models like\nLLaVA-OneVision, Cambrian-1, Llama3.2-Vision, Molmo, Qwen2-VL, and even GPT-4o\nlag 50%-70% behind human performance (over 90%). We analyze why NaturalBench is\nhard from two angles: (1) Compositionality: Solving NaturalBench requires\ndiverse visio-linguistic skills, including understanding attribute bindings,\nobject relationships, and advanced reasoning like logic and counting. To this\nend, unlike prior work that uses a single tag per sample, we tag each\nNaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2)\nBiases: NaturalBench exposes severe biases in VLMs, as models often choose the\nsame answer regardless of the image. Lastly, we apply our benchmark curation\nmethod to diverse data sources, including long captions (over 100 words) and\nnon-English languages like Chinese and Hindi, highlighting its potential for\ndynamic evaluations of VLMs.\n","authors":["Baiqi Li","Zhiqiu Lin","Wenxuan Peng","Jean de Dieu Nyandwi","Daniel Jiang","Zixian Ma","Simran Khanuja","Ranjay Krishna","Graham Neubig","Deva Ramanan"],"pdf_url":"https://arxiv.org/pdf/2410.14669v1.pdf","comment":"Accepted to NeurIPS 24; We open-source our dataset at:\n  https://huggingface.co/datasets/BaiqiL/NaturalBench; Project page at:\n  https://linzhiqiu.github.io/papers/naturalbench/"},{"id":"http://arxiv.org/abs/2409.06445v2","updated":"2024-10-18T17:37:51Z","published":"2024-09-10T12:00:40Z","title":"Learning Generative Interactive Environments By Trained Agent\n  Exploration","summary":"  World models are increasingly pivotal in interpreting and simulating the\nrules and actions of complex environments. Genie, a recent model, excels at\nlearning from visually diverse environments but relies on costly\nhuman-collected data. We observe that their alternative method of using random\nagents is too limited to explore the environment. We propose to improve the\nmodel by employing reinforcement learning based agents for data generation.\nThis approach produces diverse datasets that enhance the model's ability to\nadapt and perform well across various scenarios and realistic actions within\nthe environment. In this paper, we first release the model GenieRedux - an\nimplementation based on Genie. Additionally, we introduce GenieRedux-G, a\nvariant that uses the agent's readily available actions to factor out action\nprediction uncertainty during validation. Our evaluation, including a\nreplication of the Coinrun case study, shows that GenieRedux-G achieves\nsuperior visual fidelity and controllability using the trained agent\nexploration. The proposed approach is reproducable, scalable and adaptable to\nnew types of environments. Our codebase is available at\nhttps://github.com/insait-institute/GenieRedux .\n","authors":["Naser Kazemi","Nedko Savov","Danda Paudel","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2409.06445v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14634v1","updated":"2024-10-18T17:35:33Z","published":"2024-10-18T17:35:33Z","title":"Parallel Backpropagation for Inverse of a Convolution with Application\n  to Normalizing Flows","summary":"  Inverse of an invertible convolution is an important operation that comes up\nin Normalizing Flows, Image Deblurring, etc. The naive algorithm for\nbackpropagation of this operation using Gaussian elimination has running time\n$O(n^3)$ where $n$ is the number of pixels in the image. We give a fast\nparallel backpropagation algorithm with running time $O(\\sqrt{n})$ for a square\nimage and provide a GPU implementation of the same. Inverse Convolutions are\nusually used in Normalizing Flows in the sampling pass, making them slow. We\npropose to use Inverse Convolutions in the forward (image to latent vector)\npass of the Normalizing flow. Since the sampling pass is the inverse of the\nforward pass, it will use convolutions only, resulting in efficient sampling\ntimes. We use our parallel backpropagation algorithm for optimizing the inverse\nconvolution layer resulting in fast training times also. We implement this\napproach in various Normalizing Flow backbones, resulting in our Inverse-Flow\nmodels. We benchmark Inverse-Flow on standard datasets and show significantly\nimproved sampling times with similar bits per dimension compared to previous\nmodels.\n","authors":["Sandeep Nagar","Girish Varma"],"pdf_url":"https://arxiv.org/pdf/2410.14634v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.14633v1","updated":"2024-10-18T17:32:39Z","published":"2024-10-18T17:32:39Z","title":"Swiss Army Knife: Synergizing Biases in Knowledge from Vision Foundation\n  Models for Multi-Task Learning","summary":"  Vision Foundation Models (VFMs) have demonstrated outstanding performance on\nnumerous downstream tasks. However, due to their inherent representation biases\noriginating from different training paradigms, VFMs exhibit advantages and\ndisadvantages across distinct vision tasks. Although amalgamating the strengths\nof multiple VFMs for downstream tasks is an intuitive strategy, effectively\nexploiting these biases remains a significant challenge. In this paper, we\npropose a novel and versatile \"Swiss Army Knife\" (SAK) solution, which\nadaptively distills knowledge from a committee of VFMs to enhance multi-task\nlearning. Unlike existing methods that use a single backbone for knowledge\ntransfer, our approach preserves the unique representation bias of each teacher\nby collaborating the lightweight Teacher-Specific Adapter Path modules with the\nTeacher-Agnostic Stem. Through dynamic selection and combination of\nrepresentations with Mixture-of-Representations Routers, our SAK is capable of\nsynergizing the complementary strengths of multiple VFMs. Extensive experiments\nshow that our SAK remarkably outperforms prior state of the arts in multi-task\nlearning by 10% on the NYUD-v2 benchmark, while also providing a flexible and\nrobust framework that can readily accommodate more advanced model designs.\n","authors":["Yuxiang Lu","Shengcao Cao","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01804v4","updated":"2024-10-18T17:20:20Z","published":"2024-10-02T17:59:09Z","title":"EVER: Exact Volumetric Ellipsoid Rendering for Real-time View Synthesis","summary":"  We present Exact Volumetric Ellipsoid Rendering (EVER), a method for\nreal-time differentiable emission-only volume rendering. Unlike recent\nrasterization based approach by 3D Gaussian Splatting (3DGS), our primitive\nbased representation allows for exact volume rendering, rather than alpha\ncompositing 3D Gaussian billboards. As such, unlike 3DGS our formulation does\nnot suffer from popping artifacts and view dependent density, but still\nachieves frame rates of $\\sim\\!30$ FPS at 720p on an NVIDIA RTX4090. Since our\napproach is built upon ray tracing it enables effects such as defocus blur and\ncamera distortion (e.g. such as from fisheye cameras), which are difficult to\nachieve by rasterization. We show that our method is more accurate with fewer\nblending issues than 3DGS and follow-up work on view-consistent rendering,\nespecially on the challenging large-scale scenes from the Zip-NeRF dataset\nwhere it achieves sharpest results among real-time techniques.\n","authors":["Alexander Mai","Peter Hedman","George Kopanas","Dor Verbin","David Futschik","Qiangeng Xu","Falko Kuester","Jonathan T. Barron","Yinda Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.01804v4.pdf","comment":"Project page: https://half-potato.gitlab.io/posts/ever"},{"id":"http://arxiv.org/abs/2410.14612v1","updated":"2024-10-18T17:05:03Z","published":"2024-10-18T17:05:03Z","title":"MultiOrg: A Multi-rater Organoid-detection Dataset","summary":"  High-throughput image analysis in the biomedical domain has gained\nsignificant attention in recent years, driving advancements in drug discovery,\ndisease prediction, and personalized medicine. Organoids, specifically, are an\nactive area of research, providing excellent models for human organs and their\nfunctions. Automating the quantification of organoids in microscopy images\nwould provide an effective solution to overcome substantial manual\nquantification bottlenecks, particularly in high-throughput image analysis.\nHowever, there is a notable lack of open biomedical datasets, in contrast to\nother domains, such as autonomous driving, and, notably, only few of them have\nattempted to quantify annotation uncertainty. In this work, we present MultiOrg\na comprehensive organoid dataset tailored for object detection tasks with\nuncertainty quantification. This dataset comprises over 400 high-resolution 2d\nmicroscopy images and curated annotations of more than 60,000 organoids. Most\nimportantly, it includes three label sets for the test data, independently\nannotated by two experts at distinct time points. We additionally provide a\nbenchmark for organoid detection, and make the best model available through an\neasily installable, interactive plugin for the popular image visualization tool\nNapari, to perform organoid quantification.\n","authors":["Christina Bukas","Harshavardhan Subramanian","Fenja See","Carina Steinchen","Ivan Ezhov","Gowtham Boosarpu","Sara Asgharpour","Gerald Burgstaller","Mareike Lehmann","Florian Kofler","Marie Piraud"],"pdf_url":"https://arxiv.org/pdf/2410.14612v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14595v1","updated":"2024-10-18T16:48:31Z","published":"2024-10-18T16:48:31Z","title":"DRACO-DehazeNet: An Efficient Image Dehazing Network Combining Detail\n  Recovery and a Novel Contrastive Learning Paradigm","summary":"  Image dehazing is crucial for clarifying images obscured by haze or fog, but\ncurrent learning-based approaches is dependent on large volumes of training\ndata and hence consumed significant computational power. Additionally, their\nperformance is often inadequate under non-uniform or heavy haze. To address\nthese challenges, we developed the Detail Recovery And Contrastive DehazeNet,\nwhich facilitates efficient and effective dehazing via a dense dilated inverted\nresidual block and an attention-based detail recovery network that tailors\nenhancements to specific dehazed scene contexts. A major innovation is its\nability to train effectively with limited data, achieved through a novel\nquadruplet loss-based contrastive dehazing paradigm. This approach distinctly\nseparates hazy and clear image features while also distinguish lower-quality\nand higher-quality dehazed images obtained from each sub-modules of our\nnetwork, thereby refining the dehazing process to a larger extent. Extensive\ntests on a variety of benchmarked haze datasets demonstrated the superiority of\nour approach. The code repository for this work will be available soon.\n","authors":["Gao Yu Lee","Tanmoy Dam","Md Meftahul Ferdaus","Daniel Puiu Poenar","Vu Duong"],"pdf_url":"https://arxiv.org/pdf/2410.14595v1.pdf","comment":"Submitted to a journal and currently under review. Once the paper is\n  accepted and published, the copyright will be transferred to the\n  corresponding journal"},{"id":"http://arxiv.org/abs/2404.13370v2","updated":"2024-10-18T16:44:05Z","published":"2024-04-20T13:15:27Z","title":"Movie101v2: Improved Movie Narration Benchmark","summary":"  Automatic movie narration aims to generate video-aligned plot descriptions to\nassist visually impaired audiences. Unlike standard video captioning, it\ninvolves not only describing key visual details but also inferring plots that\nunfold across multiple movie shots, presenting distinct and complex challenges.\nTo advance this field, we introduce Movie101v2, a large-scale, bilingual\ndataset with enhanced data quality specifically designed for movie narration.\nRevisiting the task, we propose breaking down the ultimate goal of automatic\nmovie narration into three progressive stages, offering a clear roadmap with\ncorresponding evaluation metrics. Based on our new benchmark, we baseline a\nrange of large vision-language models, including GPT-4V, and conduct an\nin-depth analysis of the challenges in narration generation. Our findings\nhighlight that achieving applicable movie narration generation is a fascinating\ngoal that requires significant research.\n","authors":["Zihao Yue","Yepeng Zhang","Ziheng Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2404.13370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.17777v2","updated":"2024-10-18T16:31:49Z","published":"2024-09-26T12:15:13Z","title":"Harnessing Shared Relations via Multimodal Mixup Contrastive Learning\n  for Multimodal Classification","summary":"  Deep multimodal learning has shown remarkable success by leveraging\ncontrastive learning to capture explicit one-to-one relations across\nmodalities. However, real-world data often exhibits shared relations beyond\nsimple pairwise associations. We propose M3CoL, a Multimodal Mixup Contrastive\nLearning approach to capture nuanced shared relations inherent in multimodal\ndata. Our key contribution is a Mixup-based contrastive loss that learns robust\nrepresentations by aligning mixed samples from one modality with their\ncorresponding samples from other modalities thereby capturing shared relations\nbetween them. For multimodal classification tasks, we introduce a framework\nthat integrates a fusion module with unimodal prediction modules for auxiliary\nsupervision during training, complemented by our proposed Mixup-based\ncontrastive loss. Through extensive experiments on diverse datasets (N24News,\nROSMAP, BRCA, and Food-101), we demonstrate that M3CoL effectively captures\nshared multimodal relations and generalizes across domains. It outperforms\nstate-of-the-art methods on N24News, ROSMAP, and BRCA, while achieving\ncomparable performance on Food-101. Our work highlights the significance of\nlearning shared relations for robust multimodal learning, opening up promising\navenues for future research.\n","authors":["Raja Kumar","Raghav Singhal","Pranamya Kulkarni","Deval Mehta","Kshitij Jadhav"],"pdf_url":"https://arxiv.org/pdf/2409.17777v2.pdf","comment":"RK and RS contributed equally to this work, 20 Pages, 8 Figures, 9\n  Tables. Another version of the paper accepted at NeurIPS 2024 Workshop on\n  Unifying Representations in Neural Models (UniReps)"},{"id":"http://arxiv.org/abs/2410.08107v2","updated":"2024-10-18T16:26:30Z","published":"2024-10-10T16:54:23Z","title":"IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera","summary":"  Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for\nnovel view synthesis have achieved remarkable progress with frame-based camera\n(e.g. RGB and RGB-D cameras) recently. Compared to frame-based camera, a novel\ntype of bio-inspired visual sensor, i.e. event camera, has demonstrated\nadvantages in high temporal resolution, high dynamic range, low power\nconsumption and low latency. Due to its unique asynchronous and irregular data\ncapturing process, limited work has been proposed to apply neural\nrepresentation or 3D Gaussian splatting for an event camera. In this work, we\npresent IncEventGS, an incremental 3D Gaussian Splatting reconstruction\nalgorithm with a single event camera. To recover the 3D scene representation\nincrementally, we exploit the tracking and mapping paradigm of conventional\nSLAM pipelines for IncEventGS. Given the incoming event stream, the tracker\nfirstly estimates an initial camera motion based on prior reconstructed 3D-GS\nscene representation. The mapper then jointly refines both the 3D scene\nrepresentation and camera motion based on the previously estimated motion\ntrajectory from the tracker. The experimental results demonstrate that\nIncEventGS delivers superior performance compared to prior NeRF-based methods\nand other related baselines, even we do not have the ground-truth camera poses.\nFurthermore, our method can also deliver better performance compared to\nstate-of-the-art event visual odometry methods in terms of camera motion\nestimation. Code is publicly available at:\nhttps://github.com/wu-cvgl/IncEventGS.\n","authors":["Jian Huang","Chengrui Dong","Peidong Liu"],"pdf_url":"https://arxiv.org/pdf/2410.08107v2.pdf","comment":"Code Page: https://github.com/wu-cvgl/IncEventGS"},{"id":"http://arxiv.org/abs/2410.13174v2","updated":"2024-10-18T16:26:30Z","published":"2024-10-17T02:57:35Z","title":"Scalable Drift Monitoring in Medical Imaging AI","summary":"  The integration of artificial intelligence (AI) into medical imaging has\nadvanced clinical diagnostics but poses challenges in managing model drift and\nensuring long-term reliability. To address these challenges, we develop MMC+,\nan enhanced framework for scalable drift monitoring, building upon the\nCheXstray framework that introduced real-time drift detection for medical\nimaging AI models using multi-modal data concordance. This work extends the\noriginal framework's methodologies, providing a more scalable and adaptable\nsolution for real-world healthcare settings and offers a reliable and\ncost-effective alternative to continuous performance monitoring addressing\nlimitations of both continuous and periodic monitoring methods. MMC+ introduces\ncritical improvements to the original framework, including more robust handling\nof diverse data streams, improved scalability with the integration of\nfoundation models like MedImageInsight for high-dimensional image embeddings\nwithout site-specific training, and the introduction of uncertainty bounds to\nbetter capture drift in dynamic clinical environments. Validated with\nreal-world data from Massachusetts General Hospital during the COVID-19\npandemic, MMC+ effectively detects significant data shifts and correlates them\nwith model performance changes. While not directly predicting performance\ndegradation, MMC+ serves as an early warning system, indicating when AI systems\nmay deviate from acceptable performance bounds and enabling timely\ninterventions. By emphasizing the importance of monitoring diverse data streams\nand evaluating data shifts alongside model performance, this work contributes\nto the broader adoption and integration of AI solutions in clinical settings.\n","authors":["Jameson Merkow","Felix J. Dorfner","Xiyu Yang","Alexander Ersoy","Giridhar Dasegowda","Mannudeep Kalra","Matthew P. Lungren","Christopher P. Bridge","Ivan Tarapov"],"pdf_url":"https://arxiv.org/pdf/2410.13174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14574v1","updated":"2024-10-18T16:20:22Z","published":"2024-10-18T16:20:22Z","title":"MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts","summary":"  Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled\nscalability in deep learning. SMoE has the potential to exponentially increase\nparameter count while maintaining the efficiency of the model by only\nactivating a small subset of these parameters for a given sample. However, it\nhas been observed that SMoE suffers from unstable training and has difficulty\nadapting to new distributions, leading to the model's lack of robustness to\ndata contamination. To overcome these limitations, we first establish a\nconnection between the dynamics of the expert representations in SMoEs and\ngradient descent on a multi-objective optimization problem. Leveraging our\nframework, we then integrate momentum into SMoE and propose a new family of\nSMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate\nthat MomentumSMoE is more stable and robust than SMoE. In particular, we verify\nthe advantages of MomentumSMoE over SMoE on a variety of practical tasks\nincluding ImageNet-1K object recognition and WikiText-103 language modeling. We\ndemonstrate the applicability of MomentumSMoE to many types of SMoE models,\nincluding those in the Sparse MoE model for vision (V-MoE) and the Generalist\nLanguage Model (GLaM). We also show that other advanced momentum-based\noptimization methods, such as Adam, can be easily incorporated into the\nMomentumSMoE framework for designing new SMoE models with even better\nperformance, almost negligible additional computation cost, and simple\nimplementations.\n","authors":["Rachel S. Y. Teo","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.14574v1.pdf","comment":"10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https://github.com/rachtsy/MomentumSMoE"},{"id":"http://arxiv.org/abs/2410.13242v2","updated":"2024-10-18T15:41:44Z","published":"2024-10-17T05:53:13Z","title":"Fundus to Fluorescein Angiography Video Generation as a Retinal\n  Generative Foundation Model","summary":"  Fundus fluorescein angiography (FFA) is crucial for diagnosing and monitoring\nretinal vascular issues but is limited by its invasive nature and restricted\naccessibility compared to color fundus (CF) imaging. Existing methods that\nconvert CF images to FFA are confined to static image generation, missing the\ndynamic lesional changes. We introduce Fundus2Video, an autoregressive\ngenerative adversarial network (GAN) model that generates dynamic FFA videos\nfrom single CF images. Fundus2Video excels in video generation, achieving an\nFVD of 1497.12 and a PSNR of 11.77. Clinical experts have validated the\nfidelity of the generated videos. Additionally, the model's generator\ndemonstrates remarkable downstream transferability across ten external public\ndatasets, including blood vessel segmentation, retinal disease diagnosis,\nsystemic disease prediction, and multimodal retrieval, showcasing impressive\nzero-shot and few-shot capabilities. These findings position Fundus2Video as a\npowerful, non-invasive alternative to FFA exams and a versatile retinal\ngenerative foundation model that captures both static and temporal retinal\nfeatures, enabling the representation of complex inter-modality relationships.\n","authors":["Weiyi Zhang","Jiancheng Yang","Ruoyu Chen","Siyu Huang","Pusheng Xu","Xiaolan Chen","Shanfu Lu","Hongyu Cao","Mingguang He","Danli Shi"],"pdf_url":"https://arxiv.org/pdf/2410.13242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14540v1","updated":"2024-10-18T15:29:19Z","published":"2024-10-18T15:29:19Z","title":"Multi-modal Pose Diffuser: A Multimodal Generative Conditional Pose\n  Prior","summary":"  The Skinned Multi-Person Linear (SMPL) model plays a crucial role in 3D human\npose estimation, providing a streamlined yet effective representation of the\nhuman body. However, ensuring the validity of SMPL configurations during tasks\nsuch as human mesh regression remains a significant challenge , highlighting\nthe necessity for a robust human pose prior capable of discerning realistic\nhuman poses. To address this, we introduce MOPED:\n\\underline{M}ulti-m\\underline{O}dal \\underline{P}os\\underline{E}\n\\underline{D}iffuser. MOPED is the first method to leverage a novel multi-modal\nconditional diffusion model as a prior for SMPL pose parameters. Our method\noffers powerful unconditional pose generation with the ability to condition on\nmulti-modal inputs such as images and text. This capability enhances the\napplicability of our approach by incorporating additional context often\noverlooked in traditional pose priors. Extensive experiments across three\ndistinct tasks-pose estimation, pose denoising, and pose completion-demonstrate\nthat our multi-modal diffusion model-based prior significantly outperforms\nexisting methods. These results indicate that our model captures a broader\nspectrum of plausible human poses.\n","authors":["Calvin-Khang Ta","Arindam Dutta","Rohit Kundu","Rohit Lal","Hannah Dela Cruz","Dripta S. Raychaudhuri","Amit Roy-Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2410.14540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14536v1","updated":"2024-10-18T15:23:34Z","published":"2024-10-18T15:23:34Z","title":"A Hybrid Feature Fusion Deep Learning Framework for Leukemia Cancer\n  Detection in Microscopic Blood Sample Using Gated Recurrent Unit and\n  Uncertainty Quantification","summary":"  Acute lymphoblastic leukemia (ALL) is the most malignant form of leukemia and\nthe most common cancer in adults and children. Traditionally, leukemia is\ndiagnosed by analyzing blood and bone marrow smears under a microscope, with\nadditional cytochemical tests for confirmation. However, these methods are\nexpensive, time consuming, and highly dependent on expert knowledge. In recent\nyears, deep learning, particularly Convolutional Neural Networks (CNNs), has\nprovided advanced methods for classifying microscopic smear images, aiding in\nthe detection of leukemic cells. These approaches are quick, cost effective,\nand not subject to human bias. However, most methods lack the ability to\nquantify uncertainty, which could lead to critical misdiagnoses. In this\nresearch, hybrid deep learning models (InceptionV3-GRU, EfficientNetB3-GRU,\nMobileNetV2-GRU) were implemented to classify ALL. Bayesian optimization was\nused to fine tune the model's hyperparameters and improve its performance.\nAdditionally, Deep Ensemble uncertainty quantification was applied to address\nuncertainty during leukemia image classification. The proposed models were\ntrained on the publicly available datasets ALL-IDB1 and ALL-IDB2. Their results\nwere then aggregated at the score level using the sum rule. The parallel\narchitecture used in these models offers a high level of confidence in\ndifferentiating between ALL and non-ALL cases. The proposed method achieved a\nremarkable detection accuracy rate of 100% on the ALL-IDB1 dataset, 98.07% on\nthe ALL-IDB2 dataset, and 98.64% on the combined dataset, demonstrating its\npotential for accurate and reliable leukemia diagnosis.\n","authors":["Maksuda Akter","Rabea Khatun","Md Manowarul Islam"],"pdf_url":"https://arxiv.org/pdf/2410.14536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14524v1","updated":"2024-10-18T15:08:05Z","published":"2024-10-18T15:08:05Z","title":"Less is More: Selective Reduction of CT Data for Self-Supervised\n  Pre-Training of Deep Learning Models with Contrastive Learning Improves\n  Downstream Classification Performance","summary":"  Self-supervised pre-training of deep learning models with contrastive\nlearning is a widely used technique in image analysis. Current findings\nindicate a strong potential for contrastive pre-training on medical images.\nHowever, further research is necessary to incorporate the particular\ncharacteristics of these images. We hypothesize that the similarity of medical\nimages hinders the success of contrastive learning in the medical imaging\ndomain. To this end, we investigate different strategies based on deep\nembedding, information theory, and hashing in order to identify and reduce\nredundancy in medical pre-training datasets. The effect of these different\nreduction strategies on contrastive learning is evaluated on two pre-training\ndatasets and several downstream classification tasks. In all of our\nexperiments, dataset reduction leads to a considerable performance gain in\ndownstream tasks, e.g., an AUC score improvement from 0.78 to 0.83 for the\nCOVID CT Classification Grand Challenge, 0.97 to 0.98 for the OrganSMNIST\nClassification Challenge and 0.73 to 0.83 for a brain hemorrhage classification\ntask. Furthermore, pre-training is up to nine times faster due to the dataset\nreduction. In conclusion, the proposed approach highlights the importance of\ndataset quality and provides a transferable approach to improve contrastive\npre-training for classification downstream tasks on medical images.\n","authors":["Daniel Wolf","Tristan Payer","Catharina Silvia Lisson","Christoph Gerhard Lisson","Meinrad Beer","Michael Götz","Timo Ropinski"],"pdf_url":"https://arxiv.org/pdf/2410.14524v1.pdf","comment":"Published in Computers in Biology and Medicine"},{"id":"http://arxiv.org/abs/2409.14485v3","updated":"2024-10-18T15:03:08Z","published":"2024-09-22T15:13:31Z","title":"Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding","summary":"  Although current Multi-modal Large Language Models (MLLMs) demonstrate\npromising results in video understanding, processing extremely long videos\nremains an ongoing challenge. Typically, MLLMs struggle with handling thousands\nof visual tokens that exceed the maximum context length, and they suffer from\nthe information decay due to token aggregation. Another challenge is the high\ncomputational cost stemming from the large number of video tokens. To tackle\nthese issues, we propose Video-XL, an extra-long vision language model designed\nfor efficient hour-scale video understanding. Specifically, we argue that LLMs\ncan be adapted as effective visual condensers and propose Visual Context Latent\nSummarization which condenses visual contexts into highly compact forms.\nExtensive experiments demonstrate that our model achieves promising results on\npopular long video understanding benchmarks. For example, Video-XL outperforms\nthe current state-of-the-art method on VNBench by nearly 10\\% in accuracy.\nMoreover, Video-XL presents an impressive balance between efficiency and\neffectiveness, processing 2048 frames on a single 80GB GPU while achieving\nnearly 95% accuracy in the Needle-in-a-Haystack evaluation.\n","authors":["Yan Shu","Peitian Zhang","Zheng Liu","Minghao Qin","Junjie Zhou","Tiejun Huang","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.14485v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14509v1","updated":"2024-10-18T14:43:34Z","published":"2024-10-18T14:43:34Z","title":"CLIP-VAD: Exploiting Vision-Language Models for Voice Activity Detection","summary":"  Voice Activity Detection (VAD) is the process of automatically determining\nwhether a person is speaking and identifying the timing of their speech in an\naudiovisual data. Traditionally, this task has been tackled by processing\neither audio signals or visual data, or by combining both modalities through\nfusion or joint learning. In our study, drawing inspiration from recent\nadvancements in visual-language models, we introduce a novel approach\nleveraging Contrastive Language-Image Pretraining (CLIP) models. The CLIP\nvisual encoder analyzes video segments composed of the upper body of an\nindividual, while the text encoder handles textual descriptions automatically\ngenerated through prompt engineering. Subsequently, embeddings from these\nencoders are fused through a deep neural network to perform VAD. Our\nexperimental analysis across three VAD benchmarks showcases the superior\nperformance of our method compared to existing visual VAD approaches. Notably,\nour approach outperforms several audio-visual methods despite its simplicity,\nand without requiring pre-training on extensive audio-visual datasets.\n","authors":["Andrea Appiani","Cigdem Beyan"],"pdf_url":"https://arxiv.org/pdf/2410.14509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14508v1","updated":"2024-10-18T14:43:05Z","published":"2024-10-18T14:43:05Z","title":"LEAD: Latent Realignment for Human Motion Diffusion","summary":"  Our goal is to generate realistic human motion from natural language. Modern\nmethods often face a trade-off between model expressiveness and text-to-motion\nalignment. Some align text and motion latent spaces but sacrifice\nexpressiveness; others rely on diffusion models producing impressive motions,\nbut lacking semantic meaning in their latent space. This may compromise\nrealism, diversity, and applicability. Here, we address this by combining\nlatent diffusion with a realignment mechanism, producing a novel, semantically\nstructured space that encodes the semantics of language. Leveraging this\ncapability, we introduce the task of textual motion inversion to capture novel\nmotion concepts from a few examples. For motion synthesis, we evaluate LEAD on\nHumanML3D and KIT-ML and show comparable performance to the state-of-the-art in\nterms of realism, diversity, and text-motion consistency. Our qualitative\nanalysis and user study reveal that our synthesized motions are sharper, more\nhuman-like and comply better with the text compared to modern methods. For\nmotion textual inversion, our method demonstrates improved capacity in\ncapturing out-of-distribution characteristics in comparison to traditional\nVAEs.\n","authors":["Nefeli Andreou","Xi Wang","Victoria Fernández Abrevaya","Marie-Paule Cani","Yiorgos Chrysanthou","Vicky Kalogeiton"],"pdf_url":"https://arxiv.org/pdf/2410.14508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04960v2","updated":"2024-10-18T14:42:50Z","published":"2024-10-07T11:59:54Z","title":"On Efficient Variants of Segment Anything Model: A Survey","summary":"  The Segment Anything Model (SAM) is a foundational model for image\nsegmentation tasks, known for its strong generalization across diverse\napplications. However, its impressive performance comes with significant\ncomputational and resource demands, making it challenging to deploy in\nresource-limited environments such as edge devices. To address this, a variety\nof SAM variants have been proposed to enhance efficiency while keeping\naccuracy. This survey provides the first comprehensive review of these\nefficient SAM variants. We begin by exploring the motivations driving this\nresearch. We then present core techniques used in SAM and model acceleration.\nThis is followed by a detailed exploration of SAM acceleration strategies,\ncategorized by approach, and a discussion of several future research\ndirections. Finally, we offer a unified and extensive evaluation of these\nmethods across various hardware, assessing their efficiency and accuracy on\nrepresentative benchmarks, and providing a clear comparison of their overall\nperformance.\n","authors":["Xiaorui Sun","Jun Liu","Heng Tao Shen","Xiaofeng Zhu","Ping Hu"],"pdf_url":"https://arxiv.org/pdf/2410.04960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07361v2","updated":"2024-10-18T14:38:03Z","published":"2024-06-11T15:28:48Z","title":"Deep Implicit Optimization for Robust and Flexible Image Registration","summary":"  Deep Learning in Image Registration (DLIR) methods have been tremendously\nsuccessful in image registration due to their speed and ability to incorporate\nweak label supervision at training time. However, DLIR methods forego many of\nthe benefits of classical optimization-based methods. The functional nature of\ndeep networks do not guarantee that the predicted transformation is a local\nminima of the registration objective, the representation of the transformation\n(displacement/velocity field/affine) is fixed, and the networks are not robust\nto domain shift. Our method aims to bridge this gap between classical and\nlearning methods by incorporating optimization as a layer in a deep network. A\ndeep network is trained to predict multi-scale dense feature images that are\nregistered using a black box iterative optimization solver. This optimal warp\nis then used to minimize image and label alignment errors. By implicitly\ndifferentiating end-to-end through an iterative optimization solver, our\nlearned features are registration and label-aware, and the warp functions are\nguaranteed to be local minima of the registration objective in the feature\nspace. Our framework shows excellent performance on in-domain datasets, and is\nagnostic to domain shift such as anisotropy and varying intensity profiles. For\nthe first time, our method allows switching between arbitrary transformation\nrepresentations (free-form to diffeomorphic) at test time with zero retraining.\nEnd-to-end feature learning also facilitates interpretability of features, and\nout-of-the-box promptability using additional label-fidelity terms at\ninference.\n","authors":["Rohit Jena","Pratik Chaudhari","James C. Gee"],"pdf_url":"https://arxiv.org/pdf/2406.07361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14505v1","updated":"2024-10-18T14:37:37Z","published":"2024-10-18T14:37:37Z","title":"Neural Real-Time Recalibration for Infrared Multi-Camera Systems","summary":"  Currently, there are no learning-free or neural techniques for real-time\nrecalibration of infrared multi-camera systems. In this paper, we address the\nchallenge of real-time, highly-accurate calibration of multi-camera infrared\nsystems, a critical task for time-sensitive applications. Unlike traditional\ncalibration techniques that lack adaptability and struggle with on-the-fly\nrecalibrations, we propose a neural network-based method capable of dynamic\nreal-time calibration. The proposed method integrates a differentiable\nprojection model that directly correlates 3D geometries with their 2D image\nprojections and facilitates the direct optimization of both intrinsic and\nextrinsic camera parameters. Key to our approach is the dynamic camera pose\nsynthesis with perturbations in camera parameters, emulating realistic\noperational challenges to enhance model robustness. We introduce two model\nvariants: one designed for multi-camera systems with onboard processing of 2D\npoints, utilizing the direct 2D projections of 3D fiducials, and another for\nimage-based systems, employing color-coded projected points for implicitly\nestablishing correspondence. Through rigorous experimentation, we demonstrate\nour method is more accurate than traditional calibration techniques with or\nwithout perturbations while also being real-time, marking a significant leap in\nthe field of real-time multi-camera system calibration. The source code can be\nfound at https://github.com/theICTlab/neural-recalibration\n","authors":["Benyamin Mehmandar","Reza Talakoob","Charalambos Poullis"],"pdf_url":"https://arxiv.org/pdf/2410.14505v1.pdf","comment":"real-time camera calibration, infrared camera, neural calibration"},{"id":"http://arxiv.org/abs/2410.14489v1","updated":"2024-10-18T14:19:13Z","published":"2024-10-18T14:19:13Z","title":"An Integrated Deep Learning Model for Skin Cancer Detection Using Hybrid\n  Feature Fusion Technique","summary":"  Skin cancer is a serious and potentially fatal disease caused by DNA damage.\nEarly detection significantly increases survival rates, making accurate\ndiagnosis crucial. In this groundbreaking study, we present a hybrid framework\nbased on Deep Learning (DL) that achieves precise classification of benign and\nmalignant skin lesions. Our approach begins with dataset preprocessing to\nenhance classification accuracy, followed by training two separate pre-trained\nDL models, InceptionV3 and DenseNet121. By fusing the results of each model\nusing the weighted sum rule, our system achieves exceptional accuracy rates.\nSpecifically, we achieve a 92.27% detection accuracy rate, 92.33% sensitivity,\n92.22% specificity, 90.81% precision, and 91.57% F1-score, outperforming\nexisting models and demonstrating the robustness and trustworthiness of our\nhybrid approach. Our study represents a significant advance in skin cancer\ndiagnosis and provides a promising foundation for further research in the\nfield. With the potential to save countless lives through earlier detection,\nour hybrid deep-learning approach is a game-changer in the fight against skin\ncancer.\n","authors":["Maksuda Akter","Rabea Khatun","Md. Alamin Talukder","Md. Manowarul Islam","Md. Ashraf Uddin"],"pdf_url":"https://arxiv.org/pdf/2410.14489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14470v1","updated":"2024-10-18T13:54:46Z","published":"2024-10-18T13:54:46Z","title":"How Do Training Methods Influence the Utilization of Vision Models?","summary":"  Not all learnable parameters (e.g., weights) contribute equally to a neural\nnetwork's decision function. In fact, entire layers' parameters can sometimes\nbe reset to random values with little to no impact on the model's decisions. We\nrevisit earlier studies that examined how architecture and task complexity\ninfluence this phenomenon and ask: is this phenomenon also affected by how we\ntrain the model? We conducted experimental evaluations on a diverse set of\nImageNet-1k classification models to explore this, keeping the architecture and\ntraining data constant but varying the training pipeline. Our findings reveal\nthat the training method strongly influences which layers become critical to\nthe decision function for a given task. For example, improved training regimes\nand self-supervised training increase the importance of early layers while\nsignificantly under-utilizing deeper layers. In contrast, methods such as\nadversarial training display an opposite trend. Our preliminary results extend\nprevious findings, offering a more nuanced understanding of the inner mechanics\nof neural networks.\n  Code: https://github.com/paulgavrikov/layer_criticality\n","authors":["Paul Gavrikov","Shashank Agnihotri","Margret Keuper","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2410.14470v1.pdf","comment":"Accepted at the Interpretable AI: Past, Present and Future Workshop\n  at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2403.09939v2","updated":"2024-10-18T13:51:50Z","published":"2024-03-15T00:43:03Z","title":"Quantization Effects on Neural Networks Perception: How would\n  quantization change the perceptual field of vision models?","summary":"  Neural network quantization is a critical technique for deploying models on\nresource-limited devices. Despite its widespread use, the impact of\nquantization on model perceptual fields, particularly in relation to class\nactivation maps (CAMs), remains underexplored. This study investigates how\nquantization influences the spatial recognition abilities of vision models by\nexamining the alignment between CAMs and visual salient objects maps across\nvarious architectures. Utilizing a dataset of 10,000 images from ImageNet, we\nconduct a comprehensive evaluation of six diverse CNN architectures: VGG16,\nResNet50, EfficientNet, MobileNet, SqueezeNet, and DenseNet. Through the\nsystematic application of quantization techniques, we identify subtle changes\nin CAMs and their alignment with Salient object maps. Our results demonstrate\nthe differing sensitivities of these architectures to quantization and\nhighlight its implications for model performance and interpretability in\nreal-world applications. This work primarily contributes to a deeper\nunderstanding of neural network quantization, offering insights essential for\ndeploying efficient and interpretable models in practical settings.\n","authors":["Mohamed Amine Kerkouri","Marouane Tliba","Aladine Chetouani","Alessandro Bruno"],"pdf_url":"https://arxiv.org/pdf/2403.09939v2.pdf","comment":"Accepted & presented at IPTA 2024"},{"id":"http://arxiv.org/abs/2410.14462v1","updated":"2024-10-18T13:44:29Z","published":"2024-10-18T13:44:29Z","title":"LUDVIG: Learning-free Uplifting of 2D Visual features to Gaussian\n  Splatting scenes","summary":"  We address the task of uplifting visual features or semantic masks from 2D\nvision models to 3D scenes represented by Gaussian Splatting. Whereas common\napproaches rely on iterative optimization-based procedures, we show that a\nsimple yet effective aggregation technique yields excellent results. Applied to\nsemantic masks from Segment Anything (SAM), our uplifting approach leads to\nsegmentation quality comparable to the state of the art. We then extend this\nmethod to generic DINOv2 features, integrating 3D scene geometry through graph\ndiffusion, and achieve competitive segmentation results despite DINOv2 not\nbeing trained on millions of annotated masks like SAM.\n","authors":["Juliette Marrie","Romain Ménégaux","Michael Arbel","Diane Larlus","Julien Mairal"],"pdf_url":"https://arxiv.org/pdf/2410.14462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19525v3","updated":"2024-10-18T13:13:44Z","published":"2024-04-30T12:56:14Z","title":"MicroDreamer: Efficient 3D Generation in $\\sim$20 Seconds by Score-based\n  Iterative Reconstruction","summary":"  Optimization-based approaches, such as score distillation sampling (SDS),\nshow promise in zero-shot 3D generation but suffer from low efficiency,\nprimarily due to the high number of function evaluations (NFEs) required for\neach sample and the limitation of optimization confined to latent space. This\npaper introduces score-based iterative reconstruction (SIR), an efficient and\ngeneral algorithm mimicking a differentiable 3D reconstruction process to\nreduce the NFEs and enable optimization in pixel space. Given a single set of\nimages sampled from a multi-view score-based diffusion model, SIR repeatedly\noptimizes 3D parameters, unlike the single-step optimization in SDS. With other\nimprovements in training, we present an efficient approach called MicroDreamer\nthat generally applies to various 3D representations and 3D generation tasks.\nIn particular, MicroDreamer is 5-20 times faster than SDS in generating neural\nradiance field while retaining a comparable performance and takes about 20\nseconds to create meshes from 3D Gaussian splatting on a single A100 GPU,\nhalving the time of the fastest optimization-based baseline DreamGaussian with\nsignificantly superior performance compared to the measurement standard\ndeviation. Our code is available at https://github.com/ML-GSAI/MicroDreamer.\n","authors":["Luxi Chen","Zhengyi Wang","Zihan Zhou","Tingting Gao","Hang Su","Jun Zhu","Chongxuan Li"],"pdf_url":"https://arxiv.org/pdf/2404.19525v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14445v1","updated":"2024-10-18T13:04:35Z","published":"2024-10-18T13:04:35Z","title":"Toward Generalizing Visual Brain Decoding to Unseen Subjects","summary":"  Visual brain decoding aims to decode visual information from human brain\nactivities. Despite the great progress, one critical limitation of current\nbrain decoding research lies in the lack of generalization capability to unseen\nsubjects. Prior works typically focus on decoding brain activity of individuals\nbased on the observation that different subjects exhibit different brain\nactivities, while it remains unclear whether brain decoding can be generalized\nto unseen subjects. This study aims to answer this question. We first\nconsolidate an image-fMRI dataset consisting of stimulus-image and\nfMRI-response pairs, involving 177 subjects in the movie-viewing task of the\nHuman Connectome Project (HCP). This dataset allows us to investigate the brain\ndecoding performance with the increase of participants. We then present a\nlearning paradigm that applies uniform processing across all subjects, instead\nof employing different network heads or tokenizers for individuals as in\nprevious methods, which can accommodate a large number of subjects to explore\nthe generalization capability across different subjects. A series of\nexperiments are conducted and we have the following findings. First, the\nnetwork exhibits clear generalization capabilities with the increase of\ntraining subjects. Second, the generalization capability is common to popular\nnetwork architectures (MLP, CNN and Transformer). Third, the generalization\nperformance is affected by the similarity between subjects. Our findings reveal\nthe inherent similarities in brain activities across individuals. With the\nemerging of larger and more comprehensive datasets, it is possible to train a\nbrain decoding foundation model in the future.Codes and models can be found at\nhttps://github.com/Xiangtaokong/TGBD.\n","authors":["Xiangtao Kong","Kexin Huang","Ping Li","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14429v1","updated":"2024-10-18T12:48:22Z","published":"2024-10-18T12:48:22Z","title":"FashionR2R: Texture-preserving Rendered-to-Real Image Translation with\n  Diffusion Models","summary":"  Modeling and producing lifelike clothed human images has attracted\nresearchers' attention from different areas for decades, with the complexity\nfrom highly articulated and structured content. Rendering algorithms decompose\nand simulate the imaging process of a camera, while are limited by the accuracy\nof modeled variables and the efficiency of computation. Generative models can\nproduce impressively vivid human images, however still lacking in\ncontrollability and editability. This paper studies photorealism enhancement of\nrendered images, leveraging generative power from diffusion models on the\ncontrolled basis of rendering. We introduce a novel framework to translate\nrendered images into their realistic counterparts, which consists of two\nstages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG).\nIn DKI, we adopt positive (real) domain finetuning and negative (rendered)\ndomain embedding to inject knowledge into a pretrained Text-to-image (T2I)\ndiffusion model. In RIG, we generate the realistic image corresponding to the\ninput rendered image, with a Texture-preserving Attention Control (TAC) to\npreserve fine-grained clothing textures, exploiting the decoupled features\nencoded in the UNet structure. Additionally, we introduce SynFashion dataset,\nfeaturing high-quality digital clothing images with diverse textures. Extensive\nexperimental results demonstrate the superiority and effectiveness of our\nmethod in rendered-to-real image translation.\n","authors":["Rui Hu","Qian He","Gaofeng He","Jiedong Zhuang","Huang Chen","Huafeng Liu","Huamin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14429v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.14423v1","updated":"2024-10-18T12:37:51Z","published":"2024-10-18T12:37:51Z","title":"Integrating Deep Learning with Fundus and Optical Coherence Tomography\n  for Cardiovascular Disease Prediction","summary":"  Early identification of patients at risk of cardiovascular diseases (CVD) is\ncrucial for effective preventive care, reducing healthcare burden, and\nimproving patients' quality of life. This study demonstrates the potential of\nretinal optical coherence tomography (OCT) imaging combined with fundus\nphotographs for identifying future adverse cardiac events. We used data from\n977 patients who experienced CVD within a 5-year interval post-image\nacquisition, alongside 1,877 control participants without CVD, totaling 2,854\nsubjects. We propose a novel binary classification network based on a\nMulti-channel Variational Autoencoder (MCVAE), which learns a latent embedding\nof patients' fundus and OCT images to classify individuals into two groups:\nthose likely to develop CVD in the future and those who are not. Our model,\ntrained on both imaging modalities, achieved promising results (AUROC 0.78 +/-\n0.02, accuracy 0.68 +/- 0.002, precision 0.74 +/- 0.02, sensitivity 0.73 +/-\n0.02, and specificity 0.68 +/- 0.01), demonstrating its efficacy in identifying\npatients at risk of future CVD events based on their retinal images. This study\nhighlights the potential of retinal OCT imaging and fundus photographs as\ncost-effective, non-invasive alternatives for predicting cardiovascular disease\nrisk. The widespread availability of these imaging techniques in optometry\npractices and hospitals further enhances their potential for large-scale CVD\nrisk screening. Our findings contribute to the development of standardized,\naccessible methods for early CVD risk identification, potentially improving\npreventive care strategies and patient outcomes.\n","authors":["Cynthia Maldonado-Garcia","Arezoo Zakeri","Alejandro F Frangi","Nishant Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2410.14423v1.pdf","comment":"Part of the book series: Lecture Notes in Computer Science\n  ((LNCS,volume 15155))"},{"id":"http://arxiv.org/abs/2405.08431v4","updated":"2024-10-18T12:25:31Z","published":"2024-05-14T08:51:16Z","title":"Similarity and Quality Metrics for MR Image-To-Image Translation","summary":"  Image-to-image translation can create large impact in medical imaging, as\nimages can be synthetically transformed to other modalities, sequence types,\nhigher resolutions or lower noise levels. To ensure patient safety, these\nmethods should be validated by human readers, which requires a considerable\namount of time and costs. Quantitative metrics can effectively complement such\nstudies and provide reproducible and objective assessment of synthetic images.\nIf a reference is available, the similarity of MR images is frequently\nevaluated by SSIM and PSNR metrics, even though these metrics are not or too\nsensitive regarding specific distortions. When reference images to compare with\nare not available, non-reference quality metrics can reliably detect specific\ndistortions, such as blurriness. To provide an overview on distortion\nsensitivity, we quantitatively analyze 11 similarity (reference) and 12 quality\n(non-reference) metrics for assessing synthetic images. We additionally include\na metric on a downstream segmentation task. We investigate the sensitivity\nregarding 11 kinds of distortions and typical MR artifacts, and analyze the\ninfluence of different normalization methods on each metric and distortion.\nFinally, we derive recommendations for effective usage of the analyzed\nsimilarity and quality metrics for evaluation of image-to-image translation\nmodels.\n","authors":["Melanie Dohmen","Mark Klemens","Ivo Baltruschat","Tuan Truong","Matthias Lenga"],"pdf_url":"https://arxiv.org/pdf/2405.08431v4.pdf","comment":"21 pages, 8 figures, supplement with 16 pages, 10 figures, submitted\n  to Nature Scientific Reports"},{"id":"http://arxiv.org/abs/2409.08031v2","updated":"2024-10-18T12:22:11Z","published":"2024-09-12T13:23:24Z","title":"LED: Light Enhanced Depth Estimation at Night","summary":"  Nighttime camera-based depth estimation is a highly challenging task,\nespecially for autonomous driving applications, where accurate depth perception\nis essential for ensuring safe navigation. We aim to improve the reliability of\nperception systems at night time, where models trained on daytime data often\nfail in the absence of precise but costly LiDAR sensors. In this work, we\nintroduce Light Enhanced Depth (LED), a novel cost-effective approach that\nsignificantly improves depth estimation in low-light environments by harnessing\na pattern projected by high definition headlights available in modern vehicles.\nLED leads to significant performance boosts across multiple depth-estimation\narchitectures (encoder-decoder, Adabins, DepthFormer) both on synthetic and\nreal datasets. Furthermore, increased performances beyond illuminated areas\nreveal a holistic enhancement in scene understanding. Finally, we release the\nNighttime Synthetic Drive Dataset, a new synthetic and photo-realistic\nnighttime dataset, which comprises 49,990 comprehensively annotated images.\n","authors":["Simon de Moreau","Yasser Almehio","Andrei Bursuc","Hafid El-Idrissi","Bogdan Stanciulescu","Fabien Moutarde"],"pdf_url":"https://arxiv.org/pdf/2409.08031v2.pdf","comment":"Preprint. Code and dataset available on the project page :\n  https://simondemoreau.github.io/LED/"},{"id":"http://arxiv.org/abs/2406.08552v2","updated":"2024-10-18T12:05:21Z","published":"2024-06-12T18:00:08Z","title":"DiTFastAttn: Attention Compression for Diffusion Transformer Models","summary":"  Diffusion Transformers (DiT) excel at image and video generation but face\ncomputational challenges due to the quadratic complexity of self-attention\noperators. We propose DiTFastAttn, a post-training compression method to\nalleviate the computational bottleneck of DiT. We identify three key\nredundancies in the attention computation during DiT inference: (1) spatial\nredundancy, where many attention heads focus on local information; (2) temporal\nredundancy, with high similarity between the attention outputs of neighboring\nsteps; (3) conditional redundancy, where conditional and unconditional\ninferences exhibit significant similarity. We propose three techniques to\nreduce these redundancies: (1) Window Attention with Residual Sharing to reduce\nspatial redundancy; (2) Attention Sharing across Timesteps to exploit the\nsimilarity between steps; (3) Attention Sharing across CFG to skip redundant\ncomputations during conditional generation. We apply DiTFastAttn to DiT,\nPixArt-Sigma for image generation tasks, and OpenSora for video generation\ntasks. Our results show that for image generation, our method reduces up to 76%\nof the attention FLOPs and achieves up to 1.8x end-to-end speedup at\nhigh-resolution (2k x 2k) generation.\n","authors":["Zhihang Yuan","Hanling Zhang","Pu Lu","Xuefei Ning","Linfeng Zhang","Tianchen Zhao","Shengen Yan","Guohao Dai","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2406.08552v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14400v1","updated":"2024-10-18T12:04:23Z","published":"2024-10-18T12:04:23Z","title":"Variable Aperture Bokeh Rendering via Customized Focal Plane Guidance","summary":"  Bokeh rendering is one of the most popular techniques in photography. It can\nmake photographs visually appealing, forcing users to focus their attentions on\nparticular area of image. However, achieving satisfactory bokeh effect usually\npresents significant challenge, since mobile cameras with restricted optical\nsystems are constrained, while expensive high-end DSLR lens with large aperture\nshould be needed. Therefore, many deep learning-based computational photography\nmethods have been developed to mimic the bokeh effect in recent years.\nNevertheless, most of these methods were limited to rendering bokeh effect in\ncertain single aperture. There lacks user-friendly bokeh rendering method that\ncan provide precise focal plane control and customised bokeh generation. There\nas well lacks authentic realistic bokeh dataset that can potentially promote\nbokeh learning on variable apertures. To address these two issues, in this\npaper, we have proposed an effective controllable bokeh rendering method, and\ncontributed a Variable Aperture Bokeh Dataset (VABD). In the proposed method,\nuser can customize focal plane to accurately locate concerned subjects and\nselect target aperture information for bokeh rendering. Experimental results on\npublic EBB! benchmark dataset and our constructed dataset VABD have\ndemonstrated that the customized focal plane together aperture prompt can\nbootstrap model to simulate realistic bokeh effect. The proposed method has\nachieved competitive state-of-the-art performance with only 4.4M parameters,\nwhich is much lighter than mainstream computational bokeh models. The\ncontributed dataset and source codes will be released on github\nhttps://github.com/MoTong-AI-studio/VABM.\n","authors":["Kang Chen","Shijun Yan","Aiwen Jiang","Han Li","Zhifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14398v1","updated":"2024-10-18T12:02:21Z","published":"2024-10-18T12:02:21Z","title":"Dynamic Negative Guidance of Diffusion Models","summary":"  Negative Prompting (NP) is widely utilized in diffusion models, particularly\nin text-to-image applications, to prevent the generation of undesired features.\nIn this paper, we show that conventional NP is limited by the assumption of a\nconstant guidance scale, which may lead to highly suboptimal results, or even\ncomplete failure, due to the non-stationarity and state-dependence of the\nreverse process. Based on this analysis, we derive a principled technique\ncalled Dynamic Negative Guidance, which relies on a near-optimal time and state\ndependent modulation of the guidance without requiring additional training.\nUnlike NP, negative guidance requires estimating the posterior class\nprobability during the denoising process, which is achieved with limited\nadditional computational overhead by tracking the discrete Markov Chain during\nthe generative process. We evaluate the performance of DNG class-removal on\nMNIST and CIFAR10, where we show that DNG leads to higher safety, preservation\nof class balance and image quality when compared with baseline methods.\nFurthermore, we show that it is possible to use DNG with Stable Diffusion to\nobtain more accurate and less invasive guidance than NP.\n","authors":["Felix Koulischer","Johannes Deleu","Gabriel Raya","Thomas Demeester","Luca Ambrogioni"],"pdf_url":"https://arxiv.org/pdf/2410.14398v1.pdf","comment":"Paper currently under review. Submitted to ICLR 2025"},{"id":"http://arxiv.org/abs/2410.14389v1","updated":"2024-10-18T11:49:40Z","published":"2024-10-18T11:49:40Z","title":"SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task\n  Learning with Deep Representation Surgery","summary":"  Model merging-based multitask learning (MTL) offers a promising approach for\nperforming MTL by merging multiple expert models without requiring access to\nraw training data. However, in this paper, we examine the merged model's\nrepresentation distribution and uncover a critical issue of \"representation\nbias\". This bias arises from a significant distribution gap between the\nrepresentations of the merged and expert models, leading to the suboptimal\nperformance of the merged MTL model. To address this challenge, we first\npropose a representation surgery solution called Surgery. Surgery is a\nlightweight, task-specific module that aligns the final layer representations\nof the merged model with those of the expert models, effectively alleviating\nbias and improving the merged model's performance. Despite these improvements,\na performance gap remains compared to the traditional MTL method. Further\nanalysis reveals that representation bias phenomena exist at each layer of the\nmerged model, and aligning representations only in the last layer is\ninsufficient for fully reducing systemic bias because biases introduced at each\nlayer can accumulate and interact in complex ways. To tackle this, we then\npropose a more comprehensive solution, deep representation surgery (also called\nSurgeryV2), which mitigates representation bias across all layers, and thus\nbridges the performance gap between model merging-based MTL and traditional\nMTL. Finally, we design an unsupervised optimization objective to optimize both\nthe Surgery and SurgeryV2 modules. Our experimental results show that\nincorporating these modules into state-of-the-art (SOTA) model merging schemes\nleads to significant performance gains. Notably, our SurgeryV2 scheme reaches\nalmost the same level as individual expert models or the traditional MTL model.\nThe code is available at \\url{https://github.com/EnnengYang/SurgeryV2}.\n","authors":["Enneng Yang","Li Shen","Zhenyi Wang","Guibing Guo","Xingwei Wang","Xiaocun Cao","Jie Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2410.14389v1.pdf","comment":"This paper is an extended version of our previous work\n  [arXiv:2402.02705] presented at ICML 2024"},{"id":"http://arxiv.org/abs/2410.14379v1","updated":"2024-10-18T11:07:12Z","published":"2024-10-18T11:07:12Z","title":"AnomalyNCD: Towards Novel Anomaly Class Discovery in Industrial\n  Scenarios","summary":"  In the industrial scenario, anomaly detection could locate but cannot\nclassify anomalies. To complete their capability, we study to automatically\ndiscover and recognize visual classes of industrial anomalies. In terms of\nmulti-class anomaly classification, previous methods cluster anomalies\nrepresented by frozen pre-trained models but often fail due to poor\ndiscrimination. Novel class discovery (NCD) has the potential to tackle this.\nHowever, it struggles with non-prominent and semantically weak anomalies that\nchallenge network learning focus. To address these, we introduce AnomalyNCD, a\nmulti-class anomaly classification framework compatible with existing anomaly\ndetection methods. This framework learns anomaly-specific features and\nclassifies anomalies in a self-supervised manner. Initially, a technique called\nMain Element Binarization (MEBin) is first designed, which segments primary\nanomaly regions into masks to alleviate the impact of incorrect detections on\nlearning. Subsequently, we employ mask-guided contrastive representation\nlearning to improve feature discrimination, which focuses network attention on\nisolated anomalous regions and reduces the confusion of erroneous inputs\nthrough re-corrected pseudo labels. Finally, to enable flexible classification\nat both region and image levels during inference, we develop a region merging\nstrategy that determines the overall image category based on the classified\nanomaly regions. Our method outperforms the state-of-the-art works on the MVTec\nAD and MTD datasets. Compared with the current methods, AnomalyNCD combined\nwith zero-shot anomaly detection method achieves a 10.8% $F_1$ gain, 8.8% NMI\ngain, and 9.5% ARI gain on MVTec AD, 12.8% $F_1$ gain, 5.7% NMI gain, and 10.8%\nARI gain on MTD. The source code is available at\nhttps://github.com/HUST-SLOW/AnomalyNCD.\n","authors":["Ziming Huang","Xurui Li","Haotian Liu","Feng Xue","Yuzhe Wang","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.14379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14365v1","updated":"2024-10-18T10:51:10Z","published":"2024-10-18T10:51:10Z","title":"Impact of imperfect annotations on CNN training and performance for\n  instance segmentation and classification in digital pathology","summary":"  Segmentation and classification of large numbers of instances, such as cell\nnuclei, are crucial tasks in digital pathology for accurate diagnosis. However,\nthe availability of high-quality datasets for deep learning methods is often\nlimited due to the complexity of the annotation process. In this work, we\ninvestigate the impact of noisy annotations on the training and performance of\na state-of-the-art CNN model for the combined task of detecting, segmenting and\nclassifying nuclei in histopathology images. In this context, we investigate\nthe conditions for determining an appropriate number of training epochs to\nprevent overfitting to annotation noise during training. Our results indicate\nthat the utilisation of a small, correctly annotated validation set is\ninstrumental in avoiding overfitting and maintaining model performance to a\nlarge extent. Additionally, our findings underscore the beneficial role of\npre-training.\n","authors":["Laura Gálvez Jiménez","Christine Decaestecker"],"pdf_url":"https://arxiv.org/pdf/2410.14365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.08102v2","updated":"2024-10-18T09:58:45Z","published":"2023-02-16T06:01:31Z","title":"Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech\n  Recognition","summary":"  Visual Speech Recognition (VSR) aims to infer speech into text depending on\nlip movements alone. As it focuses on visual information to model the speech,\nits performance is inherently sensitive to personal lip appearances and\nmovements, and this makes the VSR models show degraded performance when they\nare applied to unseen speakers. In this paper, to remedy the performance\ndegradation of the VSR model on unseen speakers, we propose prompt tuning\nmethods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically,\nmotivated by recent advances in Natural Language Processing (NLP), we finetune\nprompts on adaptation data of target speakers instead of modifying the\npre-trained model parameters. Different from the previous prompt tuning methods\nmainly limited to Transformer variant architecture, we explore different types\nof prompts, the addition, the padding, and the concatenation form prompts that\ncan be applied to the VSR model which is composed of CNN and Transformer in\ngeneral. With the proposed prompt tuning, we show that the performance of the\npre-trained VSR model on unseen speakers can be largely improved by using a\nsmall amount of adaptation data (e.g., less than 5 minutes), even if the\npre-trained model is already developed with large speaker variations. Moreover,\nby analyzing the performance and parameters of different types of prompts, we\ninvestigate when the prompt tuning is preferred over the finetuning methods.\nThe effectiveness of the proposed method is evaluated on both word- and\nsentence-level VSR databases, LRW-ID and GRID.\n","authors":["Minsu Kim","Hyung-Il Kim","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2302.08102v2.pdf","comment":"IEEE TPAMI"},{"id":"http://arxiv.org/abs/2410.14343v1","updated":"2024-10-18T09:51:43Z","published":"2024-10-18T09:51:43Z","title":"2D-3D Deformable Image Registration of Histology Slide and Micro-CT with\n  ML-based Initialization","summary":"  Recent developments in the registration of histology and micro-computed\ntomography ({\\mu}CT) have broadened the perspective of pathological\napplications such as virtual histology based on {\\mu}CT. This topic remains\nchallenging because of the low image quality of soft tissue CT. Additionally,\nsoft tissue samples usually deform during the histology slide preparation,\nmaking it difficult to correlate the structures between histology slide and\n{\\mu}CT. In this work, we propose a novel 2D-3D multi-modal deformable image\nregistration method. The method uses a machine learning (ML) based\ninitialization followed by the registration. The registration is finalized by\nan analytical out-of-plane deformation refinement. The method is evaluated on\ndatasets acquired from tonsil and tumor tissues. {\\mu}CTs of both\nphase-contrast and conventional absorption modalities are investigated. The\nregistration results from the proposed method are compared with those from\nintensity- and keypoint-based methods. The comparison is conducted using both\nvisual and fiducial-based evaluations. The proposed method demonstrates\nsuperior performance compared to the other two methods.\n","authors":["Junan Chen","Matteo Ronchetti","Verena Stehl","Van Nguyen","Muhannad Al Kallaa","Mahesh Thalwaththe Gedara","Claudia Lölkes","Stefan Moser","Maximilian Seidl","Matthias Wieczorek"],"pdf_url":"https://arxiv.org/pdf/2410.14343v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.14340v1","updated":"2024-10-18T09:51:14Z","published":"2024-10-18T09:51:14Z","title":"Zero-shot Action Localization via the Confidence of Large\n  Vision-Language Models","summary":"  Precise action localization in untrimmed video is vital for fields such as\nprofessional sports and minimally invasive surgery, where the delineation of\nparticular motions in recordings can dramatically enhance analysis. But in many\ncases, large scale datasets with video-label pairs for localization are\nunavailable, limiting the opportunity to fine-tune video-understanding models.\nRecent developments in large vision-language models (LVLM) address this need\nwith impressive zero-shot capabilities in a variety of video understanding\ntasks. However, the adaptation of image-based LVLMs, with their powerful visual\nquestion answering capabilities, to action localization in long-form video is\nstill relatively unexplored. To this end, we introduce a true ZEro-shot Action\nLocalization method (ZEAL). Specifically, we leverage the built-in action\nknowledge of a large language model (LLM) to inflate actions into\nhighly-detailed descriptions of the archetypal start and end of the action.\nThese descriptions serve as queries to LVLM for generating frame-level\nconfidence scores which can be aggregated to produce localization outputs. The\nsimplicity and flexibility of our method lends it amenable to more capable\nLVLMs as they are developed, and we demonstrate remarkable results in zero-shot\naction localization on a challenging benchmark, without any training.\n","authors":["Josiah Aklilu","Xiaohan Wang","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2410.14340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14334v1","updated":"2024-10-18T09:44:35Z","published":"2024-10-18T09:44:35Z","title":"Evaluating the evaluators: Towards human-aligned metrics for missing\n  markers reconstruction","summary":"  Animation data is often obtained through optical motion capture systems,\nwhich utilize a multitude of cameras to establish the position of optical\nmarkers. However, system errors or occlusions can result in missing markers,\nthe manual cleaning of which can be time-consuming. This has sparked interest\nin machine learning-based solutions for missing marker reconstruction in the\nacademic community. Most academic papers utilize a simplistic mean square error\nas the main metric. In this paper, we show that this metric does not correlate\nwith subjective perception of the fill quality. We introduce and evaluate a set\nof better-correlated metrics that can drive progress in the field.\n","authors":["Taras Kucherenko","Derek Peristy","Judith Bütepage"],"pdf_url":"https://arxiv.org/pdf/2410.14334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14332v1","updated":"2024-10-18T09:44:25Z","published":"2024-10-18T09:44:25Z","title":"Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension","summary":"  Recent advances in Large Language Models (LLMs) have catalyzed the\ndevelopment of Large Multimodal Models (LMMs). However, existing research\nprimarily focuses on tuning language and image instructions, ignoring the\ncritical pretraining phase where models learn to process textual and visual\nmodalities jointly. In this paper, we propose a new pretraining paradigm for\nLMMs to enhance the visual comprehension capabilities of LLMs by introducing a\nnovel cross-modal comprehension stage. Specifically, we design a dynamically\nlearnable prompt token pool and employ the Hungarian algorithm to replace part\nof the original visual tokens with the most relevant prompt tokens. Then, we\nconceptualize visual tokens as analogous to a \"foreign language\" for the LLMs\nand propose a mixed attention mechanism with bidirectional visual attention and\nunidirectional textual attention to comprehensively enhance the understanding\nof visual tokens. Meanwhile, we integrate a detailed caption generation task,\nleveraging rich descriptions to further facilitate LLMs in understanding visual\nsemantic information. After pretraining on 1.5 million publicly accessible\ndata, we present a new foundation model called Croc. Experimental results\ndemonstrate that Croc achieves new state-of-the-art performance on massive\nvision-language benchmarks. To support reproducibility and facilitate further\nresearch, we release the training code and pre-trained model weights at\nhttps://github.com/deepglint/Croc.\n","authors":["Yin Xie","Kaicheng Yang","Ninghua Yang","Weimo Deng","Xiangzi Dai","Tiancheng Gu","Yumeng Wang","Xiang An","Yongle Zhao","Ziyong Feng","Jiankang Deng"],"pdf_url":"https://arxiv.org/pdf/2410.14332v1.pdf","comment":"18 pages, 11 figures"},{"id":"http://arxiv.org/abs/2410.14326v1","updated":"2024-10-18T09:37:38Z","published":"2024-10-18T09:37:38Z","title":"Fast proxy centers for Jeffreys centroids: The Jeffreys-Fisher-Rao and\n  the inductive Gauss-Bregman centers","summary":"  The symmetric Kullback-Leibler centroid also called the Jeffreys centroid of\na set of mutually absolutely continuous probability distributions on a measure\nspace provides a notion of centrality which has proven useful in many tasks\nincluding information retrieval, information fusion, and clustering in image,\nvideo and sound processing. However, the Jeffreys centroid is not available in\nclosed-form for sets of categorical or normal distributions, two widely used\nstatistical models, and thus need to be approximated numerically in practice.\nIn this paper, we first propose the new Jeffreys-Fisher-Rao center defined as\nthe Fisher-Rao midpoint of the sided Kullback-Leibler centroids as a plug-in\nreplacement of the Jeffreys centroid. This Jeffreys-Fisher-Rao center admits a\ngeneric formula for uni-parameter exponential family distributions, and\nclosed-form formula for categorical and normal distributions, matches exactly\nthe Jeffreys centroid for same-mean normal distributions, and is experimentally\nobserved in practice to be close to the Jeffreys centroid. Second, we define a\nnew type of inductive centers generalizing the principle of Gauss\narithmetic-geometric double sequence mean for pairs of densities of any given\nexponential family. This center is shown experimentally to approximate very\nwell the Jeffreys centroid and is suggested to use when the Jeffreys-Fisher-Rao\ncenter is not available in closed form. Moreover, this Gauss-Bregman inductive\ncenter always converges and matches the Jeffreys centroid for sets of same-mean\nnormal distributions. We report on our experiments demonstrating the use of the\nJeffreys-Fisher-Rao and Gauss-Bregman centers instead of the Jeffreys centroid.\nFinally, we conclude this work by reinterpreting these fast proxy centers of\nJeffreys centroids under the lens of dually flat spaces in information\ngeometry.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2410.14326v1.pdf","comment":"35 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.14324v1","updated":"2024-10-18T09:36:10Z","published":"2024-10-18T09:36:10Z","title":"HiCo: Hierarchical Controllable Diffusion Model for Layout-to-image\n  Generation","summary":"  The task of layout-to-image generation involves synthesizing images based on\nthe captions of objects and their spatial positions. Existing methods still\nstruggle in complex layout generation, where common bad cases include object\nmissing, inconsistent lighting, conflicting view angles, etc. To effectively\naddress these issues, we propose a \\textbf{Hi}erarchical \\textbf{Co}ntrollable\n(HiCo) diffusion model for layout-to-image generation, featuring object\nseperable conditioning branch structure. Our key insight is to achieve spatial\ndisentanglement through hierarchical modeling of layouts. We use a multi branch\nstructure to represent hierarchy and aggregate them in fusion module. To\nevaluate the performance of multi-objective controllable layout generation in\nnatural scenes, we introduce the HiCo-7K benchmark, derived from the GRIT-20M\ndataset and manually cleaned. https://github.com/360CVGroup/HiCo_T2I.\n","authors":["Bo Cheng","Yuhang Ma","Liebucha Wu","Shanyuan Liu","Ao Ma","Xiaoyu Wu","Dawei Leng","Yuhui Yin"],"pdf_url":"https://arxiv.org/pdf/2410.14324v1.pdf","comment":"NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.13824v2","updated":"2024-10-18T09:01:01Z","published":"2024-10-17T17:48:54Z","title":"Harnessing Webpage UIs for Text-Rich Visual Understanding","summary":"  Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.\n","authors":["Junpeng Liu","Tianyue Ou","Yifan Song","Yuxiao Qu","Wai Lam","Chenyan Xiong","Wenhu Chen","Graham Neubig","Xiang Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12013v2","updated":"2024-10-18T08:57:30Z","published":"2024-06-26T12:33:34Z","title":"Dating ancient manuscripts using radiocarbon and AI-based writing style\n  analysis","summary":"  Determining the chronology of ancient handwritten manuscripts is essential\nfor reconstructing the evolution of ideas. For the Dead Sea Scrolls, this is\nparticularly important. However, there is an almost complete lack of\ndate-bearing manuscripts evenly distributed across the timeline and written in\nsimilar scripts available for palaeographic comparison. Here, we present Enoch,\na state-of-the-art AI-based date-prediction model, trained on the basis of new\nradiocarbon-dated samples of the scrolls. Enoch uses established\nhandwriting-style descriptors and applies Bayesian ridge regression. The\nchallenge of this study is that the number of radiocarbon-dated manuscripts is\nsmall, while current machine learning requires an abundance of training data.\nWe show that by using combined angular and allographic writing style feature\nvectors and applying Bayesian ridge regression, Enoch could predict the\nradiocarbon-based dates from style, supported by leave-one-out validation, with\nvaried MAEs of 27.9 to 30.7 years relative to the radiocarbon dating. Enoch was\nthen used to estimate the dates of 135 unseen manuscripts, revealing that 79\nper cent of the samples were considered 'realistic' upon palaeographic post-hoc\nevaluation. We present a new chronology of the scrolls. The radiocarbon ranges\nand Enoch's style-based predictions are often older than the traditionally\nassumed palaeographic estimates. In the range of 300-50 BCE, Enoch's date\nprediction provides an improved granularity. The study is in line with current\ndevelopments in multimodal machine-learning techniques, and the methods can be\nused for date prediction in other partially-dated manuscript collections. This\nresearch shows how Enoch's quantitative, probability-based approach can be a\ntool for palaeographers and historians, re-dating ancient Jewish key texts and\ncontributing to current debates on Jewish and Christian origins.\n","authors":["Mladen Popović","Maruf A. Dhali","Lambert Schomaker","Johannes van der Plicht","Kaare Lund Rasmussen","Jacopo La Nasa","Ilaria Degano","Maria Perla Colombini","Eibert Tigchelaar"],"pdf_url":"https://arxiv.org/pdf/2407.12013v2.pdf","comment":"16 pages of main article, 103 pages of supplementary materials; the\n  first version of this article is originally prepared in July 2023 after the\n  completion of all the experiments"},{"id":"http://arxiv.org/abs/2410.14285v1","updated":"2024-10-18T08:40:26Z","published":"2024-10-18T08:40:26Z","title":"Advanced Underwater Image Quality Enhancement via Hybrid\n  Super-Resolution Convolutional Neural Networks and Multi-Scale Retinex-Based\n  Defogging Techniques","summary":"  The difficulties of underwater image degradation due to light scattering,\nabsorption, and fog-like particles which lead to low resolution and poor\nvisibility are discussed in this study report. We suggest a sophisticated\nhybrid strategy that combines Multi-Scale Retinex (MSR) defogging methods with\nSuper-Resolution Convolutional Neural Networks (SRCNN) to address these\nproblems. The Retinex algorithm mimics human visual perception to reduce uneven\nlighting and fogging, while the SRCNN component improves the spatial resolution\nof underwater photos.Through the combination of these methods, we are able to\nenhance the clarity, contrast, and colour restoration of underwater images,\noffering a reliable way to improve image quality in difficult underwater\nconditions. The research conducts extensive experiments on real-world\nunderwater datasets to further illustrate the efficacy of the suggested\napproach. In terms of sharpness, visibility, and feature retention,\nquantitative evaluation which use metrics like the Structural Similarity Index\nMeasure (SSIM) and Peak Signal-to-Noise Ratio (PSNR) demonstrates notable\nadvances over conventional techniques.In real-time underwater applications like\nmarine exploration, underwater robotics, and autonomous underwater vehicles,\nwhere clear and high-resolution imaging is crucial for operational success, the\ncombination of deep learning and conventional image processing techniques\noffers a computationally efficient framework with superior results.\n","authors":["Yugandhar Reddy Gogireddy","Jithendra Reddy Gogireddy"],"pdf_url":"https://arxiv.org/pdf/2410.14285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14283v1","updated":"2024-10-18T08:39:56Z","published":"2024-10-18T08:39:56Z","title":"Takin-ADA: Emotion Controllable Audio-Driven Animation with Canonical\n  and Landmark Loss Optimization","summary":"  Existing audio-driven facial animation methods face critical challenges,\nincluding expression leakage, ineffective subtle expression transfer, and\nimprecise audio-driven synchronization. We discovered that these issues stem\nfrom limitations in motion representation and the lack of fine-grained control\nover facial expressions. To address these problems, we present Takin-ADA, a\nnovel two-stage approach for real-time audio-driven portrait animation. In the\nfirst stage, we introduce a specialized loss function that enhances subtle\nexpression transfer while reducing unwanted expression leakage. The second\nstage utilizes an advanced audio processing technique to improve lip-sync\naccuracy. Our method not only generates precise lip movements but also allows\nflexible control over facial expressions and head motions. Takin-ADA achieves\nhigh-resolution (512x512) facial animations at up to 42 FPS on an RTX 4090 GPU,\noutperforming existing commercial solutions. Extensive experiments demonstrate\nthat our model significantly surpasses previous methods in video quality,\nfacial dynamics realism, and natural head movements, setting a new benchmark in\nthe field of audio-driven facial animation.\n","authors":["Bin Lin","Yanzhen Yu","Jianhao Ye","Ruitao Lv","Yuguang Yang","Ruoye Xie","Pan Yu","Hongbin Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.14283v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2410.14282v1","updated":"2024-10-18T08:39:49Z","published":"2024-10-18T08:39:49Z","title":"You Only Look Twice! for Failure Causes Identification of Drill Bits","summary":"  Efficient identification of the root causes of drill bit failure is crucial\ndue to potential impacts such as operational losses, safety threats, and\ndelays. Early recognition of these failures enables proactive maintenance,\nreducing risks and financial losses associated with unforeseen breakdowns and\nprolonged downtime. Thus, our study investigates various causes of drill bit\nfailure using images of different blades. The process involves annotating\ncutters with their respective locations and damage types, followed by the\ndevelopment of two YOLO Location and Damage Cutter Detection models, as well as\nmulti-class multi-label Decision Tree and Random Forests models to identify the\ncauses of failure by assessing the cutters' location and damage type.\nAdditionally, RRFCI is proposed for the classification of failure causes.\nNotably, the cutter location detection model achieved a high score of 0.97 mPA,\nand the cutter damage detection model yielded a 0.49 mPA. The rule-based\napproach over-performed both DT and RF in failure cause identification,\nachieving a macro-average F1-score of 0.94 across all damage causes. The\nintegration of the complete automated pipeline successfully identified 100\\% of\nthe 24 failure causes when tested on independent sets of ten drill bits,\nshowcasing its potential to efficiently assist experts in identifying the root\ncauses of drill bit damages.\n","authors":["Asma Yamani","Nehal Al-Otaiby","Haifa Al-Shemmeri","Imane Boudellioua"],"pdf_url":"https://arxiv.org/pdf/2410.14282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14279v1","updated":"2024-10-18T08:35:57Z","published":"2024-10-18T08:35:57Z","title":"ClearSR: Latent Low-Resolution Image Embeddings Help Diffusion-Based\n  Real-World Super Resolution Models See Clearer","summary":"  We present ClearSR, a new method that can better take advantage of latent\nlow-resolution image (LR) embeddings for diffusion-based real-world image\nsuper-resolution (Real-ISR). Previous Real-ISR models mostly focus on how to\nactivate more generative priors of text-to-image diffusion models to make the\noutput high-resolution (HR) images look better. However, since these methods\nrely too much on the generative priors, the content of the output images is\noften inconsistent with the input LR ones. To mitigate the above issue, in this\nwork, we explore using latent LR embeddings to constrain the control signals\nfrom ControlNet, and extract LR information at both detail and structure\nlevels. We show that the proper use of latent LR embeddings can produce\nhigher-quality control signals, which enables the super-resolution results to\nbe more consistent with the LR image and leads to clearer visual results. In\naddition, we also show that latent LR embeddings can be used to control the\ninference stage, allowing for the improvement of fidelity and generation\nability simultaneously. Experiments demonstrate that our model can achieve\nbetter performance across multiple metrics on several test sets and generate\nmore consistent SR results with LR images than existing methods. Our code will\nbe made publicly available.\n","authors":["Yuhao Wan","Peng-Tao Jiang","Qibin Hou","Hao Zhang","Jinwei Chen","Ming-Ming Cheng","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2410.14279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08091v2","updated":"2024-10-18T08:27:05Z","published":"2024-10-10T16:33:27Z","title":"Distribution Guidance Network for Weakly Supervised Point Cloud Semantic\n  Segmentation","summary":"  Despite alleviating the dependence on dense annotations inherent to fully\nsupervised methods, weakly supervised point cloud semantic segmentation suffers\nfrom inadequate supervision signals. In response to this challenge, we\nintroduce a novel perspective that imparts auxiliary constraints by regulating\nthe feature space under weak supervision. Our initial investigation identifies\nwhich distributions accurately characterize the feature space, subsequently\nleveraging this priori to guide the alignment of the weakly supervised\nembeddings. Specifically, we analyze the superiority of the mixture of von\nMises-Fisher distributions (moVMF) among several common distribution\ncandidates. Accordingly, we develop a Distribution Guidance Network (DGNet),\nwhich comprises a weakly supervised learning branch and a distribution\nalignment branch. Leveraging reliable clustering initialization derived from\nthe weakly supervised learning branch, the distribution alignment branch\nalternately updates the parameters of the moVMF and the network, ensuring\nalignment with the moVMF-defined latent space. Extensive experiments validate\nthe rationality and effectiveness of our distribution choice and network\ndesign. Consequently, DGNet achieves state-of-the-art performance under\nmultiple datasets and various weakly supervised settings.\n","authors":["Zhiyi Pan","Wei Gao","Shan Liu","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2410.08091v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02512v2","updated":"2024-10-18T08:25:52Z","published":"2024-05-03T22:55:56Z","title":"SatSwinMAE: Efficient Autoencoding for Multiscale Time-series Satellite\n  Imagery","summary":"  Recent advancements in foundation models have significantly impacted various\nfields, including natural language processing, computer vision, and multi-modal\ntasks. One area that stands to benefit greatly is Earth observation, where\nthese models can efficiently process large-scale, unlabeled geospatial data. In\nthis work we extend the SwinMAE model to integrate temporal information for\nsatellite time-series data. The architecture employs a hierarchical 3D Masked\nAutoencoder (MAE) with Video Swin Transformer blocks to effectively capture\nmulti-scale spatio-temporal dependencies in satellite imagery. To enhance\ntransfer learning, we incorporate both encoder and decoder pretrained weights,\nalong with skip connections to preserve scale-specific information. This forms\nan architecture similar to SwinUNet with an additional temporal component. Our\napproach shows significant performance improvements over existing\nstate-of-the-art foundation models for all the evaluated downstream tasks: land\ncover segmentation, building density prediction, flood mapping, wildfire scar\nmapping and multi-temporal crop segmentation. Particularly, in the land cover\nsegmentation task of the PhilEO Bench dataset, it outperforms other geospatial\nfoundation models with a 10.4% higher accuracy.\n","authors":["Yohei Nakayama","Jiawei Su","Luis M. Pazos-Outón"],"pdf_url":"https://arxiv.org/pdf/2405.02512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14265v1","updated":"2024-10-18T08:20:37Z","published":"2024-10-18T08:20:37Z","title":"HYPNOS : Highly Precise Foreground-focused Diffusion Finetuning for\n  Inanimate Objects","summary":"  In recent years, personalized diffusion-based text-to-image generative tasks\nhave been a hot topic in computer vision studies. A robust diffusion model is\ndetermined by its ability to perform near-perfect reconstruction of certain\nproduct outcomes given few related input samples. Unfortunately, the current\nprominent diffusion-based finetuning technique falls short in maintaining the\nforeground object consistency while being constrained to produce diverse\nbackgrounds in the image outcome. In the worst scenario, the overfitting issue\nmay occur, meaning that the foreground object is less controllable due to the\ncondition above, for example, the input prompt information is transferred\nambiguously to both foreground and background regions, instead of the supposed\nbackground region only. To tackle the issues above, we proposed Hypnos, a\nhighly precise foreground-focused diffusion finetuning technique. On the image\nlevel, this strategy works best for inanimate object generation tasks, and to\ndo so, Hypnos implements two main approaches, namely: (i) a content-centric\nprompting strategy and (ii) the utilization of our additional\nforeground-focused discriminative module. The utilized module is connected with\nthe diffusion model and finetuned with our proposed set of supervision\nmechanism. Combining the strategies above yielded to the foreground-background\ndisentanglement capability of the diffusion model. Our experimental results\nshowed that the proposed strategy gave a more robust performance and visually\npleasing results compared to the former technique. For better elaborations, we\nalso provided extensive studies to assess the fruitful outcomes above, which\nreveal how personalization behaves in regard to several training conditions.\n","authors":["Oliverio Theophilus Nathanael","Jonathan Samuel Lumentut","Nicholas Hans Muliawan","Edbert Valencio Angky","Felix Indra Kurniadi","Alfi Yusrotis Zakiyyah","Jeklin Harefa"],"pdf_url":"https://arxiv.org/pdf/2410.14265v1.pdf","comment":"26 pages, 12 figures, to appear on the Rich Media with Generative AI\n  workshop in conjunction with Asian Conference on Computer Vision (ACCV) 2024"},{"id":"http://arxiv.org/abs/2406.00125v3","updated":"2024-10-18T08:18:36Z","published":"2024-05-31T18:32:46Z","title":"TotalVibeSegmentator: Full Body MRI Segmentation for the NAKO and UK\n  Biobank","summary":"  Objectives: To present a publicly available torso segmentation network for\nlarge epidemiology datasets on volumetric interpolated breath-hold examination\n(VIBE) images. Materials & Methods: We extracted preliminary segmentations from\nTotalSegmentator, spine, and body composition networks for VIBE images, then\nimproved them iteratively and retrained a nnUNet network. Using subsets of NAKO\n(85 subjects) and UK Biobank (16 subjects), we evaluated with Dice-score on a\nholdout set (12 subjects) and existing organ segmentation approach (1000\nsubjects), generating 71 semantic segmentation types for VIBE images. We\nprovide an additional network for the vertebra segments 22 individual vertebra\ntypes. Results: We achieved an average Dice score of 0.89 +- 0.07 overall 71\nsegmentation labels. We scored > 0.90 Dice-score on the abdominal organs except\nfor the pancreas with a Dice of 0.70. Conclusion: Our work offers a detailed\nand refined publicly available full torso segmentation on VIBE images.\n","authors":["Robert Graf","Paul-Sören Platzek","Evamaria Olga Riedel","Constanze Ramschütz","Sophie Starck","Hendrik Kristian Möller","Matan Atad","Henry Völzke","Robin Bülow","Carsten Oliver Schmidt","Julia Rüdebusch","Matthias Jung","Marco Reisert","Jakob Weiss","Maximilian Löffler","Fabian Bamberg","Bene Wiestler","Johannes C. Paetzold","Daniel Rueckert","Jan Stefan Kirschke"],"pdf_url":"https://arxiv.org/pdf/2406.00125v3.pdf","comment":"https://github.com/robert-graf/TotalVibeSegmentator"},{"id":"http://arxiv.org/abs/2410.14250v1","updated":"2024-10-18T08:01:36Z","published":"2024-10-18T08:01:36Z","title":"Vision-Language Navigation with Energy-Based Policy","summary":"  Vision-language navigation (VLN) requires an agent to execute actions\nfollowing human instructions. Existing VLN models are optimized through expert\ndemonstrations by supervised behavioural cloning or incorporating manual reward\nengineering. While straightforward, these efforts overlook the accumulation of\nerrors in the Markov decision process, and struggle to match the distribution\nof the expert policy. Going beyond this, we propose an Energy-based Navigation\nPolicy (ENP) to model the joint state-action distribution using an energy-based\nmodel. At each step, low energy values correspond to the state-action pairs\nthat the expert is most likely to perform, and vice versa. Theoretically, the\noptimization objective is equivalent to minimizing the forward divergence\nbetween the occupancy measure of the expert and ours. Consequently, ENP learns\nto globally align with the expert policy by maximizing the likelihood of the\nactions and modeling the dynamics of the navigation states in a collaborative\nmanner. With a variety of VLN architectures, ENP achieves promising\nperformances on R2R, REVERIE, RxR, and R2R-CE, unleashing the power of existing\nVLN models.\n","authors":["Rui Liu","Wenguan Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.14250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13621v2","updated":"2024-10-18T08:01:27Z","published":"2024-10-17T14:55:09Z","title":"Enhanced Prompt-leveraged Weakly Supervised Cancer Segmentation based on\n  Segment Anything","summary":"  This work proposes a novel approach beyond supervised learning for effective\npathological image analysis, addressing the challenge of limited robust labeled\ndata. Pathological diagnosis of diseases like cancer has conventionally relied\non the evaluation of morphological features by physicians and pathologists.\nHowever, recent advancements in compute-aided diagnosis (CAD) systems are\ngaining significant attention as diagnostic support tools. Although the\nadvancement of deep learning has improved CAD significantly, segmentation\nmodels typically require large pixel-level annotated dataset, and such labeling\nis expensive. Existing studies not based on supervised approaches still\nstruggle with limited generalization, and no practical approach has emerged\nyet. To address this issue, we present a weakly supervised semantic\nsegmentation (WSSS) model by combining class activation map and Segment\nAnything Model (SAM)-based pseudo-labeling. For effective pretraining, we adopt\nthe SAM-a foundation model that is pretrained on large datasets and operates in\nzero-shot configurations using only coarse prompts. The proposed approach\ntransfer enhanced Attention Dropout Layer's knowledge to SAM, thereby\ngenerating pseudo-labels. To demonstrate the superiority of the proposed\nmethod, experimental studies are conducted on histopathological breast cancer\ndatasets. The proposed method outperformed other WSSS methods across three\ndatasets, demonstrating its efficiency by achieving this with only 12GB of GPU\nmemory during training. Our code is available at :\nhttps://github.com/QI-NemoSong/EPLC-SAM\n","authors":["Joonhyeon Song","Seohwan Yun","Seongho Yoon","Joohyeok Kim","Sangmin Lee"],"pdf_url":"https://arxiv.org/pdf/2410.13621v2.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.14247v1","updated":"2024-10-18T07:52:03Z","published":"2024-10-18T07:52:03Z","title":"ERDDCI: Exact Reversible Diffusion via Dual-Chain Inversion for\n  High-Quality Image Editing","summary":"  Diffusion models (DMs) have been successfully applied to real image editing.\nThese models typically invert images into latent noise vectors used to\nreconstruct the original images (known as inversion), and then edit them during\nthe inference process. However, recent popular DMs often rely on the assumption\nof local linearization, where the noise injected during the inversion process\nis expected to approximate the noise removed during the inference process.\nWhile DM efficiently generates images under this assumption, it can also\naccumulate errors during the diffusion process due to the assumption,\nultimately negatively impacting the quality of real image reconstruction and\nediting. To address this issue, we propose a novel method, referred to as\nERDDCI (Exact Reversible Diffusion via Dual-Chain Inversion). ERDDCI uses the\nnew Dual-Chain Inversion (DCI) for joint inference to derive an exact\nreversible diffusion process. By using DCI, our method effectively avoids the\ncumbersome optimization process in existing inversion approaches and achieves\nhigh-quality image editing. Additionally, to accommodate image operations under\nhigh guidance scales, we introduce a dynamic control strategy that enables more\nrefined image reconstruction and editing. Our experiments demonstrate that\nERDDCI significantly outperforms state-of-the-art methods in a 50-step\ndiffusion process. It achieves rapid and precise image reconstruction with an\nSSIM of 0.999 and an LPIPS of 0.001, and also delivers competitive results in\nimage editing.\n","authors":["Jimin Dai","Yingzhen Zhang","Shuo Chen","Jian Yang","Lei Luo"],"pdf_url":"https://arxiv.org/pdf/2410.14247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14245v1","updated":"2024-10-18T07:51:31Z","published":"2024-10-18T07:51:31Z","title":"PReP: Efficient context-based shape retrieval for missing parts","summary":"  In this paper we study the problem of shape part retrieval in the point cloud\ndomain. Shape retrieval methods in the literature rely on the presence of an\nexisting query object, but what if the part we are looking for is not\navailable? We present Part Retrieval Pipeline (PReP), a pipeline that\ncreatively utilizes metric learning techniques along with a trained\nclassification model to measure the suitability of potential replacement parts\nfrom a database, as part of an application scenario targeting circular economy.\nThrough an innovative training procedure with increasing difficulty, it is able\nto learn to recognize suitable parts relying only on shape context. Thanks to\nits low parameter size and computational requirements, it can be used to sort\nthrough a warehouse of potentially tens of thousand of spare parts in just a\nfew seconds. We also establish an alternative baseline approach to compare\nagainst, and extensively document the unique challenges associated with this\ntask, as well as identify the design choices to solve them.\n","authors":["Vlassis Fotis","Ioannis Romanelis","Georgios Mylonas","Athanasios Kalogeras","Konstantinos Moustakas"],"pdf_url":"https://arxiv.org/pdf/2410.14245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14242v1","updated":"2024-10-18T07:47:59Z","published":"2024-10-18T07:47:59Z","title":"Pseudo-label Refinement for Improving Self-Supervised Learning Systems","summary":"  Self-supervised learning systems have gained significant attention in recent\nyears by leveraging clustering-based pseudo-labels to provide supervision\nwithout the need for human annotations. However, the noise in these\npseudo-labels caused by the clustering methods poses a challenge to the\nlearning process leading to degraded performance. In this work, we propose a\npseudo-label refinement (SLR) algorithm to address this issue. The cluster\nlabels from the previous epoch are projected to the current epoch\ncluster-labels space and a linear combination of the new label and the\nprojected label is computed as a soft refined label containing the information\nfrom the previous epoch clusters as well as from the current epoch. In contrast\nto the common practice of using the maximum value as a cluster/class indicator,\nwe employ hierarchical clustering on these soft pseudo-labels to generate\nrefined hard-labels. This approach better utilizes the information embedded in\nthe soft labels, outperforming the simple maximum value approach for hard label\ngeneration. The effectiveness of the proposed SLR algorithm is evaluated in the\ncontext of person re-identification (Re-ID) using unsupervised domain\nadaptation (UDA). Experimental results demonstrate that the modified Re-ID\nbaseline, incorporating the SLR algorithm, achieves significantly improved mean\nAverage Precision (mAP) performance in various UDA tasks, including\nreal-to-synthetic, synthetic-to-real, and different real-to-real scenarios.\nThese findings highlight the efficacy of the SLR algorithm in enhancing the\nperformance of self-supervised learning systems.\n","authors":[" Zia-ur-Rehman","Arif Mahmood","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2410.14242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14238v1","updated":"2024-10-18T07:40:41Z","published":"2024-10-18T07:40:41Z","title":"Storyboard guided Alignment for Fine-grained Video Action Recognition","summary":"  Fine-grained video action recognition can be conceptualized as a video-text\nmatching problem. Previous approaches often rely on global video semantics to\nconsolidate video embeddings, which can lead to misalignment in video-text\npairs due to a lack of understanding of action semantics at an atomic\ngranularity level. To tackle this challenge, we propose a multi-granularity\nframework based on two observations: (i) videos with different global semantics\nmay share similar atomic actions or appearances, and (ii) atomic actions within\na video can be momentary, slow, or even non-directly related to the global\nvideo semantics. Inspired by the concept of storyboarding, which disassembles a\nscript into individual shots, we enhance global video semantics by generating\nfine-grained descriptions using a pre-trained large language model. These\ndetailed descriptions capture common atomic actions depicted in videos. A\nfiltering metric is proposed to select the descriptions that correspond to the\natomic actions present in both the videos and the descriptions. By employing\nglobal semantics and fine-grained descriptions, we can identify key frames in\nvideos and utilize them to aggregate embeddings, thereby making the embedding\nmore accurate. Extensive experiments on various video action recognition\ndatasets demonstrate superior performance of our proposed method in supervised,\nfew-shot, and zero-shot settings.\n","authors":["Enqi Liu","Liyuan Pan","Yan Yang","Yiran Zhong","Zhijing Wu","Xinxiao Wu","Liu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.14238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.05822v3","updated":"2024-10-18T07:24:54Z","published":"2023-08-10T18:43:44Z","title":"Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded\n  Egocentric Perception","summary":"  We depend on our own memory to encode, store, and retrieve our experiences.\nHowever, memory lapses can occur. One promising avenue for achieving memory\naugmentation is through the use of augmented reality head-mounted displays to\ncapture and preserve egocentric videos, a practice commonly referred to as\nlifelogging. However, a significant challenge arises from the sheer volume of\nvideo data generated through lifelogging, as the current technology lacks the\ncapability to encode and store such large amounts of data efficiently. Further,\nretrieving specific information from extensive video archives requires\nsubstantial computational power, further complicating the task of quickly\naccessing desired content. To address these challenges, we propose a memory\naugmentation agent that involves leveraging natural language encoding for video\ndata and storing them in a vector database. This approach harnesses the power\nof large vision language models to perform the language encoding process.\nAdditionally, we propose using large language models to facilitate natural\nlanguage querying. Our agent underwent extensive evaluation using the QA-Ego4D\ndataset and achieved state-of-the-art results with a BLEU score of 8.3,\noutperforming conventional machine learning models that scored between 3.4 and\n5.8. Additionally, we conducted a user study in which participants interacted\nwith the human memory augmentation agent through episodic memory and open-ended\nquestions. The results of this study show that the agent results in\nsignificantly better recall performance on episodic memory tasks compared to\nhuman participants. The results also highlight the agent's practical\napplicability and user acceptance.\n","authors":["Junxiao Shen","John Dudley","Per Ola Kristensson"],"pdf_url":"https://arxiv.org/pdf/2308.05822v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18791v3","updated":"2024-10-18T07:21:56Z","published":"2024-03-27T17:35:24Z","title":"Object Pose Estimation via the Aggregation of Diffusion Features","summary":"  Estimating the pose of objects from images is a crucial task of 3D scene\nunderstanding, and recent approaches have shown promising results on very large\nbenchmarks. However, these methods experience a significant performance drop\nwhen dealing with unseen objects. We believe that it results from the limited\ngeneralizability of image features. To address this problem, we have an\nin-depth analysis on the features of diffusion models, e.g. Stable Diffusion,\nwhich hold substantial potential for modeling unseen objects. Based on this\nanalysis, we then innovatively introduce these diffusion features for object\npose estimation. To achieve this, we propose three distinct architectures that\ncan effectively capture and aggregate diffusion features of different\ngranularity, greatly improving the generalizability of object pose estimation.\nOur approach outperforms the state-of-the-art methods by a considerable margin\non three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our\nmethod achieves higher accuracy than the previous best arts on unseen objects:\n97.9% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the\nstrong generalizability of our method. Our code is released at\nhttps://github.com/Tianfu18/diff-feats-pose.\n","authors":["Tianfu Wang","Guosheng Hu","Hongguang Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18791v3.pdf","comment":"Accepted to CVPR2024, fix typo"},{"id":"http://arxiv.org/abs/2410.09421v2","updated":"2024-10-18T07:10:38Z","published":"2024-10-12T07:56:47Z","title":"VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language\n  Models Alignment","summary":"  As large vision-language models (LVLMs) evolve rapidly, the demand for\nhigh-quality and diverse data to align these models becomes increasingly\ncrucial. However, the creation of such data with human supervision proves\ncostly and time-intensive. In this paper, we investigate the efficacy of AI\nfeedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the\nfirst large-scale vision-language feedback dataset, comprising over 82K\nmulti-modal instructions and comprehensive rationales generated by\noff-the-shelf models without human annotations. To evaluate the effectiveness\nof AI feedback for vision-language alignment, we train Silkie, an LVLM\nfine-tuned via direct preference optimization on VLFeedback. Silkie showcases\nexceptional performance regarding helpfulness, visual faithfulness, and safety\nmetrics. It outperforms its base model by 6.9\\% and 9.5\\% in perception and\ncognition tasks, reduces hallucination issues on MMHal-Bench, and exhibits\nenhanced resilience against red-teaming attacks. Furthermore, our analysis\nunderscores the advantage of AI feedback, particularly in fostering preference\ndiversity to deliver more comprehensive improvements. Our dataset, training\ncode and models are available at https://vlf-silkie.github.io.\n","authors":["Lei Li","Zhihui Xie","Mukai Li","Shunian Chen","Peiyi Wang","Liang Chen","Yazheng Yang","Benyou Wang","Lingpeng Kong","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2410.09421v2.pdf","comment":"EMNLP 2024 Main Conference camera-ready version (fixed small typos).\n  This article supersedes arXiv:2312.10665"},{"id":"http://arxiv.org/abs/2410.14214v1","updated":"2024-10-18T07:02:57Z","published":"2024-10-18T07:02:57Z","title":"MambaSCI: Efficient Mamba-UNet for Quad-Bayer Patterned Video Snapshot\n  Compressive Imaging","summary":"  Color video snapshot compressive imaging (SCI) employs computational imaging\ntechniques to capture multiple sequential video frames in a single\nBayer-patterned measurement. With the increasing popularity of quad-Bayer\npattern in mainstream smartphone cameras for capturing high-resolution videos,\nmobile photography has become more accessible to a wider audience. However,\nexisting color video SCI reconstruction algorithms are designed based on the\ntraditional Bayer pattern. When applied to videos captured by quad-Bayer\ncameras, these algorithms often result in color distortion and ineffective\ndemosaicing, rendering them impractical for primary equipment. To address this\nchallenge, we propose the MambaSCI method, which leverages the Mamba and UNet\narchitectures for efficient reconstruction of quad-Bayer patterned color video\nSCI. To the best of our knowledge, our work presents the first algorithm for\nquad-Bayer patterned SCI reconstruction, and also the initial application of\nthe Mamba model to this task. Specifically, we customize Residual-Mamba-Blocks,\nwhich residually connect the Spatial-Temporal Mamba (STMamba),\nEdge-Detail-Reconstruction (EDR) module, and Channel Attention (CA) module.\nRespectively, STMamba is used to model long-range spatial-temporal dependencies\nwith linear complexity, EDR is for better edge-detail reconstruction, and CA is\nused to compensate for the missing channel information interaction in Mamba\nmodel. Experiments demonstrate that MambaSCI surpasses state-of-the-art methods\nwith lower computational and memory costs. PyTorch style pseudo-code for the\ncore modules is provided in the supplementary materials.\n","authors":["Zhenghao Pan","Haijin Zeng","Jiezhang Cao","Yongyong Chen","Kai Zhang","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2410.14214v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.10167v2","updated":"2024-10-18T06:57:51Z","published":"2024-10-14T05:23:12Z","title":"X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing","summary":"  Human sensing, which employs various sensors and advanced deep learning\ntechnologies to accurately capture and interpret human body information, has\nsignificantly impacted fields like public security and robotics. However,\ncurrent human sensing primarily depends on modalities such as cameras and\nLiDAR, each of which has its own strengths and limitations. Furthermore,\nexisting multi-modal fusion solutions are typically designed for fixed modality\ncombinations, requiring extensive retraining when modalities are added or\nremoved for diverse scenarios. In this paper, we propose a modality-invariant\nfoundation model for all modalities, X-Fi, to address this issue. X-Fi enables\nthe independent or combinatory use of sensor modalities without additional\ntraining by utilizing a transformer structure to accommodate variable input\nsizes and incorporating a novel \"X-fusion\" mechanism to preserve\nmodality-specific features during multimodal integration. This approach not\nonly enhances adaptability but also facilitates the learning of complementary\nfeatures across modalities. Extensive experiments conducted on the MM-Fi and\nXRF55 datasets, employing six distinct modalities, demonstrate that X-Fi\nachieves state-of-the-art performance in human pose estimation (HPE) and human\nactivity recognition (HAR) tasks. The findings indicate that our proposed model\ncan efficiently support a wide range of human sensing applications, ultimately\ncontributing to the evolution of scalable, multimodal sensing technologies.\n","authors":["Xinyan Chen","Jianfei Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14210v1","updated":"2024-10-18T06:52:51Z","published":"2024-10-18T06:52:51Z","title":"Shape Transformation Driven by Active Contour for Class-Imbalanced\n  Semi-Supervised Medical Image Segmentation","summary":"  Annotating 3D medical images demands expert knowledge and is time-consuming.\nAs a result, semi-supervised learning (SSL) approaches have gained significant\ninterest in 3D medical image segmentation. The significant size differences\namong various organs in the human body lead to imbalanced class distribution,\nwhich is a major challenge in the real-world application of these SSL\napproaches. To address this issue, we develop a novel Shape Transformation\ndriven by Active Contour (STAC), that enlarges smaller organs to alleviate\nimbalanced class distribution across different organs. Inspired by curve\nevolution theory in active contour methods, STAC employs a signed distance\nfunction (SDF) as the level set function, to implicitly represent the shape of\norgans, and deforms voxels in the direction of the steepest descent of SDF\n(i.e., the normal vector). To ensure that the voxels far from expansion organs\nremain unchanged, we design an SDF-based weight function to control the degree\nof deformation for each voxel. We then use STAC as a data-augmentation process\nduring the training stage. Experimental results on two benchmark datasets\ndemonstrate that the proposed method significantly outperforms some\nstate-of-the-art methods. Source code is publicly available at\nhttps://github.com/GuGuLL123/STAC.\n","authors":["Yuliang Gu","Yepeng Liu","Zhichao Sun","Jinchi Zhu","Yongchao Xu","Laurent Najman"],"pdf_url":"https://arxiv.org/pdf/2410.14210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06719v3","updated":"2024-10-18T06:39:27Z","published":"2024-10-09T09:43:36Z","title":"Suppress Content Shift: Better Diffusion Features via Off-the-Shelf\n  Generation Techniques","summary":"  Diffusion models are powerful generative models, and this capability can also\nbe applied to discrimination. The inner activations of a pre-trained diffusion\nmodel can serve as features for discriminative tasks, namely, diffusion\nfeature. We discover that diffusion feature has been hindered by a hidden yet\nuniversal phenomenon that we call content shift. To be specific, there are\ncontent differences between features and the input image, such as the exact\nshape of a certain object. We locate the cause of content shift as one inherent\ncharacteristic of diffusion models, which suggests the broad existence of this\nphenomenon in diffusion feature. Further empirical study also indicates that\nits negative impact is not negligible even when content shift is not visually\nperceivable. Hence, we propose to suppress content shift to enhance the overall\nquality of diffusion features. Specifically, content shift is related to the\ninformation drift during the process of recovering an image from the noisy\ninput, pointing out the possibility of turning off-the-shelf generation\ntechniques into tools for content shift suppression. We further propose a\npractical guideline named GATE to efficiently evaluate the potential benefit of\na technique and provide an implementation of our methodology. Despite the\nsimplicity, the proposed approach has achieved superior results on various\ntasks and datasets, validating its potential as a generic booster for diffusion\nfeatures. Our code is available at\nhttps://github.com/Darkbblue/diffusion-content-shift.\n","authors":["Benyuan Meng","Qianqian Xu","Zitai Wang","Zhiyong Yang","Xiaochun Cao","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2410.06719v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2410.03558"},{"id":"http://arxiv.org/abs/2410.14201v1","updated":"2024-10-18T06:31:57Z","published":"2024-10-18T06:31:57Z","title":"Text-to-Image Representativity Fairness Evaluation Framework","summary":"  Text-to-Image generative systems are progressing rapidly to be a source of\nadvertisement and media and could soon serve as image searches or artists.\nHowever, there is a significant concern about the representativity bias these\nmodels embody and how these biases can propagate in the social fabric after\nfine-tuning them. Therefore, continuously monitoring and evaluating these\nmodels for fairness is important. To address this issue, we propose\nText-to-Image (TTI) Representativity Fairness Evaluation Framework. In this\nframework, we evaluate three aspects of a TTI system; diversity, inclusion, and\nquality. For each aspect, human-based and model-based approaches are proposed\nand evaluated for their ability to capture the bias and whether they can\nsubstitute each other. The framework starts by suggesting the prompts for\ngenerating the images for the evaluation based on the context and the sensitive\nattributes under study. Then the three aspects are evaluated using the proposed\napproaches. Based on the evaluation, a decision is made regarding the\nrepresentativity bias within the TTI system. The evaluation of our framework on\nStable Diffusion shows that the framework can effectively capture the bias in\nTTI systems. The results also confirm that our proposed model based-approaches\ncan substitute human-based approaches in three out of four components with high\ncorrelation, which could potentially reduce costs and automate the process. The\nstudy suggests that continual learning of the model on more inclusive data\nacross disadvantaged minorities such as Indians and Middle Easterners is\nessential to mitigate current stereotyping and lack of inclusiveness.\n","authors":["Asma Yamani","Malak Baslyman"],"pdf_url":"https://arxiv.org/pdf/2410.14201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14200v1","updated":"2024-10-18T06:31:40Z","published":"2024-10-18T06:31:40Z","title":"E3D-GPT: Enhanced 3D Visual Foundation for Medical Vision-Language Model","summary":"  The development of 3D medical vision-language models holds significant\npotential for disease diagnosis and patient treatment. However, compared to 2D\nmedical images, 3D medical images, such as CT scans, face challenges related to\nlimited training data and high dimension, which severely restrict the progress\nof 3D medical vision-language models. To address these issues, we collect a\nlarge amount of unlabeled 3D CT data and utilize self-supervised learning to\nconstruct a 3D visual foundation model for extracting 3D visual features. Then,\nwe apply 3D spatial convolutions to aggregate and project high-level image\nfeatures, reducing computational complexity while preserving spatial\ninformation. We also construct two instruction-tuning datasets based on BIMCV-R\nand CT-RATE to fine-tune the 3D vision-language model. Our model demonstrates\nsuperior performance compared to existing methods in report generation, visual\nquestion answering, and disease diagnosis. Code and data will be made publicly\navailable soon.\n","authors":["Haoran Lai","Zihang Jiang","Qingsong Yao","Rongsheng Wang","Zhiyang He","Xiaodong Tao","Wei Wei","Weifu Lv","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.14200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03558v3","updated":"2024-10-18T06:19:45Z","published":"2024-10-04T16:05:14Z","title":"Not All Diffusion Model Activations Have Been Evaluated as\n  Discriminative Features","summary":"  Diffusion models are initially designed for image generation. Recent research\nshows that the internal signals within their backbones, named activations, can\nalso serve as dense features for various discriminative tasks such as semantic\nsegmentation. Given numerous activations, selecting a small yet effective\nsubset poses a fundamental problem. To this end, the early study of this field\nperforms a large-scale quantitative comparison of the discriminative ability of\nthe activations. However, we find that many potential activations have not been\nevaluated, such as the queries and keys used to compute attention scores.\nMoreover, recent advancements in diffusion architectures bring many new\nactivations, such as those within embedded ViT modules. Both combined,\nactivation selection remains unresolved but overlooked. To tackle this issue,\nthis paper takes a further step with a much broader range of activations\nevaluated. Considering the significant increase in activations, a full-scale\nquantitative comparison is no longer operational. Instead, we seek to\nunderstand the properties of these activations, such that the activations that\nare clearly inferior can be filtered out in advance via simple qualitative\nevaluation. After careful analysis, we discover three properties universal\namong diffusion models, enabling this study to go beyond specific models. On\ntop of this, we present effective feature selection solutions for several\npopular diffusion models. Finally, the experiments across multiple\ndiscriminative tasks validate the superiority of our method over the SOTA\ncompetitors. Our code is available at\nhttps://github.com/Darkbblue/generic-diffusion-feature.\n","authors":["Benyuan Meng","Qianqian Xu","Zitai Wang","Xiaochun Cao","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2410.03558v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14195v1","updated":"2024-10-18T06:12:36Z","published":"2024-10-18T06:12:36Z","title":"Rethinking Transformer for Long Contextual Histopathology Whole Slide\n  Image Analysis","summary":"  Histopathology Whole Slide Image (WSI) analysis serves as the gold standard\nfor clinical cancer diagnosis in the daily routines of doctors. To develop\ncomputer-aided diagnosis model for WSIs, previous methods typically employ\nMulti-Instance Learning to enable slide-level prediction given only slide-level\nlabels. Among these models, vanilla attention mechanisms without pairwise\ninteractions have traditionally been employed but are unable to model\ncontextual information. More recently, self-attention models have been utilized\nto address this issue. To alleviate the computational complexity of long\nsequences in large WSIs, methods like HIPT use region-slicing, and TransMIL\nemploys approximation of full self-attention. Both approaches suffer from\nsuboptimal performance due to the loss of key information. Moreover, their use\nof absolute positional embedding struggles to effectively handle long\ncontextual dependencies in shape-varying WSIs. In this paper, we first analyze\nhow the low-rank nature of the long-sequence attention matrix constrains the\nrepresentation ability of WSI modelling. Then, we demonstrate that the rank of\nattention matrix can be improved by focusing on local interactions via a local\nattention mask. Our analysis shows that the local mask aligns with the\nattention patterns in the lower layers of the Transformer. Furthermore, the\nlocal attention mask can be implemented during chunked attention calculation,\nreducing the quadratic computational complexity to linear with a small local\nbandwidth. Building on this, we propose a local-global hybrid Transformer for\nboth computational acceleration and local-global information interactions\nmodelling. Our method, Long-contextual MIL (LongMIL), is evaluated through\nextensive experiments on various WSI tasks to validate its superiority. Our\ncode will be available at github.com/invoker-LL/Long-MIL.\n","authors":["Honglin Li","Yunlong Zhang","Pingyi Chen","Zhongyi Shui","Chenglu Zhu","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.14195v1.pdf","comment":"NeurIPS-2024. arXiv admin note: text overlap with arXiv:2311.12885"},{"id":"http://arxiv.org/abs/2410.13195v2","updated":"2024-10-18T06:02:28Z","published":"2024-10-17T03:48:02Z","title":"UniG: Modelling Unitary 3D Gaussians for View-consistent 3D\n  Reconstruction","summary":"  In this work, we present UniG, a view-consistent 3D reconstruction and novel\nview synthesis model that generates a high-fidelity representation of 3D\nGaussians from sparse images. Existing 3D Gaussians-based methods usually\nregress Gaussians per-pixel of each view, create 3D Gaussians per view\nseparately, and merge them through point concatenation. Such a view-independent\nreconstruction approach often results in a view inconsistency issue, where the\npredicted positions of the same 3D point from different views may have\ndiscrepancies. To address this problem, we develop a DETR (DEtection\nTRansformer)-like framework, which treats 3D Gaussians as decoder queries and\nupdates their parameters layer by layer by performing multi-view\ncross-attention (MVDFA) over multiple input images. In this way, multiple views\nnaturally contribute to modeling a unitary representation of 3D Gaussians,\nthereby making 3D reconstruction more view-consistent. Moreover, as the number\nof 3D Gaussians used as decoder queries is irrespective of the number of input\nviews, allow an arbitrary number of input images without causing memory\nexplosion. Extensive experiments validate the advantages of our approach,\nshowcasing superior performance over existing methods quantitatively (improving\nPSNR by 4.2 dB when trained on Objaverse and tested on the GSO benchmark) and\nqualitatively. The code will be released at https://github.com/jwubz123/UNIG.\n","authors":["Jiamin Wu","Kenkun Liu","Yukai Shi","Xiaoke Jiang","Yuan Yao","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.13195v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14189v1","updated":"2024-10-18T05:48:06Z","published":"2024-10-18T05:48:06Z","title":"Neural Signed Distance Function Inference through Splatting 3D Gaussians\n  Pulled on Zero-Level Set","summary":"  It is vital to infer a signed distance function (SDF) in multi-view based\nsurface reconstruction. 3D Gaussian splatting (3DGS) provides a novel\nperspective for volume rendering, and shows advantages in rendering efficiency\nand quality. Although 3DGS provides a promising neural rendering option, it is\nstill hard to infer SDFs for surface reconstruction with 3DGS due to the\ndiscreteness, the sparseness, and the off-surface drift of 3D Gaussians. To\nresolve these issues, we propose a method that seamlessly merge 3DGS with the\nlearning of neural SDFs. Our key idea is to more effectively constrain the SDF\ninference with the multi-view consistency. To this end, we dynamically align 3D\nGaussians on the zero-level set of the neural SDF using neural pulling, and\nthen render the aligned 3D Gaussians through the differentiable rasterization.\nMeanwhile, we update the neural SDF by pulling neighboring space to the pulled\n3D Gaussians, which progressively refine the signed distance field near the\nsurface. With both differentiable pulling and splatting, we jointly optimize 3D\nGaussians and the neural SDF with both RGB and geometry constraints, which\nrecovers more accurate, smooth, and complete surfaces with more geometry\ndetails. Our numerical and visual comparisons show our superiority over the\nstate-of-the-art results on the widely used benchmarks.\n","authors":["Wenyuan Zhang","Yu-Shen Liu","Zhizhong Han"],"pdf_url":"https://arxiv.org/pdf/2410.14189v1.pdf","comment":"Accepted by NeurIPS 2024. Project page:\n  https://wen-yuan-zhang.github.io/GS-Pull/"},{"id":"http://arxiv.org/abs/2406.13123v2","updated":"2024-10-18T05:20:34Z","published":"2024-06-19T00:38:19Z","title":"ViLCo-Bench: VIdeo Language COntinual learning Benchmark","summary":"  Video language continual learning involves continuously adapting to\ninformation from video and text inputs, enhancing a model's ability to handle\nnew tasks while retaining prior knowledge. This field is a relatively\nunder-explored area, and establishing appropriate datasets is crucial for\nfacilitating communication and research in this field. In this study, we\npresent the first dedicated benchmark, ViLCo-Bench, designed to evaluate\ncontinual learning models across a range of video-text tasks. The dataset\ncomprises ten-minute-long videos and corresponding language queries collected\nfrom publicly available datasets. Additionally, we introduce a novel\nmemory-efficient framework that incorporates self-supervised learning and\nmimics long-term and short-term memory effects. This framework addresses\nchallenges including memory complexity from long video clips, natural language\ncomplexity from open queries, and text-video misalignment. We posit that\nViLCo-Bench, with greater complexity compared to existing continual learning\nbenchmarks, would serve as a critical tool for exploring the video-language\ndomain, extending beyond conventional class-incremental tasks, and addressing\ncomplex and limited annotation issues. The curated data, evaluations, and our\nnovel method are available at https://github.com/cruiseresearchgroup/ViLCo.\n","authors":["Tianqi Tang","Shohreh Deldari","Hao Xue","Celso De Melo","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2406.13123v2.pdf","comment":"14 pages, 4 figures, 8 tables, Accepted at NeurIPS Dataset and\n  Benchmark Track 2024"},{"id":"http://arxiv.org/abs/2404.10210v3","updated":"2024-10-18T05:17:29Z","published":"2024-04-16T01:41:22Z","title":"MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and\n  Knowledge Distillation for Skeleton-based Action Recognition","summary":"  In recent years, skeleton-based action recognition, leveraging multimodal\nGraph Convolutional Networks (GCN), has achieved remarkable results. However,\ndue to their deep structure and reliance on continuous floating-point\noperations, GCN-based methods are energy-intensive. We propose an innovative\nSpiking Graph Convolutional Network with Multimodal Fusion and Knowledge\nDistillation (MK-SGN) to address this issue. By merging the energy efficiency\nof Spiking Neural Network (SNN) with the graph representation capability of\nGCN, the proposed MK-SGN reduces energy consumption while maintaining\nrecognition accuracy. Firstly, we convert Graph Convolutional Networks (GCN)\ninto Spiking Graph Convolutional Networks (SGN) establishing a new benchmark\nand paving the way for future research exploration. During this process, we\nintroduce a spiking attention mechanism and design a Spiking-Spatio Graph\nConvolution module with a Spatial Global Spiking Attention mechanism (SA-SGC),\nenhancing feature learning capability. Secondly, we propose a Spiking\nMultimodal Fusion module (SMF), leveraging mutual information to process\nmultimodal data more efficiently. Lastly, we delve into knowledge distillation\nmethods from multimodal GCN to SGN and propose a novel, integrated method that\nsimultaneously focuses on both intermediate layer distillation and soft label\ndistillation to improve the performance of SGN. MK-SGN outperforms the\nstate-of-the-art GCN-like frameworks on three challenging datasets for\nskeleton-based action recognition in reducing energy consumption. It also\noutperforms the state-of-the-art SNN frameworks in accuracy. Specifically, our\nmethod reduces energy consumption by more than 98% compared to typical\nGCN-based methods, while maintaining competitive accuracy on the NTU-RGB+D 60\ncross-subject split using 4-time steps.\n","authors":["Naichuan Zheng","Hailun Xia","Zeyu Liang","Yuanyuan Chai"],"pdf_url":"https://arxiv.org/pdf/2404.10210v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14179v1","updated":"2024-10-18T05:15:50Z","published":"2024-10-18T05:15:50Z","title":"MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart\n  Problems","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nabilities across various tasks, including visual question answering and chart\ncomprehension, yet existing benchmarks for chart-related tasks fall short in\ncapturing the complexity of real-world multi-chart scenarios. Current\nbenchmarks primarily focus on single-chart tasks, neglecting the multi-hop\nreasoning required to extract and integrate information from multiple charts,\nwhich is essential in practical applications. To fill this gap, we introduce\nMultiChartQA, a benchmark that evaluates MLLMs' capabilities in four key areas:\ndirect question answering, parallel question answering, comparative reasoning,\nand sequential reasoning. Our evaluation of a wide range of MLLMs reveals\nsignificant performance gaps compared to humans. These results highlight the\nchallenges in multi-chart comprehension and the potential of MultiChartQA to\ndrive advancements in this field. Our code and data are available at\nhttps://github.com/Zivenzhu/Multi-chart-QA\n","authors":["Zifeng Zhu","Mengzhao Jia","Zhihan Zhang","Lang Li","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.14179v1.pdf","comment":"18 pages, 9 figures"},{"id":"http://arxiv.org/abs/2305.19513v2","updated":"2024-10-18T05:14:57Z","published":"2023-05-31T02:52:38Z","title":"Hard Region Aware Network for Remote Sensing Change Detection","summary":"  Change detection (CD) is essential for various real-world applications, such\nas urban management and disaster assessment. Numerous CD methods have been\nproposed, and considerable results have been achieved recently. However,\ndetecting changes in hard regions, i.e., the change boundary and irrelevant\npseudo changes caused by background clutters, remains difficult for these\nmethods, since they pose equal attention for all regions in bi-temporal images.\nThis paper proposes a novel change detection network, termed as HRANet, which\nprovides accurate change maps via hard region mining. Specifically, an online\nhard region estimation branch is constructed to model the pixel-wise hard\nsamples, supervised by the error between predicted change maps and\ncorresponding ground truth during the training process. A cross-layer knowledge\nreview module is introduced to distill temporal change information from\nlow-level to high-level features, thereby enhancing the feature representation\ncapabilities. Finally, the hard region aware features extracted from the online\nhard region estimation branch and multi-level temporal difference features are\naggregated into a unified feature representation to improve the accuracy of CD.\nExperimental results on two benchmark datasets demonstrate the superior\nperformance of HRANet in the CD task.\n","authors":["Zhenglai Li","Chang Tang","Xinwang Liu","Xingchen Hu","Xianju Li","Ning Li","Changdong Li"],"pdf_url":"https://arxiv.org/pdf/2305.19513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01701v2","updated":"2024-10-18T05:12:00Z","published":"2024-08-03T07:47:16Z","title":"Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action\n  Recognition via Learning Temporal-Frequency Dynamics","summary":"  In skeletal-based action recognition, Graph Convolutional Networks (GCNs)\nbased methods face limitations due to their complexity and high energy\nconsumption. Spiking Neural Networks (SNNs) have gained attention in recent\nyears for their low energy consumption, but existing methods combining GCNs and\nSNNs fail to fully utilize the temporal characteristics of skeletal sequences,\nleading to increased storage and computational costs. To address this issue, we\npropose a Signal-SGN(Spiking Graph Convolutional Network), which leverages the\ntemporal dimension of skeletal sequences as the spiking timestep and treats\nfeatures as discrete stochastic signals. The core of the network consists of a\n1D Spiking Graph Convolutional Network (1D-SGN) and a Frequency Spiking\nConvolutional Network (FSN). The SGN performs graph convolution on single\nframes and incorporates spiking network characteristics to capture inter-frame\ntemporal relationships, while the FSN uses Fast Fourier Transform (FFT) and\ncomplex convolution to extract temporal-frequency features. We also introduce a\nmulti-scale wavelet transform feature fusion module(MWTF) to capture spectral\nfeatures of temporal signals, enhancing the model's classification capability.\nWe propose a pluggable temporal-frequency spatial semantic feature extraction\nmodule(TFSM) to enhance the model's ability to distinguish features without\nincreasing inference-phase consumption. Our numerous experiments on the NTU\nRGB+D, NTU RGB+D 120, and NW-UCLA datasets demonstrate that the proposed models\nnot only surpass existing SNN-based methods in accuracy but also reduce\ncomputational and storage costs during training. Furthermore, they achieve\ncompetitive accuracy compared to corresponding GCN-based methods, which is\nquite remarkable.\n","authors":["Naichuan Zheng","Hailun Xia","Dapeng Liu"],"pdf_url":"https://arxiv.org/pdf/2408.01701v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14178v1","updated":"2024-10-18T05:11:07Z","published":"2024-10-18T05:11:07Z","title":"Feature Augmentation based Test-Time Adaptation","summary":"  Test-time adaptation (TTA) allows a model to be adapted to an unseen domain\nwithout accessing the source data. Due to the nature of practical environments,\nTTA has a limited amount of data for adaptation. Recent TTA methods further\nrestrict this by filtering input data for reliability, making the effective\ndata size even smaller and limiting adaptation potential. To address this\nissue, We propose Feature Augmentation based Test-time Adaptation (FATA), a\nsimple method that fully utilizes the limited amount of input data through\nfeature augmentation. FATA employs Normalization Perturbation to augment\nfeatures and adapts the model using the FATA loss, which makes the outputs of\nthe augmented and original features similar. FATA is model-agnostic and can be\nseamlessly integrated into existing models without altering the model\narchitecture. We demonstrate the effectiveness of FATA on various models and\nscenarios on ImageNet-C and Office-Home, validating its superiority in diverse\nreal-world conditions.\n","authors":["Younggeol Cho","Youngrae Kim","Junho Yoon","Seunghoon Hong","Dongman Lee"],"pdf_url":"https://arxiv.org/pdf/2410.14178v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2410.14177v1","updated":"2024-10-18T05:09:07Z","published":"2024-10-18T05:09:07Z","title":"Learning autonomous driving from aerial imagery","summary":"  In this work, we consider the problem of learning end to end perception to\ncontrol for ground vehicles solely from aerial imagery. Photogrammetric\nsimulators allow the synthesis of novel views through the transformation of\npre-generated assets into novel views.However, they have a large setup cost,\nrequire careful collection of data and often human effort to create usable\nsimulators. We use a Neural Radiance Field (NeRF) as an intermediate\nrepresentation to synthesize novel views from the point of view of a ground\nvehicle. These novel viewpoints can then be used for several downstream\nautonomous navigation applications. In this work, we demonstrate the utility of\nnovel view synthesis though the application of training a policy for end to end\nlearning from images and depth data. In a traditional real to sim to real\nframework, the collected data would be transformed into a visual simulator\nwhich could then be used to generate novel views. In contrast, using a NeRF\nallows a compact representation and the ability to optimize over the parameters\nof the visual simulator as more data is gathered in the environment. We\ndemonstrate the efficacy of our method in a custom built mini-city environment\nthrough the deployment of imitation policies on robotic cars. We additionally\nconsider the task of place localization and demonstrate that our method is able\nto relocalize the car in the real world.\n","authors":["Varun Murali","Guy Rosman","Sertac Karaman","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2410.14177v1.pdf","comment":"Presented at IROS 2024"},{"id":"http://arxiv.org/abs/2402.13876v3","updated":"2024-10-18T05:04:40Z","published":"2024-02-21T15:35:59Z","title":"Scene Prior Filtering for Depth Super-Resolution","summary":"  Multi-modal fusion is vital to the success of super-resolution of depth maps.\nHowever, commonly used fusion strategies, such as addition and concatenation,\nfall short of effectively bridging the modal gap. As a result, guided image\nfiltering methods have been introduced to mitigate this issue. Nevertheless, it\nis observed that their filter kernels usually encounter significant texture\ninterference and edge inaccuracy. To tackle these two challenges, we introduce\na Scene Prior Filtering network, SPFNet, which utilizes the priors surface\nnormal and semantic map from large-scale models. Specifically, we design an\nAll-in-one Prior Propagation that computes the similarity between multi-modal\nscene priors, i.e., RGB, normal, semantic, and depth, to reduce the texture\ninterference. In addition, we present a One-to-one Prior Embedding that\ncontinuously embeds each single-modal prior into depth using Mutual Guided\nFiltering, further alleviating the texture interference while enhancing edges.\nOur SPFNet has been extensively evaluated on both real and synthetic datasets,\nachieving state-of-the-art performance.\n","authors":["Zhengxue Wang","Zhiqiang Yan","Ming-Hsuan Yang","Jinshan Pan","Guangwei Gao","Ying Tai","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2402.13876v3.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2406.14862v4","updated":"2024-10-18T04:39:35Z","published":"2024-06-21T04:39:03Z","title":"LatentExplainer: Explaining Latent Representations in Deep Generative\n  Models with Multi-modal Foundation Models","summary":"  Deep generative models like VAEs and diffusion models have advanced various\ngeneration tasks by leveraging latent variables to learn data distributions and\ngenerate high-quality samples. Despite the field of explainable AI making\nstrides in interpreting machine learning models, understanding latent variables\nin generative models remains challenging. This paper introduces\n\\textit{LatentExplainer}, a framework for automatically generating semantically\nmeaningful explanations of latent variables in deep generative models.\n\\textit{LatentExplainer} tackles three main challenges: inferring the meaning\nof latent variables, aligning explanations with inductive biases, and handling\nvarying degrees of explainability. Our approach perturbs latent variables,\ninterpreting changes in generated data, and uses multi-modal large language\nmodels (MLLMs) to produce human-understandable explanations. We evaluate our\nproposed method on several real-world and synthetic datasets, and the results\ndemonstrate superior performance in generating high-quality explanations for\nlatent variables. The results highlight the effectiveness of incorporating\ninductive biases and uncertainty quantification, significantly enhancing model\ninterpretability.\n","authors":["Mengdan Zhu","Raasikh Kanjiani","Jiahui Lu","Andrew Choi","Qirui Ye","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.14862v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00902v2","updated":"2024-10-18T04:37:33Z","published":"2024-07-01T01:57:21Z","title":"From Introspection to Best Practices: Principled Analysis of\n  Demonstrations in Multimodal In-Context Learning","summary":"  Motivated by in-context learning (ICL) capabilities of Large Language models\n(LLMs), multimodal LLMs with additional visual modality are also exhibited with\nsimilar ICL abilities when multiple image-text pairs are provided as\ndemonstrations. However, relatively less work has been done to investigate the\nprinciples behind how and why multimodal ICL works. We conduct a systematic and\nprincipled evaluation of multimodal ICL for models of different scales on a\nbroad spectrum of new yet critical tasks. Through perturbations over different\nmodality information, we show that modalities matter differently across tasks\nin multimodal ICL. Guided by task-specific modality impact, we recommend\nmodality-driven demonstration strategies to boost ICL performance. We also find\nthat models may follow inductive biases from multimodal ICL even if they are\nrarely seen in or contradict semantic priors from pretraining data. Our\nprincipled analysis provides a comprehensive way of understanding the role of\ndemonstrations in multimodal in-context learning, and sheds light on\neffectively improving multimodal ICL on a wide range of tasks.\n","authors":["Nan Xu","Fei Wang","Sheng Zhang","Hoifung Poon","Muhao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.00902v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12324v2","updated":"2024-10-18T04:32:34Z","published":"2024-10-16T07:44:56Z","title":"PAPL-SLAM: Principal Axis-Anchored Monocular Point-Line SLAM","summary":"  In point-line SLAM systems, the utilization of line structural information\nand the optimization of lines are two significant problems. The former is\nusually addressed through structural regularities, while the latter typically\ninvolves using minimal parameter representations of lines in optimization.\nHowever, separating these two steps leads to the loss of constraint information\nto each other. We anchor lines with similar directions to a principal axis and\noptimize them with $n+2$ parameters for $n$ lines, solving both problems\ntogether. Our method considers scene structural information, which can be\neasily extended to different world hypotheses while significantly reducing the\nnumber of line parameters to be optimized, enabling rapid and accurate mapping\nand tracking. To further enhance the system's robustness and avoid mismatch, we\nhave modeled the line-axis probabilistic data association and provided the\nalgorithm for axis creation, updating, and optimization. Additionally,\nconsidering that most real-world scenes conform to the Atlanta World\nhypothesis, we provide a structural line detection strategy based on vertical\npriors and vanishing points. Experimental results and ablation studies on\nvarious indoor and outdoor datasets demonstrate the effectiveness of our\nsystem.\n","authors":["Guanghao Li","Yu Cao","Qi Chen","Yifan Yang","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2410.12324v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.04733v2","updated":"2024-10-18T04:28:28Z","published":"2024-10-07T03:52:06Z","title":"PredFormer: Transformers Are Effective Spatial-Temporal Predictive\n  Learners","summary":"  Spatiotemporal predictive learning methods generally fall into two\ncategories: recurrent-based approaches, which face challenges in\nparallelization and performance, and recurrent-free methods, which employ\nconvolutional neural networks (CNNs) as encoder-decoder architectures. These\nmethods benefit from strong inductive biases but often at the expense of\nscalability and generalization. This paper proposes PredFormer, a pure\ntransformer-based framework for spatiotemporal predictive learning. Motivated\nby the Vision Transformers (ViT) design, PredFormer leverages carefully\ndesigned Gated Transformer blocks, following a comprehensive analysis of 3D\nattention mechanisms, including full-, factorized-, and\ninterleaved-spatial-temporal attention. With its recurrent-free,\ntransformer-based design, PredFormer is both simple and efficient,\nsignificantly outperforming previous methods by large margins. Extensive\nexperiments on synthetic and real-world datasets demonstrate that PredFormer\nachieves state-of-the-art performance. On Moving MNIST, PredFormer achieves a\n51.3% reduction in MSE relative to SimVP. For TaxiBJ, the model decreases MSE\nby 33.1% and boosts FPS from 533 to 2364. Additionally, on WeatherBench, it\nreduces MSE by 11.1% while enhancing FPS from 196 to 404. These performance\ngains in both accuracy and efficiency demonstrate PredFormer's potential for\nreal-world applications. The source code will be released at\nhttps://github.com/yyyujintang/PredFormer .\n","authors":["Yujin Tang","Lu Qi","Fei Xie","Xiangtai Li","Chao Ma","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2410.04733v2.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2407.04127v2","updated":"2024-10-18T04:23:00Z","published":"2024-07-04T19:00:34Z","title":"Biometric Authentication Based on Enhanced Remote Photoplethysmography\n  Signal Morphology","summary":"  Remote photoplethysmography (rPPG) is a non-contact method for measuring\ncardiac signals from facial videos, offering a convenient alternative to\ncontact photoplethysmography (cPPG) obtained from contact sensors. Recent\nstudies have shown that each individual possesses a unique cPPG signal\nmorphology that can be utilized as a biometric identifier, which has inspired\nus to utilize the morphology of rPPG signals extracted from facial videos for\nperson authentication. Since the facial appearance and rPPG are mixed in the\nfacial videos, we first de-identify facial videos to remove facial appearance\nwhile preserving the rPPG information, which protects facial privacy and\nguarantees that only rPPG is used for authentication. The de-identified videos\nare fed into an rPPG model to get the rPPG signal morphology for\nauthentication. In the first training stage, unsupervised rPPG training is\nperformed to get coarse rPPG signals. In the second training stage, an\nrPPG-cPPG hybrid training is performed by incorporating external cPPG datasets\nto achieve rPPG biometric authentication and enhance rPPG signal morphology.\nOur approach needs only de-identified facial videos with subject IDs to train\nrPPG authentication models. The experimental results demonstrate that rPPG\nsignal morphology hidden in facial videos can be used for biometric\nauthentication. The code is available at\nhttps://github.com/zhaodongsun/rppg_biometrics.\n","authors":["Zhaodong Sun","Xiaobai Li","Jukka Komulainen","Guoying Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.04127v2.pdf","comment":"accepted by IJCB 2024, Best Paper Runner-Up Award"},{"id":"http://arxiv.org/abs/2410.14169v1","updated":"2024-10-18T04:19:10Z","published":"2024-10-18T04:19:10Z","title":"DaRePlane: Direction-aware Representations for Dynamic Scene\n  Reconstruction","summary":"  Numerous recent approaches to modeling and re-rendering dynamic scenes\nleverage plane-based explicit representations, addressing slow training times\nassociated with models like neural radiance fields (NeRF) and Gaussian\nsplatting (GS). However, merely decomposing 4D dynamic scenes into multiple 2D\nplane-based representations is insufficient for high-fidelity re-rendering of\nscenes with complex motions. In response, we present DaRePlane, a novel\ndirection-aware representation approach that captures scene dynamics from six\ndifferent directions. This learned representation undergoes an inverse\ndual-tree complex wavelet transformation (DTCWT) to recover plane-based\ninformation. Within NeRF pipelines, DaRePlane computes features for each\nspace-time point by fusing vectors from these recovered planes, then passed to\na tiny MLP for color regression. When applied to Gaussian splatting, DaRePlane\ncomputes the features of Gaussian points, followed by a tiny multi-head MLP for\nspatial-time deformation prediction. Notably, to address redundancy introduced\nby the six real and six imaginary direction-aware wavelet coefficients, we\nintroduce a trainable masking approach, mitigating storage issues without\nsignificant performance decline. To demonstrate the generality and efficiency\nof DaRePlane, we test it on both regular and surgical dynamic scenes, for both\nNeRF and GS systems. Extensive experiments show that DaRePlane yields\nstate-of-the-art performance in novel view synthesis for various complex\ndynamic scenes.\n","authors":["Ange Lou","Benjamin Planche","Zhongpai Gao","Yamin Li","Tianyu Luan","Hao Ding","Meng Zheng","Terrence Chen","Ziyan Wu","Jack Noble"],"pdf_url":"https://arxiv.org/pdf/2410.14169v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2403.02265"},{"id":"http://arxiv.org/abs/2410.13726v2","updated":"2024-10-18T04:19:02Z","published":"2024-10-17T16:32:36Z","title":"DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework\n  for Talking Head Video Generation","summary":"  Talking head generation intends to produce vivid and realistic talking head\nvideos from a single portrait and speech audio clip. Although significant\nprogress has been made in diffusion-based talking head generation, almost all\nmethods rely on autoregressive strategies, which suffer from limited context\nutilization beyond the current generation step, error accumulation, and slower\ngeneration speed. To address these challenges, we present DAWN (Dynamic frame\nAvatar With Non-autoregressive diffusion), a framework that enables all-at-once\ngeneration of dynamic-length video sequences. Specifically, it consists of two\nmain components: (1) audio-driven holistic facial dynamics generation in the\nlatent motion space, and (2) audio-driven head pose and blink generation.\nExtensive experiments demonstrate that our method generates authentic and vivid\nvideos with precise lip motions, and natural pose/blink movements.\nAdditionally, with a high generation speed, DAWN possesses strong extrapolation\ncapabilities, ensuring the stable production of high-quality long videos. These\nresults highlight the considerable promise and potential impact of DAWN in the\nfield of talking head video generation. Furthermore, we hope that DAWN sparks\nfurther exploration of non-autoregressive approaches in diffusion models. Our\ncode will be publicly available at https://github.com/Hanbo-Cheng/DAWN-pytorch.\n","authors":["Hanbo Cheng","Limin Lin","Chenyu Liu","Pengcheng Xia","Pengfei Hu","Jiefeng Ma","Jun Du","Jia Pan"],"pdf_url":"https://arxiv.org/pdf/2410.13726v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14164v1","updated":"2024-10-18T04:04:58Z","published":"2024-10-18T04:04:58Z","title":"Optimal DLT-based Solutions for the Perspective-n-Point","summary":"  We propose a modified normalized direct linear transform (DLT) algorithm for\nsolving the perspective-n-point (PnP) problem with much better behavior than\nthe conventional DLT. The modification consists of analytically weighting the\ndifferent measurements in the linear system with a negligible increase in\ncomputational load. Our approach exhibits clear improvements -- in both\nperformance and runtime -- when compared to popular methods such as EPnP, CPnP,\nRPnP, and OPnP. Our new non-iterative solution approaches that of the true\noptimal found via Gauss-Newton optimization, but at a fraction of the\ncomputational cost. Our optimal DLT (oDLT) implementation, as well as the\nexperiments, are released in open source.\n","authors":["Sébastien Henry","John A. Christian"],"pdf_url":"https://arxiv.org/pdf/2410.14164v1.pdf","comment":"8 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.14161v1","updated":"2024-10-18T04:00:26Z","published":"2024-10-18T04:00:26Z","title":"Unlabeled Action Quality Assessment Based on Multi-dimensional Adaptive\n  Constrained Dynamic Time Warping","summary":"  The growing popularity of online sports and exercise necessitates effective\nmethods for evaluating the quality of online exercise executions. Previous\naction quality assessment methods, which relied on labeled scores from motion\nvideos, exhibited slightly lower accuracy and discriminability. This limitation\nhindered their rapid application to newly added exercises. To address this\nproblem, this paper presents an unlabeled Multi-Dimensional Exercise Distance\nAdaptive Constrained Dynamic Time Warping (MED-ACDTW) method for action quality\nassessment. Our approach uses an athletic version of DTW to compare features\nfrom template and test videos, eliminating the need for score labels during\ntraining. The result shows that utilizing both 2D and 3D spatial dimensions,\nalong with multiple human body features, improves the accuracy by 2-3% compared\nto using either 2D or 3D pose estimation alone. Additionally, employing MED for\nscore calculation enhances the precision of frame distance matching, which\nsignificantly boosts overall discriminability. The adaptive constraint scheme\nenhances the discriminability of action quality assessment by approximately\n30%. Furthermore, to address the absence of a standardized perspective in\nsports class evaluations, we introduce a new dataset called BGym.\n","authors":["Renguang Chen","Guolong Zheng","Xu Yang","Zhide Chen","Jiwu Shu","Wencheng Yang","Kexin Zhu","Chen Feng"],"pdf_url":"https://arxiv.org/pdf/2410.14161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14159v1","updated":"2024-10-18T03:58:29Z","published":"2024-10-18T03:58:29Z","title":"Assessing Open-world Forgetting in Generative Image Model Customization","summary":"  Recent advances in diffusion models have significantly enhanced image\ngeneration capabilities. However, customizing these models with new classes\noften leads to unintended consequences that compromise their reliability. We\nintroduce the concept of open-world forgetting to emphasize the vast scope of\nthese unintended alterations, contrasting it with the well-studied closed-world\nforgetting, which is measurable by evaluating performance on a limited set of\nclasses or skills. Our research presents the first comprehensive investigation\ninto open-world forgetting in diffusion models, focusing on semantic and\nappearance drift of representations. We utilize zero-shot classification to\nanalyze semantic drift, revealing that even minor model adaptations lead to\nunpredictable shifts affecting areas far beyond newly introduced concepts, with\ndramatic drops in zero-shot classification of up to 60%. Additionally, we\nobserve significant changes in texture and color of generated content when\nanalyzing appearance drift. To address these issues, we propose a mitigation\nstrategy based on functional regularization, designed to preserve original\ncapabilities while accommodating new concepts. Our study aims to raise\nawareness of unintended changes due to model customization and advocates for\nthe analysis of open-world forgetting in future research on model customization\nand finetuning methods. Furthermore, we provide insights for developing more\nrobust adaptation methodologies.\n","authors":["Héctor Laria","Alex Gomez-Villa","Imad Eddine Marouf","Kai Wang","Bogdan Raducanu","Joost van de Weijer"],"pdf_url":"https://arxiv.org/pdf/2410.14159v1.pdf","comment":"Project page: https://hecoding.github.io/open-world-forgetting/"},{"id":"http://arxiv.org/abs/2410.14148v1","updated":"2024-10-18T03:34:32Z","published":"2024-10-18T03:34:32Z","title":"Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment","summary":"  The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.\n","authors":["Chenhang Cui","An Zhang","Yiyang Zhou","Zhaorun Chen","Gelei Deng","Huaxiu Yao","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.14148v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2410.14143v1","updated":"2024-10-18T03:31:00Z","published":"2024-10-18T03:31:00Z","title":"Preview-based Category Contrastive Learning for Knowledge Distillation","summary":"  Knowledge distillation is a mainstream algorithm in model compression by\ntransferring knowledge from the larger model (teacher) to the smaller model\n(student) to improve the performance of student. Despite many efforts, existing\nmethods mainly investigate the consistency between instance-level feature\nrepresentation or prediction, which neglects the category-level information and\nthe difficulty of each sample, leading to undesirable performance. To address\nthese issues, we propose a novel preview-based category contrastive learning\nmethod for knowledge distillation (PCKD). It first distills the structural\nknowledge of both instance-level feature correspondence and the relation\nbetween instance features and category centers in a contrastive learning\nfashion, which can explicitly optimize the category representation and explore\nthe distinct correlation between representations of instances and categories,\ncontributing to discriminative category centers and better classification\nresults. Besides, we introduce a novel preview strategy to dynamically\ndetermine how much the student should learn from each sample according to their\ndifficulty. Different from existing methods that treat all samples equally and\ncurriculum learning that simply filters out hard samples, our method assigns a\nsmall weight for hard instances as a preview to better guide the student\ntraining. Extensive experiments on several challenging datasets, including\nCIFAR-100 and ImageNet, demonstrate the superiority over state-of-the-art\nmethods.\n","authors":["Muhe Ding","Jianlong Wu","Xue Dong","Xiaojie Li","Pengda Qin","Tian Gan","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2410.14143v1.pdf","comment":"14 pages, 8 figures, Journal"},{"id":"http://arxiv.org/abs/2410.13674v2","updated":"2024-10-18T03:28:38Z","published":"2024-10-17T15:33:35Z","title":"Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning\n  via Image-Guided Diffusion","summary":"  Low-quality or scarce data has posed significant challenges for training deep\nneural networks in practice. While classical data augmentation cannot\ncontribute very different new data, diffusion models opens up a new door to\nbuild self-evolving AI by generating high-quality and diverse synthetic data\nthrough text-guided prompts. However, text-only guidance cannot control\nsynthetic images' proximity to the original images, resulting in\nout-of-distribution data detrimental to the model performance. To overcome the\nlimitation, we study image guidance to achieve a spectrum of interpolations\nbetween synthetic and real images. With stronger image guidance, the generated\nimages are similar to the training data but hard to learn. While with weaker\nimage guidance, the synthetic images will be easier for model but contribute to\na larger distribution gap with the original data. The generated full spectrum\nof data enables us to build a novel \"Diffusion Curriculum (DisCL)\". DisCL\nadjusts the image guidance level of image synthesis for each training stage: It\nidentifies and focuses on hard samples for the model and assesses the most\neffective guidance level of synthetic images to improve hard data learning. We\napply DisCL to two challenging tasks: long-tail (LT) classification and\nlearning from low-quality data. It focuses on lower-guidance images of\nhigh-quality to learn prototypical features as a warm-up of learning\nhigher-guidance images that might be weak on diversity or quality. Extensive\nexperiments showcase a gain of 2.7% and 2.1% in OOD and ID macro-accuracy when\napplying DisCL to iWildCam dataset. On ImageNet-LT, DisCL improves the base\nmodel's tail-class accuracy from 4.4% to 23.64% and leads to a 4.02%\nimprovement in all-class accuracy.\n","authors":["Yijun Liang","Shweta Bhardwaj","Tianyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.13674v2.pdf","comment":"23 pages, including references and appendix. Code is available at\n  http://github.com/tianyi-lab/DisCL"},{"id":"http://arxiv.org/abs/2410.02052v3","updated":"2024-10-18T03:27:37Z","published":"2024-10-02T21:42:35Z","title":"ExACT: Teaching AI Agents to Explore with Reflective-MCTS and\n  Exploratory Learning","summary":"  Autonomous agents have demonstrated significant potential in automating\ncomplex multistep decision-making tasks. However, even state-of-the-art\nvision-language models (VLMs), such as GPT-4o, still fall short of human-level\nperformance, particularly in intricate web environments and long-horizon tasks.\nTo address these limitations, we present ExACT, an approach to combine\ntest-time search and self-learning to build o1-like models for agentic\napplications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a\nnovel test time algorithm designed to enhance AI agents' ability to explore\ndecision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating\ncontrastive reflection, allowing agents to learn from past interactions and\ndynamically improve their search efficiency; and 2) using multi-agent debate\nfor reliable state evaluation. Next, we introduce Exploratory Learning, a novel\nlearning strategy to teach agents to search at inference time without relying\non any external search algorithms. On the challenging VisualWebArena benchmark,\nour GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across\nvarious tasks compared to the previous state-of-the-art. Additionally, we show\nthat the knowledge and experience gained from test-time search can be\neffectively transferred back to GPT-4o via fine-tuning. After Exploratory\nLearning, GPT-4o 1) demonstrates the ability to explore the environment,\nevaluate a state, and backtrack to viable ones when it detects that the current\nstate cannot lead to success, and 2) matches 87% of R-MCTS's performance while\nusing significantly less compute. Notably, our work demonstrates the compute\nscaling properties in both training - data collection with R-MCTS - and testing\ntime. These results suggest a promising research direction to enhance VLMs'\ncapabilities for agentic applications via test-time search and self-learning.\n","authors":["Xiao Yu","Baolin Peng","Vineeth Vajipey","Hao Cheng","Michel Galley","Jianfeng Gao","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02052v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14138v1","updated":"2024-10-18T03:22:06Z","published":"2024-10-18T03:22:06Z","title":"ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and\n  Wisdom","summary":"  Large vision-language models (LVLMs) have witnessed significant progress on\nvisual understanding tasks. However, they often prioritize language knowledge\nover image information on visual reasoning tasks, incurring performance\ndegradation. To tackle this issue, we first identify the drawbacks of existing\nsolutions (i.e., insufficient and irrelevant visual descriptions, and limited\nmulti-modal capacities). We then decompose visual reasoning process into two\nstages: visual perception (i.e., eyesight) and textual reasoning (i.e.,\nwisdom), and introduce a novel visual reasoning framework named ProReason. This\nframework features multi-run proactive perception and decoupled\nvision-reasoning capabilities. Briefly, given a multi-modal question, ProReason\niterates proactive information collection and reasoning until the answer can be\nconcluded with necessary and sufficient visual descriptions. Notably, the\ndisassociation of capabilities allows seamless integration of existing large\nlanguage models (LLMs) to compensate for the reasoning deficits of LVLMs. Our\nextensive experiments demonstrate that ProReason outperforms both existing\nmulti-step reasoning frameworks and passive peer methods on a wide range of\nbenchmarks for both open-source and closed-source models. In addition, with the\nassistance of LLMs, ProReason achieves a performance improvement of up to 15%\non MMMU benchmark. Our insights into existing solutions and the decoupled\nperspective for feasible integration of LLMs illuminate future research on\nvisual reasoning techniques, especially LLM-assisted ones.\n","authors":["Jingqi Zhou","Sheng Wang","Jingwei Dong","Lei Li","Jiahui Gao","Lingpeng Kong","Chuan Wu"],"pdf_url":"https://arxiv.org/pdf/2410.14138v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14132v1","updated":"2024-10-18T03:00:03Z","published":"2024-10-18T03:00:03Z","title":"ViConsFormer: Constituting Meaningful Phrases of Scene Texts using\n  Transformer-based Method in Vietnamese Text-based Visual Question Answering","summary":"  Text-based VQA is a challenging task that requires machines to use scene\ntexts in given images to yield the most appropriate answer for the given\nquestion. The main challenge of text-based VQA is exploiting the meaning and\ninformation from scene texts. Recent studies tackled this challenge by\nconsidering the spatial information of scene texts in images via embedding 2D\ncoordinates of their bounding boxes. In this study, we follow the definition of\nmeaning from linguistics to introduce a novel method that effectively exploits\nthe information from scene texts written in Vietnamese. Experimental results\nshow that our proposed method obtains state-of-the-art results on two\nlarge-scale Vietnamese Text-based VQA datasets. The implementation can be found\nat this link.\n","authors":["Nghia Hieu Nguyen","Tho Thanh Quan","Ngan Luu-Thuy Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.14132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14131v1","updated":"2024-10-18T02:57:14Z","published":"2024-10-18T02:57:14Z","title":"Deep Learning Applications in Medical Image Analysis: Advancements,\n  Challenges, and Future Directions","summary":"  Medical image analysis has emerged as an essential element of contemporary\nhealthcare, facilitating physicians in achieving expedited and precise\ndiagnosis. Recent breakthroughs in deep learning, a subset of artificial\nintelligence, have markedly revolutionized the analysis of medical pictures,\nimproving the accuracy and efficiency of clinical procedures. Deep learning\nalgorithms, especially convolutional neural networks (CNNs), have demonstrated\nremarkable proficiency in autonomously learning features from multidimensional\nmedical pictures, including MRI, CT, and X-ray scans, without the necessity for\nmanual feature extraction. These models have been utilized across multiple\nmedical disciplines, including pathology, radiology, ophthalmology, and\ncardiology, where they aid in illness detection, classification, and\nsegmentation tasks......\n","authors":["Aimina Ali Eli","Abida Ali"],"pdf_url":"https://arxiv.org/pdf/2410.14131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11656v2","updated":"2024-10-18T02:34:02Z","published":"2023-11-20T10:45:39Z","title":"Double-Condensing Attention Condenser: Leveraging Attention in Deep\n  Learning to Detect Skin Cancer from Skin Lesion Images","summary":"  Skin cancer is the most common type of cancer in the United States and is\nestimated to affect one in five Americans. Recent advances have demonstrated\nstrong performance on skin cancer detection, as exemplified by state of the art\nperformance in the SIIM-ISIC Melanoma Classification Challenge; however these\nsolutions leverage ensembles of complex deep neural architectures requiring\nimmense storage and compute costs, and therefore may not be tractable. A recent\nmovement for TinyML applications is integrating Double-Condensing Attention\nCondensers (DC-AC) into a self-attention neural network backbone architecture\nto allow for faster and more efficient computation. This paper explores\nleveraging an efficient self-attention structure to detect skin cancer in skin\nlesion images and introduces a deep neural network design with DC-AC customized\nfor skin cancer detection from skin lesion images. The final model is publicly\navailable as a part of a global open-source initiative dedicated to\naccelerating advancement in machine learning to aid clinicians in the fight\nagainst cancer. Future work of this research includes iterating on the design\nof the selected network architecture and refining the approach to generalize to\nother forms of cancer.\n","authors":["Chi-en Amy Tai","Elizabeth Janes","Chris Czarnecki","Alexander Wong"],"pdf_url":"https://arxiv.org/pdf/2311.11656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05617v2","updated":"2024-10-18T02:15:51Z","published":"2024-08-10T19:31:21Z","title":"Residual-INR: Communication Efficient On-Device Learning Using Implicit\n  Neural Representation","summary":"  Edge computing is a distributed computing paradigm that collects and\nprocesses data at or near the source of data generation. The on-device learning\nat edge relies on device-to-device wireless communication to facilitate\nreal-time data sharing and collaborative decision-making among multiple\ndevices. This significantly improves the adaptability of the edge computing\nsystem to the changing environments. However, as the scale of the edge\ncomputing system is getting larger, communication among devices is becoming the\nbottleneck because of the limited bandwidth of wireless communication leads to\nlarge data transfer latency. To reduce the amount of device-to-device data\ntransmission and accelerate on-device learning, in this paper, we propose\nResidual-INR, a fog computing-based communication-efficient on-device learning\nframework by utilizing implicit neural representation (INR) to compress\nimages/videos into neural network weights. Residual-INR enhances data transfer\nefficiency by collecting JPEG images from edge devices, compressing them into\nINR format at the fog node, and redistributing them for on-device learning. By\nusing a smaller INR for full image encoding and a separate object INR for\nhigh-quality object region reconstruction through residual encoding, our\ntechnique can reduce the encoding redundancy while maintaining the object\nquality. Residual-INR is a promising solution for edge on-device learning\nbecause it reduces data transmission by up to 5.16 x across a network of 10\nedge devices. It also facilitates CPU-free accelerated on-device learning,\nachieving up to 2.9 x speedup without sacrificing accuracy. Our code is\navailable at: https://github.com/sharclab/Residual-INR.\n","authors":["Hanqiu Chen","Xuebin Yao","Pradeep Subedi","Cong Hao"],"pdf_url":"https://arxiv.org/pdf/2408.05617v2.pdf","comment":"This paper has been accepted by ICCAD 2024"},{"id":"http://arxiv.org/abs/2407.01003v3","updated":"2024-10-18T01:50:27Z","published":"2024-07-01T06:35:53Z","title":"Embedded Prompt Tuning: Towards Enhanced Calibration of Pretrained\n  Models for Medical Images","summary":"  Foundation models pre-trained on large-scale data have been widely witnessed\nto achieve success in various natural imaging downstream tasks.\nParameter-efficient fine-tuning (PEFT) methods aim to adapt foundation models\nto new domains by updating only a small portion of parameters in order to\nreduce computational overhead. However, the effectiveness of these PEFT\nmethods, especially in cross-domain few-shot scenarios, e.g., medical image\nanalysis, has not been fully explored. In this work, we facilitate the study of\nthe performance of PEFT when adapting foundation models to medical image\nclassification tasks. Furthermore, to alleviate the limitations of prompt\nintroducing ways and approximation capabilities on Transformer architectures of\nmainstream prompt tuning methods, we propose the Embedded Prompt Tuning (EPT)\nmethod by embedding prompt tokens into the expanded channels. We also find that\nthere are anomalies in the feature space distribution of foundation models\nduring pre-training process, and prompt tuning can help mitigate this negative\nimpact. To explain this phenomenon, we also introduce a novel perspective to\nunderstand prompt tuning: Prompt tuning is a distribution calibrator. And we\nsupport it by analyzing patch-wise scaling and feature separation operations\ncontained in EPT. Our experiments show that EPT outperforms several\nstate-of-the-art fine-tuning methods by a significant margin on few-shot\nmedical image classification tasks, and completes the fine-tuning process\nwithin highly competitive time, indicating EPT is an effective PEFT method. The\nsource code is available at github.com/zuwenqiang/EPT.\n","authors":["Wenqiang Zu","Shenghao Xie","Qing Zhao","Guoqi Li","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2407.01003v3.pdf","comment":"16 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:2306.09579, arXiv:2203.12119 by other authors"},{"id":"http://arxiv.org/abs/2402.10403v3","updated":"2024-10-18T01:44:05Z","published":"2024-02-16T02:01:24Z","title":"Polyhedral Complex Derivation from Piecewise Trilinear Networks","summary":"  Recent advancements in visualizing deep neural networks provide insights into\ntheir structures and mesh extraction from Continuous Piecewise Affine (CPWA)\nfunctions. Meanwhile, developments in neural surface representation learning\nincorporate non-linear positional encoding, addressing issues like spectral\nbias; however, this poses challenges in applying mesh extraction techniques\nbased on CPWA functions. Focusing on trilinear interpolating methods as\npositional encoding, we present theoretical insights and an analytical mesh\nextraction, showing the transformation of hypersurfaces to flat planes within\nthe trilinear region under the eikonal constraint. Moreover, we introduce a\nmethod for approximating intersecting points among three hypersurfaces\ncontributing to broader applications. We empirically validate correctness and\nparsimony through chamfer distance and efficiency, and angular distance, while\nexamining the correlation between the eikonal loss and the planarity of the\nhypersurfaces.\n","authors":["Jin-Hwa Kim"],"pdf_url":"https://arxiv.org/pdf/2402.10403v3.pdf","comment":"Accepted at NeurIPS 2024. Updated with the camera-ready version"},{"id":"http://arxiv.org/abs/2406.14878v2","updated":"2024-10-18T01:40:19Z","published":"2024-06-21T05:58:19Z","title":"MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object\n  Detection","summary":"  LiDAR-based 3D object detection is crucial for various applications but often\nexperiences performance degradation in real-world deployments due to domain\nshifts. While most studies focus on cross-dataset shifts, such as changes in\nenvironments and object geometries, practical corruptions from sensor\nvariations and weather conditions remain underexplored. In this work, we\npropose a novel online test-time adaptation framework for 3D detectors that\neffectively tackles these shifts, including a challenging cross-corruption\nscenario where cross-dataset shifts and corruptions co-occur. By leveraging\nlong-term knowledge from previous test batches, our approach mitigates\ncatastrophic forgetting and adapts effectively to diverse shifts. Specifically,\nwe propose a Model Synergy (MOS) strategy that dynamically selects historical\ncheckpoints with diverse knowledge and assembles them to best accommodate the\ncurrent test batch. This assembly is directed by our proposed Synergy Weights\n(SW), which perform a weighted averaging of the selected checkpoints,\nminimizing redundancy in the composite model. The SWs are computed by\nevaluating the similarity of predicted bounding boxes on the test data and the\nindependence of features between checkpoint pairs in the model bank. To\nmaintain an efficient and informative model bank, we discard checkpoints with\nthe lowest average SW scores, replacing them with newly updated models. Our\nmethod was rigorously tested against existing test-time adaptation strategies\nacross three datasets and eight types of corruptions, demonstrating superior\nadaptability to dynamic scenes and conditions. Notably, it achieved a 67.3%\nimprovement in a challenging cross-corruption scenario, offering a more\ncomprehensive benchmark for adaptation. The source code will be made publicly\navailable.\n","authors":["Zhuoxiao Chen","Junjie Meng","Mahsa Baktashmotlagh","Yonggang Zhang","Zi Huang","Yadan Luo"],"pdf_url":"https://arxiv.org/pdf/2406.14878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14103v1","updated":"2024-10-18T00:50:56Z","published":"2024-10-18T00:50:56Z","title":"Extreme Precipitation Nowcasting using Multi-Task Latent Diffusion\n  Models","summary":"  Deep learning models have made remarkable strides in precipitation\nprediction, yet they continue to struggle with capturing the spatial details of\nthe features of radar images, particularly over high precipitation intensity\nareas. This shortcoming is evident in the form of low forecast accuracy in the\nspatial positioning of radar echo images across varying precipitation intensity\nregions. To address this challenge, we introduce the multi-task latent\ndiffusion model(MTLDM), a novel approach for precipitation prediction. The\nbasic concept of the MTLDM is based on the understanding that the radar image\nrepresenting precipitation is the result of multiple factors. Therefore, we\nadopt a divide-and-conquer approach, that is, we decompose the radar image\nusing decomposition technology and then predict the decomposed sub-images\nseparately. We conceptualize the precipitation image as a composition of\nvarious components corresponding to different precipitation intensities. The\nMTLDM decomposes the precipitation image into these distinct components and\nemploys a dedicated task to predict each one. This method enables\nspatiotemporally consistent prediction of real-world precipitation areas up to\n5-80 min in advance, outperforming existing state-of-the-art techniques across\nmultiple evaluation metrics.\n","authors":["Li Chaorong","Ling Xudong","Yang Qiang","Qin Fengqing","Huang Yuanyuan"],"pdf_url":"https://arxiv.org/pdf/2410.14103v1.pdf","comment":"12 pages, 6figures"},{"id":"http://arxiv.org/abs/2410.03302v3","updated":"2024-10-18T00:46:30Z","published":"2024-10-04T10:36:22Z","title":"Action Selection Learning for Multi-label Multi-view Action Recognition","summary":"  Multi-label multi-view action recognition aims to recognize multiple\nconcurrent or sequential actions from untrimmed videos captured by multiple\ncameras. Existing work has focused on multi-view action recognition in a narrow\narea with strong labels available, where the onset and offset of each action\nare labeled at the frame-level. This study focuses on real-world scenarios\nwhere cameras are distributed to capture a wide-range area with only weak\nlabels available at the video-level. We propose the method named Multi-view\nAction Selection Learning (MultiASL), which leverages action selection learning\nto enhance view fusion by selecting the most useful information from different\nviewpoints. The proposed method includes a Multi-view Spatial-Temporal\nTransformer video encoder to extract spatial and temporal features from\nmulti-viewpoint videos. Action Selection Learning is employed at the\nframe-level, using pseudo ground-truth obtained from weak labels at the\nvideo-level, to identify the most relevant frames for action recognition.\nExperiments in a real-world office environment using the MM-Office dataset\ndemonstrate the superior performance of the proposed method compared to\nexisting methods. The source code is available at\nhttps://github.com/thanhhff/MultiASL/.\n","authors":["Trung Thanh Nguyen","Yasutomo Kawanishi","Takahiro Komamizu","Ichiro Ide"],"pdf_url":"https://arxiv.org/pdf/2410.03302v3.pdf","comment":"ACM Multimedia Asia 2024"},{"id":"http://arxiv.org/abs/2410.14093v1","updated":"2024-10-18T00:18:27Z","published":"2024-10-18T00:18:27Z","title":"Enhancing In-vehicle Multiple Object Tracking Systems with Embeddable\n  Ising Machines","summary":"  A cognitive function of tracking multiple objects, needed in autonomous\nmobile vehicles, comprises object detection and their temporal association.\nWhile great progress owing to machine learning has been recently seen for\nelaborating the similarity matrix between the objects that have been recognized\nand the objects detected in a current video frame, less for the assignment\nproblem that finally determines the temporal association, which is a\ncombinatorial optimization problem. Here we show an in-vehicle multiple object\ntracking system with a flexible assignment function for tracking through\nmultiple long-term occlusion events. To solve the flexible assignment problem\nformulated as a nondeterministic polynomial time-hard problem, the system\nrelies on an embeddable Ising machine based on a quantum-inspired algorithm\ncalled simulated bifurcation. Using a vehicle-mountable computing platform, we\ndemonstrate a realtime system-wide throughput (23 frames per second on average)\nwith the enhanced functionality.\n","authors":["Kosuke Tatsumura","Yohei Hamakawa","Masaya Yamasaki","Koji Oya","Hiroshi Fujimoto"],"pdf_url":"https://arxiv.org/pdf/2410.14093v1.pdf","comment":"18 pages, 7 figures, 2 tables"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.08821v3","updated":"2024-10-18T17:50:57Z","published":"2024-08-16T16:09:59Z","title":"EasyRec: Simple yet Effective Language Models for Recommendation","summary":"  Deep neural networks have become a powerful technique for learning\nrepresentations from user-item interaction data in collaborative filtering (CF)\nfor recommender systems. However, many existing methods heavily rely on unique\nuser and item IDs, which limits their ability to perform well in practical\nzero-shot learning scenarios where sufficient training data may be unavailable.\nInspired by the success of language models (LMs) and their strong\ngeneralization capabilities, a crucial question arises: How can we harness the\npotential of language models to empower recommender systems and elevate its\ngeneralization capabilities to new heights? In this study, we propose EasyRec -\nan effective and easy-to-use approach that seamlessly integrates text-based\nsemantic understanding with collaborative signals. EasyRec employs a\ntext-behavior alignment framework, which combines contrastive learning with\ncollaborative language model tuning, to ensure a strong alignment between the\ntext-enhanced semantic space and the collaborative behavior information.\nExtensive empirical evaluations across diverse real-world datasets demonstrate\nthe superior performance of EasyRec compared to state-of-the-art alternative\nmodels, particularly in the challenging text-based zero-shot recommendation\nscenarios. Furthermore, the study highlights the potential of seamlessly\nintegrating EasyRec as a plug-and-play component into text-enhanced\ncollaborative filtering frameworks, thereby empowering existing recommender\nsystems to elevate their recommendation performance and adapt to the evolving\nuser preferences in dynamic environments. For better result reproducibility of\nour EasyRec framework, the model implementation details, source code, and\ndatasets are available at the link: https://github.com/HKUDS/EasyRec.\n","authors":["Xubin Ren","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2408.08821v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14629v1","updated":"2024-10-18T17:30:17Z","published":"2024-10-18T17:30:17Z","title":"SIMformer: Single-Layer Vanilla Transformer Can Learn Free-Space\n  Trajectory Similarity","summary":"  Free-space trajectory similarity calculation, e.g., DTW, Hausdorff, and\nFrechet, often incur quadratic time complexity, thus learning-based methods\nhave been proposed to accelerate the computation. The core idea is to train an\nencoder to transform trajectories into representation vectors and then compute\nvector similarity to approximate the ground truth. However, existing methods\nface dual challenges of effectiveness and efficiency: 1) they all utilize\nEuclidean distance to compute representation similarity, which leads to the\nsevere curse of dimensionality issue -- reducing the distinguishability among\nrepresentations and significantly affecting the accuracy of subsequent\nsimilarity search tasks; 2) most of them are trained in triplets manner and\noften necessitate additional information which downgrades the efficiency; 3)\nprevious studies, while emphasizing the scalability in terms of efficiency,\noverlooked the deterioration of effectiveness when the dataset size grows. To\ncope with these issues, we propose a simple, yet accurate, fast, scalable model\nthat only uses a single-layer vanilla transformer encoder as the feature\nextractor and employs tailored representation similarity functions to\napproximate various ground truth similarity measures. Extensive experiments\ndemonstrate our model significantly mitigates the curse of dimensionality issue\nand outperforms the state-of-the-arts in effectiveness, efficiency, and\nscalability.\n","authors":["Chuang Yang","Renhe Jiang","Xiaohang Xu","Chuan Xiao","Kaoru Sezaki"],"pdf_url":"https://arxiv.org/pdf/2410.14629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14625v1","updated":"2024-10-18T17:27:07Z","published":"2024-10-18T17:27:07Z","title":"Enhancing AI Accessibility in Veterinary Medicine: Linking Classifiers\n  and Electronic Health Records","summary":"  In the rapidly evolving landscape of veterinary healthcare, integrating\nmachine learning (ML) clinical decision-making tools with electronic health\nrecords (EHRs) promises to improve diagnostic accuracy and patient care.\nHowever, the seamless integration of ML classifiers into existing EHRs in\nveterinary medicine is frequently hindered by the rigidity of EHR systems or\nthe limited availability of IT resources. To address this shortcoming, we\npresent Anna, a freely-available software solution that provides ML classifier\nresults for EHR laboratory data in real-time.\n","authors":["Chun Yin Kong","Picasso Vasquez","Makan Farhoodimoghadam","Chris Brandt","Titus C. Brown","Krystle L. Reagan","Allison Zwingenberger","Stefan M. Keller"],"pdf_url":"https://arxiv.org/pdf/2410.14625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14609v1","updated":"2024-10-18T17:03:17Z","published":"2024-10-18T17:03:17Z","title":"DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual\n  Distillation in Conversational Search","summary":"  Conversational Search (CS) is the task of retrieving relevant documents from\na corpus within a conversational context, combining retrieval with\nconversational context modeling. With the explosion of Large Language Models\n(LLMs), the CS field has seen major improvements with LLMs rewriting user\nqueries, accounting for conversational context. However, engaging LLMs at\ninference time harms efficiency. Current methods address this by distilling\nembeddings from human-rewritten queries to learn the context modeling task.\nYet, these approaches predominantly focus on context modeling, and only treat\nthe contrastive component of the retrieval task within a\ndistillation-independent loss term. To address these limitations, we propose a\nnew distillation method, as a relaxation of the previous objective, unifying\nretrieval and context modeling. We relax the existing training objectives by\ndistilling similarity scores between conversations and documents, rather than\nrelying solely on representation learning. Our proposed distillation objective\nallows for more freedom in the representation space and leverages the\ncontrastive nature of document relevance. Through experiments on Learned Sparse\nRetrieval (LSR) across 5 CS datasets, our approach demonstrates substantial\nimprovements in both in-domain and out-of-domain retrieval performance,\noutperforming state-of-the-art with gains of up to 6 points in recall for\nout-of-domain datasets. Additionally, through the relaxation of the objective,\nwe propose a multi-teacher distillation, using multiple LLMs as teachers,\nyielding additional gains, and outperforming the teachers themselves in\nin-domain experiments. Finally, analysis of the sparsity of the models reveals\nthat our distillation allows for better control over the sparsity of the\ntrained models.\n","authors":["Simon Lupart","Mohammad Aliannejadi","Evangelos Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2410.14609v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14567v1","updated":"2024-10-18T16:11:29Z","published":"2024-10-18T16:11:29Z","title":"RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions","summary":"  Conversational AI agents use Retrieval Augmented Generation (RAG) to provide\nverifiable document-grounded responses to user inquiries. However, many natural\nquestions do not have good answers: about 25\\% contain false\nassumptions~\\cite{Yu2023:CREPE}, and over 50\\% are\nambiguous~\\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve\ntheir responses to confusing questions. This paper presents a novel synthetic\ndata generation method to efficiently create a diverse set of context-grounded\nconfusing questions from a given document corpus. We conduct an empirical\ncomparative evaluation of several large language models as RAG agents to\nmeasure the accuracy of confusion detection and appropriate response\ngeneration. We contribute a benchmark dataset to the public domain.\n","authors":["Zhiyuan Peng","Jinming Nian","Alexandre Evfimievski","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2410.14567v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2407.14346v2","updated":"2024-10-18T13:59:54Z","published":"2024-07-19T14:28:53Z","title":"Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals","summary":"  Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.\n","authors":["Akash Kumar Mohankumar","Gururaj K","Gagan Madan","Amit Singh"],"pdf_url":"https://arxiv.org/pdf/2407.14346v2.pdf","comment":"Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure"},{"id":"http://arxiv.org/abs/2409.14217v2","updated":"2024-10-18T13:25:29Z","published":"2024-09-21T18:39:53Z","title":"Revisiting BPR: A Replicability Study of a Common Recommender System\n  Baseline","summary":"  Bayesian Personalized Ranking (BPR), a collaborative filtering approach based\non matrix factorization, frequently serves as a benchmark for recommender\nsystems research. However, numerous studies often overlook the nuances of BPR\nimplementation, claiming that it performs worse than newly proposed methods\nacross various tasks. In this paper, we thoroughly examine the features of the\nBPR model, indicating their impact on its performance, and investigate\nopen-source BPR implementations. Our analysis reveals inconsistencies between\nthese implementations and the original BPR paper, leading to a significant\ndecrease in performance of up to 50% for specific implementations. Furthermore,\nthrough extensive experiments on real-world datasets under modern evaluation\nsettings, we demonstrate that with proper tuning of its hyperparameters, the\nBPR model can achieve performance levels close to state-of-the-art methods on\nthe top-n recommendation tasks and even outperform them on specific datasets.\nSpecifically, on the Million Song Dataset, the BPR model with hyperparameters\ntuning statistically significantly outperforms Mult-VAE by 10% in NDCG@100 with\nbinary relevance function.\n","authors":["Aleksandr Milogradskii","Oleg Lashinin","Alexander P","Marina Ananyeva","Sergey Kolesnikov"],"pdf_url":"https://arxiv.org/pdf/2409.14217v2.pdf","comment":"This paper is accepted at the Reproducibility track of the ACM RecSys\n  '24 conference"},{"id":"http://arxiv.org/abs/2410.14452v1","updated":"2024-10-18T13:24:18Z","published":"2024-10-18T13:24:18Z","title":"SPFresh: Incremental In-Place Update for Billion-Scale Vector Search","summary":"  Approximate Nearest Neighbor Search (ANNS) is now widely used in various\napplications, ranging from information retrieval, question answering, and\nrecommendation, to search for similar high-dimensional vectors. As the amount\nof vector data grows continuously, it becomes important to support updates to\nvector index, the enabling technique that allows for efficient and accurate\nANNS on vectors. Because of the curse of high dimensionality, it is often\ncostly to identify the right neighbors of a single new vector, a necessary\nprocess for index update. To amortize update costs, existing systems maintain a\nsecondary index to accumulate updates, which are merged by the main index by\nglobal rebuilding the entire index periodically. However, this approach has\nhigh fluctuations of search latency and accuracy, not even to mention that it\nrequires substantial resources and is extremely time-consuming for rebuilds. We\nintroduce SPFresh, a system that supports in-place vector updates. At the heart\nof SPFresh is LIRE, a lightweight incremental rebalancing protocol to split\nvector partitions and reassign vectors in the nearby partitions to adapt to\ndata distribution shift. LIRE achieves low-overhead vector updates by only\nreassigning vectors at the boundary between partitions, where in a high-quality\nvector index the amount of such vectors are deemed small. With LIRE, SPFresh\nprovides superior query latency and accuracy to solutions based on global\nrebuild, with only 1% of DRAM and less than 10% cores needed at the peak\ncompared to the state-of-the-art, in a billion scale vector index with 1% of\ndaily vector update rate.\n","authors":["Yuming Xu","Hengyu Liang","Jin Li","Shuotao Xu","Qi Chen","Qianxi Zhang","Cheng Li","Ziyue Yang","Fan Yang","Yuqing Yang","Peng Cheng","Mao Yang"],"pdf_url":"https://arxiv.org/pdf/2410.14452v1.pdf","comment":"SOSP 23"},{"id":"http://arxiv.org/abs/2410.13428v2","updated":"2024-10-18T09:55:18Z","published":"2024-10-17T10:51:34Z","title":"Generate and Instantiate What You Prefer: Text-Guided Diffusion for\n  Sequential Recommendation","summary":"  Recent advancements in generative recommendation systems, particularly in the\nrealm of sequential recommendation tasks, have shown promise in enhancing\ngeneralization to new items. Among these approaches, diffusion-based generative\nrecommendation has emerged as an effective tool, leveraging its ability to\ncapture data distributions and generate high-quality samples. Despite\neffectiveness, two primary challenges have been identified: 1) the lack of\nconsistent modeling of data distribution for oracle items; and 2) the\ndifficulty in scaling to more informative control signals beyond historical\ninteractions. These issues stem from the uninformative nature of ID embeddings,\nwhich necessitate random initialization and limit the incorporation of\nadditional control signals. To address these limitations, we propose iDreamRec\nto involve more concrete prior knowledge to establish item embeddings,\nparticularly through detailed item text descriptions and advanced Text\nEmbedding Models (TEM). More importantly, by converting item descriptions into\nembeddings aligned with TEM, we enable the integration of intention\ninstructions as control signals to guide the generation of oracle items.\nExperimental results on four datasets demonstrate that iDreamRec not only\noutperforms existing diffusion-based generative recommenders but also\nfacilitates the incorporation of intention instructions for more precise and\neffective recommendation generation.\n","authors":["Guoqing Hu","Zhangyi Yang","Zhibo Cai","An Zhang","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14331v1","updated":"2024-10-18T09:43:30Z","published":"2024-10-18T09:43:30Z","title":"ChartifyText: Automated Chart Generation from Data-Involved Texts via\n  LLM","summary":"  Text documents with numerical values involved are widely used in various\napplications such as scientific research, economy, public health and\njournalism. However, it is difficult for readers to quickly interpret such\ndata-involved texts and gain deep insights. To fill this research gap, this\nwork aims to automatically generate charts to accurately convey the underlying\ndata and ideas to readers, which is essentially a challenging task. The\nchallenges originate from text ambiguities, intrinsic sparsity and uncertainty\nof data in text documents, and subjective sentiment differences. Specifically,\nwe propose ChartifyText, a novel fully-automated approach that leverages Large\nLanguage Models (LLMs) to convert complex data-involved texts to expressive\ncharts. It consists of two major modules: tabular data inference and expressive\nchart generation. The tabular data inference module employs systematic prompt\nengineering to guide the LLM (e.g., GPT-4) to infer table data, where data\nranges, uncertainties, missing data values and corresponding subjective\nsentiments are explicitly considered. The expressive chart generation module\naugments standard charts with intuitive visual encodings and concise texts to\naccurately convey the underlying data and insights. We extensively evaluate the\neffectiveness of ChartifyText on real-world data-involved text documents\nthrough case studies, in-depth interviews with three visualization experts, and\na carefully-designed user study with 15 participants. The results demonstrate\nthe usefulness and effectiveness of ChartifyText in helping readers efficiently\nand effectively make sense of data-involved texts.\n","authors":["Songheng Zhang","Lei Wang","Toby Jia-Jun Li","Qiaomu Shen","Yixin Cao","Yong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14626v2","updated":"2024-10-18T08:56:18Z","published":"2023-10-23T07:00:51Z","title":"Conversational Recommender System and Large Language Model Are Made for\n  Each Other in E-commerce Pre-sales Dialogue","summary":"  E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases.\n","authors":["Yuanxing Liu","Wei-Nan Zhang","Yifan Chen","Yuchi Zhang","Haopeng Bai","Fan Feng","Hengbin Cui","Yongbin Li","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2310.14626v2.pdf","comment":"EMNLP 2023 Findings"},{"id":"http://arxiv.org/abs/2410.13230v2","updated":"2024-10-18T08:36:37Z","published":"2024-10-17T05:33:50Z","title":"Starbucks: Improved Training for 2D Matryoshka Embeddings","summary":"  Effective approaches that can scale embedding model depth (i.e. layers) and\nembedding size allow for the creation of models that are highly scalable across\ndifferent computational resources and task requirements. While the recently\nproposed 2D Matryoshka training approach can efficiently produce a single\nembedding model such that its sub-layers and sub-dimensions can measure text\nsimilarity, its effectiveness is significantly worse than if smaller models\nwere trained separately. To address this issue, we propose Starbucks, a new\ntraining strategy for Matryoshka-like embedding models, which encompasses both\nthe fine-tuning and pre-training phases. For the fine-tuning phase, we discover\nthat, rather than sampling a random sub-layer and sub-dimensions for each\ntraining steps, providing a fixed list of layer-dimension pairs, from small\nsize to large sizes, and computing the loss across all pairs significantly\nimproves the effectiveness of 2D Matryoshka embedding models, bringing them on\npar with their separately trained counterparts. To further enhance performance,\nwe introduce a new pre-training strategy, which applies masked autoencoder\nlanguage modelling to sub-layers and sub-dimensions during pre-training,\nresulting in a stronger backbone for subsequent fine-tuning of the embedding\nmodel. Experimental results on both semantic text similarity and retrieval\nbenchmarks demonstrate that the proposed pre-training and fine-tuning\nstrategies significantly improved the effectiveness over 2D Matryoshka models,\nenabling Starbucks models to perform more efficiently and effectively than\nseparately trained models.\n","authors":["Shengyao Zhuang","Shuai Wang","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2410.13230v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06572v2","updated":"2024-10-18T08:20:38Z","published":"2024-06-03T17:07:46Z","title":"Graph Neural Network Enhanced Retrieval for Question Answering of LLMs","summary":"  Retrieval augmented generation has revolutionized large language model (LLM)\noutputs by providing factual supports. Nevertheless, it struggles to capture\nall the necessary knowledge for complex reasoning questions. Existing retrieval\nmethods typically divide reference documents into passages, treating them in\nisolation. These passages, however, are often interrelated, such as passages\nthat are contiguous or share the same keywords. Therefore, it is crucial to\nrecognize such relatedness for enhancing the retrieval process. In this paper,\nwe propose a novel retrieval method, called GNN-Ret, which leverages graph\nneural networks (GNNs) to enhance retrieval by exploiting the relatedness\nbetween passages. Specifically, we first construct a graph of passages by\nconnecting passages that are structure-related or keyword-related. A graph\nneural network (GNN) is then leveraged to exploit the relationships between\npassages and improve the retrieval of supporting passages. Furthermore, we\nextend our method to handle multi-hop reasoning questions using a recurrent\ngraph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates\nthe graphs of passages from previous steps, thereby enhancing the retrieval of\nsupporting passages. Extensive experiments on benchmark datasets demonstrate\nthat GNN-Ret achieves higher accuracy for question answering with a single\nquery of LLMs than strong baselines that require multiple queries, and RGNN-Ret\nfurther improves accuracy and achieves state-of-the-art performance, with up to\n10.4% accuracy improvement on the 2WikiMQA dataset.\n","authors":["Zijian Li","Qingyan Guo","Jiawei Shao","Lei Song","Jiang Bian","Jun Zhang","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2406.06572v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.14241v1","updated":"2024-10-18T07:44:12Z","published":"2024-10-18T07:44:12Z","title":"Graph Neural Patching for Cold-Start Recommendations","summary":"  The cold start problem in recommender systems remains a critical challenge.\nCurrent solutions often train hybrid models on auxiliary data for both cold and\nwarm users/items, potentially degrading the experience for the latter. This\ndrawback limits their viability in practical scenarios where the satisfaction\nof existing warm users/items is paramount. Although graph neural networks\n(GNNs) excel at warm recommendations by effective collaborative signal\nmodeling, they haven't been effectively leveraged for the cold-start issue\nwithin a user-item graph, which is largely due to the lack of initial\nconnections for cold user/item entities. Addressing this requires a GNN adept\nat cold-start recommendations without sacrificing performance for existing\nones. To this end, we introduce Graph Neural Patching for Cold-Start\nRecommendations (GNP), a customized GNN framework with dual functionalities:\nGWarmer for modeling collaborative signal on existing warm users/items and\nPatching Networks for simulating and enhancing GWarmer's performance on\ncold-start recommendations. Extensive experiments on three benchmark datasets\nconfirm GNP's superiority in recommending both warm and cold users/items.\n","authors":["Hao Chen","Yu Yang","Yuanchen Bei","Zefan Wang","Yue Xu","Feiran Huang"],"pdf_url":"https://arxiv.org/pdf/2410.14241v1.pdf","comment":"13 pages, accepted by Australasian Database Conference 2024. arXiv\n  admin note: substantial text overlap with arXiv:2209.12215"},{"id":"http://arxiv.org/abs/2406.00012v2","updated":"2024-10-18T06:07:06Z","published":"2024-05-20T09:24:45Z","title":"FINED: Feed Instance-Wise Information Need with Essential and\n  Disentangled Parametric Knowledge from the Past","summary":"  Recommender models play a vital role in various industrial scenarios, while\noften faced with the catastrophic forgetting problem caused by the fast\nshifting data distribution. To alleviate this problem, a common approach is to\nreuse knowledge from the historical data. However, preserving the vast and\nfast-accumulating data is hard, which causes dramatic storage overhead.\nMemorizing old data through a parametric knowledge base is then proposed, which\ncompresses the vast amount of raw data into model parameters. Despite the\nflexibility, how to improve the memorization and generalization capabilities of\nthe parametric knowledge base and suit the flexible information need of each\ninstance are challenging. In this paper, we propose FINED to Feed INstance-wise\ninformation need with Essential and Disentangled parametric knowledge from past\ndata for recommendation enhancement. Concretely, we train a knowledge extractor\nthat extracts knowledge patterns of arbitrary order from past data and a\nknowledge encoder that memorizes the arbitrary order patterns, which serves as\nthe retrieval key generator and memory network respectively in the following\nknowledge reusing phase. The whole process is regularized by the proposed two\nconstraints, which improve the capabilities of the parametric knowledge base\nwithout increasing the size of it. The essential principle helps to compress\nthe input into representative vectors that capture the task-relevant\ninformation and filter out the noisy information. The disentanglement principle\nreduces the redundancy of stored information and pushes the knowledge base to\nfocus on capturing the disentangled invariant patterns. These two rules\ntogether promote rational compression of information for robust and generalized\nknowledge representations. Extensive experiments on two datasets justify the\neffectiveness of the proposed method.\n","authors":["Kounianhua Du","Jizheng Chen","Jianghao Lin","Menghui Zhu","Bo Chen","Shuai Li","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.00012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14170v1","updated":"2024-10-18T04:20:46Z","published":"2024-10-18T04:20:46Z","title":"Personalized Image Generation with Large Multimodal Models","summary":"  Personalized content filtering, such as recommender systems, has become a\ncritical infrastructure to alleviate information overload. However, these\nsystems merely filter existing content and are constrained by its limited\ndiversity, making it difficult to meet users' varied content needs. To address\nthis limitation, personalized content generation has emerged as a promising\ndirection with broad applications. Nevertheless, most existing research focuses\non personalized text generation, with relatively little attention given to\npersonalized image generation. The limited work in personalized image\ngeneration faces challenges in accurately capturing users' visual preferences\nand needs from noisy user-interacted images and complex multimodal\ninstructions. Worse still, there is a lack of supervised data for training\npersonalized image generation models.\n  To overcome the challenges, we propose a Personalized Image Generation\nFramework named Pigeon, which adopts exceptional large multimodal models with\nthree dedicated modules to capture users' visual preferences and needs from\nnoisy user history and multimodal instructions. To alleviate the data scarcity,\nwe introduce a two-stage preference alignment scheme, comprising masked\npreference reconstruction and pairwise preference alignment, to align Pigeon\nwith the personalized image generation task. We apply Pigeon to personalized\nsticker and movie poster generation, where extensive quantitative results and\nhuman evaluation highlight its superiority over various generative baselines.\n","authors":["Yiyan Xu","Wenjie Wang","Yang Zhang","Tang Biao","Peng Yan","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2410.14170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14167v1","updated":"2024-10-18T04:17:49Z","published":"2024-10-18T04:17:49Z","title":"Optimizing Retrieval-Augmented Generation with Elasticsearch for\n  Enhanced Question-Answering Systems","summary":"  This study aims to improve the accuracy and quality of large-scale language\nmodels (LLMs) in answering questions by integrating Elasticsearch into the\nRetrieval Augmented Generation (RAG) framework. The experiment uses the\nStanford Question Answering Dataset (SQuAD) version 2.0 as the test dataset and\ncompares the performance of different retrieval methods, including traditional\nmethods based on keyword matching or semantic similarity calculation, BM25-RAG\nand TF-IDF- RAG, and the newly proposed ES-RAG scheme. The results show that\nES-RAG not only has obvious advantages in retrieval efficiency but also\nperforms well in key indicators such as accuracy, which is 0.51 percentage\npoints higher than TF-IDF-RAG. In addition, Elasticsearch's powerful search\ncapabilities and rich configuration options enable the entire\nquestion-answering system to better handle complex queries and provide more\nflexible and efficient responses based on the diverse needs of users. Future\nresearch directions can further explore how to optimize the interaction\nmechanism between Elasticsearch and LLM, such as introducing higher-level\nsemantic understanding and context-awareness capabilities, to achieve a more\nintelligent and humanized question-answering experience.\n","authors":["Jiajing Chen","Runyuan Bao","Hongye Zheng","Zhen Qi","Jianjun Wei","Jiacheng Hu"],"pdf_url":"https://arxiv.org/pdf/2410.14167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14122v1","updated":"2024-10-18T02:31:36Z","published":"2024-10-18T02:31:36Z","title":"Towards Robust Transcription: Exploring Noise Injection Strategies for\n  Training Data Augmentation","summary":"  Recent advancements in Automatic Piano Transcription (APT) have significantly\nimproved system performance, but the impact of noisy environments on the system\nperformance remains largely unexplored. This study investigates the impact of\nwhite noise at various Signal-to-Noise Ratio (SNR) levels on state-of-the-art\nAPT models and evaluates the performance of the Onsets and Frames model when\ntrained on noise-augmented data. We hope this research provides valuable\ninsights as preliminary work toward developing transcription models that\nmaintain consistent performance across a range of acoustic conditions.\n","authors":["Yonghyun Kim","Alexander Lerch"],"pdf_url":"https://arxiv.org/pdf/2410.14122v1.pdf","comment":"Accepted to the Late-Breaking Demo Session of the 25th International\n  Society for Music Information Retrieval (ISMIR) Conference, 2024"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2410.14673v1","updated":"2024-10-18T17:59:25Z","published":"2024-10-18T17:59:25Z","title":"Self-supervised contrastive learning performs non-linear system\n  identification","summary":"  Self-supervised learning (SSL) approaches have brought tremendous success\nacross many tasks and domains. It has been argued that these successes can be\nattributed to a link between SSL and identifiable representation learning:\nTemporal structure and auxiliary variables ensure that latent representations\nare related to the true underlying generative factors of the data. Here, we\ndeepen this connection and show that SSL can perform system identification in\nlatent space. We propose DynCL, a framework to uncover linear, switching linear\nand non-linear dynamics under a non-linear observation model, give theoretical\nguarantees and validate them empirically.\n","authors":["Rodrigo González Laiz","Tobias Schmidt","Steffen Schneider"],"pdf_url":"https://arxiv.org/pdf/2410.14673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14670v1","updated":"2024-10-18T17:58:53Z","published":"2024-10-18T17:58:53Z","title":"Decomposing The Dark Matter of Sparse Autoencoders","summary":"  Sparse autoencoders (SAEs) are a promising technique for decomposing language\nmodel activations into interpretable linear features. However, current SAEs\nfall short of completely explaining model performance, resulting in \"dark\nmatter\": unexplained variance in activations. This work investigates dark\nmatter as an object of study in its own right. Surprisingly, we find that much\nof SAE dark matter--about half of the error vector itself and >90% of its\nnorm--can be linearly predicted from the initial activation vector.\nAdditionally, we find that the scaling behavior of SAE error norms at a per\ntoken level is remarkably predictable: larger SAEs mostly struggle to\nreconstruct the same contexts as smaller SAEs. We build on the linear\nrepresentation hypothesis to propose models of activations that might lead to\nthese observations, including postulating a new type of \"introduced error\";\nthese insights imply that the part of the SAE error vector that cannot be\nlinearly predicted (\"nonlinear\" error) might be fundamentally different from\nthe linearly predictable component. To validate this hypothesis, we empirically\nanalyze nonlinear SAE error and show that 1) it contains fewer not yet learned\nfeatures, 2) SAEs trained on it are quantitatively worse, 3) it helps predict\nSAE per-token scaling behavior, and 4) it is responsible for a proportional\namount of the downstream increase in cross entropy loss when SAE activations\nare inserted into the model. Finally, we examine two methods to reduce\nnonlinear SAE error at a fixed sparsity: inference time gradient pursuit, which\nleads to a very slight decrease in nonlinear error, and linear transformations\nfrom earlier layer SAE outputs, which leads to a larger reduction.\n","authors":["Joshua Engels","Logan Riggs","Max Tegmark"],"pdf_url":"https://arxiv.org/pdf/2410.14670v1.pdf","comment":"Code at https://github.com/JoshEngels/SAE-Dark-Matter"},{"id":"http://arxiv.org/abs/2410.14667v1","updated":"2024-10-18T17:57:01Z","published":"2024-10-18T17:57:01Z","title":"Stochastic Gradient Descent Jittering for Inverse Problems: Alleviating\n  the Accuracy-Robustness Tradeoff","summary":"  Inverse problems aim to reconstruct unseen data from corrupted or perturbed\nmeasurements. While most work focuses on improving reconstruction quality,\ngeneralization accuracy and robustness are equally important, especially for\nsafety-critical applications. Model-based architectures (MBAs), such as loop\nunrolling methods, are considered more interpretable and achieve better\nreconstructions. Empirical evidence suggests that MBAs are more robust to\nperturbations than black-box solvers, but the accuracy-robustness tradeoff in\nMBAs remains underexplored. In this work, we propose a simple yet effective\ntraining scheme for MBAs, called SGD jittering, which injects noise\niteration-wise during reconstruction. We theoretically demonstrate that SGD\njittering not only generalizes better than the standard mean squared error\ntraining but is also more robust to average-case attacks. We validate SGD\njittering using denoising toy examples, seismic deconvolution, and single-coil\nMRI reconstruction. The proposed method achieves cleaner reconstructions for\nout-of-distribution data and demonstrates enhanced robustness to adversarial\nattacks.\n","authors":["Peimeng Guan","Mark A. Davenport"],"pdf_url":"https://arxiv.org/pdf/2410.14667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14666v1","updated":"2024-10-18T17:56:11Z","published":"2024-10-18T17:56:11Z","title":"DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie\n  Character-Aware Discourse Graph","summary":"  Summarizing movie screenplays presents a unique set of challenges compared to\nstandard document summarization. Screenplays are not only lengthy, but also\nfeature a complex interplay of characters, dialogues, and scenes, with numerous\ndirect and subtle relationships and contextual nuances that are difficult for\nmachine learning models to accurately capture and comprehend. Recent attempts\nat screenplay summarization focus on fine-tuning transformer-based pre-trained\nmodels, but these models often fall short in capturing long-term dependencies\nand latent relationships, and frequently encounter the \"lost in the middle\"\nissue. To address these challenges, we introduce DiscoGraMS, a novel resource\nthat represents movie scripts as a movie character-aware discourse graph (CaD\nGraph). This approach is well-suited for various downstream tasks, such as\nsummarization, question-answering, and salience detection. The model aims to\npreserve all salient information, offering a more comprehensive and faithful\nrepresentation of the screenplay's content. We further explore a baseline\nmethod that combines the CaD Graph with the corresponding movie script through\na late fusion of graph and text modalities, and we present very initial\npromising results.\n","authors":["Maitreya Prafulla Chitale","Uday Bindal","Rajakrishnan Rajkumar","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2410.14666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14665v1","updated":"2024-10-18T17:55:15Z","published":"2024-10-18T17:55:15Z","title":"Online Reinforcement Learning with Passive Memory","summary":"  This paper considers an online reinforcement learning algorithm that\nleverages pre-collected data (passive memory) from the environment for online\ninteraction. We show that using passive memory improves performance and further\nprovide theoretical guarantees for regret that turns out to be near-minimax\noptimal. Results show that the quality of passive memory determines\nsub-optimality of the incurred regret. The proposed approach and results hold\nin both continuous and discrete state-action spaces.\n","authors":["Anay Pattanaik","Lav R. Varshney"],"pdf_url":"https://arxiv.org/pdf/2410.14665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06331v2","updated":"2024-10-18T17:53:46Z","published":"2024-10-08T20:12:11Z","title":"Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing","summary":"  The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper\nMLP layers, unlike single-hop tasks, which rely on earlier layers. This\ndistinction explains the poor performance of current methods in multi-hop\nqueries, as they primarily focus on editing shallow layers, leaving deeper\nlayers unchanged. To address this, we propose IFMET, a novel locate-then-edit\nKE approach designed to edit both shallow and deep MLP layers. IFMET employs\nmulti-hop editing prompts and supplementary sets to locate and modify knowledge\nacross different reasoning stages. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\neffectively overcoming the limitations of previous locate-then-edit methods.\n","authors":["Zhuoran Zhang","Yongxiang Li","Zijian Kan","Keyuan Cheng","Lijie Hu","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06331v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2410.14660v1","updated":"2024-10-18T17:51:51Z","published":"2024-10-18T17:51:51Z","title":"A Large Language Model-Driven Reward Design Framework via Dynamic\n  Feedback for Reinforcement Learning","summary":"  Large Language Models (LLMs) have shown significant potential in designing\nreward functions for Reinforcement Learning (RL) tasks. However, obtaining\nhigh-quality reward code often involves human intervention, numerous LLM\nqueries, or repetitive RL training. To address these issues, we propose CARD, a\nLLM-driven Reward Design framework that iteratively generates and improves\nreward function code. Specifically, CARD includes a Coder that generates and\nverifies the code, while a Evaluator provides dynamic feedback to guide the\nCoder in improving the code, eliminating the need for human feedback. In\naddition to process feedback and trajectory feedback, we introduce Trajectory\nPreference Evaluation (TPE), which evaluates the current reward function based\non trajectory preferences. If the code fails the TPE, the Evaluator provides\npreference feedback, avoiding RL training at every iteration and making the\nreward function better aligned with the task objective. Empirical results on\nMeta-World and ManiSkill2 demonstrate that our method achieves an effective\nbalance between task performance and token efficiency, outperforming or\nmatching the baselines across all tasks. On 10 out of 12 tasks, CARD shows\nbetter or comparable performance to policies trained with expert-designed\nrewards, and our method even surpasses the oracle on 3 tasks.\n","authors":["Shengjie Sun","Runze Liu","Jiafei Lyu","Jing-Wen Yang","Liangpeng Zhang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2410.14660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14659v1","updated":"2024-10-18T17:51:37Z","published":"2024-10-18T17:51:37Z","title":"Harnessing Causality in Reinforcement Learning With Bagged Decision\n  Times","summary":"  We consider reinforcement learning (RL) for a class of problems with bagged\ndecision times. A bag contains a finite sequence of consecutive decision times.\nThe transition dynamics are non-Markovian and non-stationary within a bag.\nFurther, all actions within a bag jointly impact a single reward, observed at\nthe end of the bag. Our goal is to construct an online RL algorithm to maximize\nthe discounted sum of the bag-specific rewards. To handle non-Markovian\ntransitions within a bag, we utilize an expert-provided causal directed acyclic\ngraph (DAG). Based on the DAG, we construct the states as a dynamical Bayesian\nsufficient statistic of the observed history, which results in Markovian state\ntransitions within and across bags. We then frame this problem as a periodic\nMarkov decision process (MDP) that allows non-stationarity within a period. An\nonline RL algorithm based on Bellman-equations for stationary MDPs is\ngeneralized to handle periodic MDPs. To justify the proposed RL algorithm, we\nshow that our constructed state achieves the maximal optimal value function\namong all state constructions for a periodic MDP. Further we prove the Bellman\noptimality equations for periodic MDPs. We evaluate the proposed method on\ntestbed variants, constructed with real data from a mobile health clinical\ntrial.\n","authors":["Daiqi Gao","Hsin-Yu Lai","Predrag Klasnja","Susan A. Murphy"],"pdf_url":"https://arxiv.org/pdf/2410.14659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14655v1","updated":"2024-10-18T17:48:27Z","published":"2024-10-18T17:48:27Z","title":"Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens","summary":"  Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks.\n","authors":["Zhepeng Cen","Yao Liu","Siliang Zeng","Pratik Chaudhar","Huzefa Rangwala","George Karypis","Rasool Fakoor"],"pdf_url":"https://arxiv.org/pdf/2410.14655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14649v1","updated":"2024-10-18T17:46:37Z","published":"2024-10-18T17:46:37Z","title":"EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary\n  Search","summary":"  The high computational costs of large language models (LLMs) have led to a\nflurry of research on LLM compression, via methods such as quantization,\nsparsification, or structured pruning. A new frontier in this area is given by\n\\emph{dynamic, non-uniform} compression methods, which adjust the compression\nlevels (e.g., sparsity) per-block or even per-layer in order to minimize\naccuracy loss, while guaranteeing a global compression threshold. Yet, current\nmethods rely on heuristics for identifying the \"importance\" of a given layer\ntowards the loss, based on assumptions such as \\emph{error monotonicity}, i.e.\nthat the end-to-end model compression error is proportional to the sum of\nlayer-wise errors. In this paper, we revisit this area, and propose a new and\ngeneral approach for dynamic compression that is provably optimal in a given\ninput range. We begin from the motivating observation that, in general,\n\\emph{error monotonicity does not hold for LLMs}: compressed models with lower\nsum of per-layer errors can perform \\emph{worse} than models with higher error\nsums. To address this, we propose a new general evolutionary framework for\ndynamic LLM compression called EvoPress, which has provable convergence, and\nlow sample and evaluation complexity. We show that these theoretical guarantees\nlead to highly competitive practical performance for dynamic compression of\nLlama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art\nresults across all compression approaches: structural pruning (block/layer\ndropping), unstructured sparsity, as well as quantization with dynamic\nbitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress.\n","authors":["Oliver Sieberling","Denis Kuznedelev","Eldar Kurtic","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2410.14649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14640v1","updated":"2024-10-18T17:41:19Z","published":"2024-10-18T17:41:19Z","title":"HR-Bandit: Human-AI Collaborated Linear Recourse Bandit","summary":"  Human doctors frequently recommend actionable recourses that allow patients\nto modify their conditions to access more effective treatments. Inspired by\nsuch healthcare scenarios, we propose the Recourse Linear UCB\n($\\textsf{RLinUCB}$) algorithm, which optimizes both action selection and\nfeature modifications by balancing exploration and exploitation. We further\nextend this to the Human-AI Linear Recourse Bandit ($\\textsf{HR-Bandit}$),\nwhich integrates human expertise to enhance performance. $\\textsf{HR-Bandit}$\noffers three key guarantees: (i) a warm-start guarantee for improved initial\nperformance, (ii) a human-effort guarantee to minimize required human\ninteractions, and (iii) a robustness guarantee that ensures sublinear regret\neven when human decisions are suboptimal. Empirical results, including a\nhealthcare case study, validate its superior performance against existing\nbenchmarks.\n","authors":["Junyu Cao","Ruijiang Gao","Esmaeil Keyvanshokooh"],"pdf_url":"https://arxiv.org/pdf/2410.14640v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2410.14639v1","updated":"2024-10-18T17:40:58Z","published":"2024-10-18T17:40:58Z","title":"Convergence of Manifold Filter-Combine Networks","summary":"  In order to better understand manifold neural networks (MNNs), we introduce\nManifold Filter-Combine Networks (MFCNs). The filter-combine framework\nparallels the popular aggregate-combine paradigm for graph neural networks\n(GNNs) and naturally suggests many interesting families of MNNs which can be\ninterpreted as the manifold analog of various popular GNNs. We then propose a\nmethod for implementing MFCNs on high-dimensional point clouds that relies on\napproximating the manifold by a sparse graph. We prove that our method is\nconsistent in the sense that it converges to a continuum limit as the number of\ndata points tends to infinity.\n","authors":["David R. Johnson","Joyce Chew","Siddharth Viswanath","Edward De Brouwer","Deanna Needell","Smita Krishnaswamy","Michael Perlmutter"],"pdf_url":"https://arxiv.org/pdf/2410.14639v1.pdf","comment":"Accepted to NeurIPS Workshop on Symmetry and Geometry in Neural\n  Representations (Extended Abstract Track)"},{"id":"http://arxiv.org/abs/2410.14634v1","updated":"2024-10-18T17:35:33Z","published":"2024-10-18T17:35:33Z","title":"Parallel Backpropagation for Inverse of a Convolution with Application\n  to Normalizing Flows","summary":"  Inverse of an invertible convolution is an important operation that comes up\nin Normalizing Flows, Image Deblurring, etc. The naive algorithm for\nbackpropagation of this operation using Gaussian elimination has running time\n$O(n^3)$ where $n$ is the number of pixels in the image. We give a fast\nparallel backpropagation algorithm with running time $O(\\sqrt{n})$ for a square\nimage and provide a GPU implementation of the same. Inverse Convolutions are\nusually used in Normalizing Flows in the sampling pass, making them slow. We\npropose to use Inverse Convolutions in the forward (image to latent vector)\npass of the Normalizing flow. Since the sampling pass is the inverse of the\nforward pass, it will use convolutions only, resulting in efficient sampling\ntimes. We use our parallel backpropagation algorithm for optimizing the inverse\nconvolution layer resulting in fast training times also. We implement this\napproach in various Normalizing Flow backbones, resulting in our Inverse-Flow\nmodels. We benchmark Inverse-Flow on standard datasets and show significantly\nimproved sampling times with similar bits per dimension compared to previous\nmodels.\n","authors":["Sandeep Nagar","Girish Varma"],"pdf_url":"https://arxiv.org/pdf/2410.14634v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2109.09889v3","updated":"2024-10-18T17:32:27Z","published":"2021-09-21T00:09:03Z","title":"A Distance-based Anomaly Detection Framework for Deep Reinforcement\n  Learning","summary":"  In deep reinforcement learning (RL) systems, abnormal states pose significant\nrisks by potentially triggering unpredictable behaviors and unsafe actions,\nthus impeding the deployment of RL systems in real-world scenarios. It is\ncrucial for reliable decision-making systems to have the capability to cast an\nalert whenever they encounter unfamiliar observations that they are not\nequipped to handle. In this paper, we propose a novel Mahalanobis\ndistance-based (MD) anomaly detection framework, called \\textit{MDX}, for deep\nRL algorithms. MDX simultaneously addresses random, adversarial, and\nout-of-distribution (OOD) state outliers in both offline and online settings.\nIt utilizes Mahalanobis distance within class-conditional distributions for\neach action and operates within a statistical hypothesis testing framework\nunder the Gaussian assumption. We further extend it to robust and\ndistribution-free versions by incorporating Robust MD and conformal inference\ntechniques. Through extensive experiments on classical control environments,\nAtari games, and autonomous driving scenarios, we demonstrate the effectiveness\nof our MD-based detection framework. MDX offers a simple, unified, and\npractical anomaly detection tool for enhancing the safety and reliability of RL\nsystems in real-world applications.\n","authors":["Hongming Zhang","Ke Sun","Bo Xu","Linglong Kong","Martin Müller"],"pdf_url":"https://arxiv.org/pdf/2109.09889v3.pdf","comment":"19 pages, 21 figures"},{"id":"http://arxiv.org/abs/2410.14630v1","updated":"2024-10-18T17:30:20Z","published":"2024-10-18T17:30:20Z","title":"On the Regularization of Learnable Embeddings for Time Series Processing","summary":"  In processing multiple time series, accounting for the individual features of\neach sequence can be challenging. To address this, modern deep learning methods\nfor time series analysis combine a shared (global) model with local layers,\nspecific to each time series, often implemented as learnable embeddings.\nIdeally, these local embeddings should encode meaningful representations of the\nunique dynamics of each sequence. However, when these are learned end-to-end as\nparameters of a forecasting model, they may end up acting as mere sequence\nidentifiers. Shared processing blocks may then become reliant on such\nidentifiers, limiting their transferability to new contexts. In this paper, we\naddress this issue by investigating methods to regularize the learning of local\nlearnable embeddings for time series processing. Specifically, we perform the\nfirst extensive empirical study on the subject and show how such\nregularizations consistently improve performance in widely adopted\narchitectures. Furthermore, we show that methods preventing the co-adaptation\nof local and global parameters are particularly effective in this context. This\nhypothesis is validated by comparing several methods preventing the downstream\nmodels from relying on sequence identifiers, going as far as completely\nresetting the embeddings during training. The obtained results provide an\nimportant contribution to understanding the interplay between learnable local\nparameters and shared processing layers: a key challenge in modern time series\nprocessing models and a step toward developing effective foundation models for\ntime series.\n","authors":["Luca Butera","Giovanni De Felice","Andrea Cini","Cesare Alippi"],"pdf_url":"https://arxiv.org/pdf/2410.14630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14629v1","updated":"2024-10-18T17:30:17Z","published":"2024-10-18T17:30:17Z","title":"SIMformer: Single-Layer Vanilla Transformer Can Learn Free-Space\n  Trajectory Similarity","summary":"  Free-space trajectory similarity calculation, e.g., DTW, Hausdorff, and\nFrechet, often incur quadratic time complexity, thus learning-based methods\nhave been proposed to accelerate the computation. The core idea is to train an\nencoder to transform trajectories into representation vectors and then compute\nvector similarity to approximate the ground truth. However, existing methods\nface dual challenges of effectiveness and efficiency: 1) they all utilize\nEuclidean distance to compute representation similarity, which leads to the\nsevere curse of dimensionality issue -- reducing the distinguishability among\nrepresentations and significantly affecting the accuracy of subsequent\nsimilarity search tasks; 2) most of them are trained in triplets manner and\noften necessitate additional information which downgrades the efficiency; 3)\nprevious studies, while emphasizing the scalability in terms of efficiency,\noverlooked the deterioration of effectiveness when the dataset size grows. To\ncope with these issues, we propose a simple, yet accurate, fast, scalable model\nthat only uses a single-layer vanilla transformer encoder as the feature\nextractor and employs tailored representation similarity functions to\napproximate various ground truth similarity measures. Extensive experiments\ndemonstrate our model significantly mitigates the curse of dimensionality issue\nand outperforms the state-of-the-arts in effectiveness, efficiency, and\nscalability.\n","authors":["Chuang Yang","Renhe Jiang","Xiaohang Xu","Chuan Xiao","Kaoru Sezaki"],"pdf_url":"https://arxiv.org/pdf/2410.14629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14625v1","updated":"2024-10-18T17:27:07Z","published":"2024-10-18T17:27:07Z","title":"Enhancing AI Accessibility in Veterinary Medicine: Linking Classifiers\n  and Electronic Health Records","summary":"  In the rapidly evolving landscape of veterinary healthcare, integrating\nmachine learning (ML) clinical decision-making tools with electronic health\nrecords (EHRs) promises to improve diagnostic accuracy and patient care.\nHowever, the seamless integration of ML classifiers into existing EHRs in\nveterinary medicine is frequently hindered by the rigidity of EHR systems or\nthe limited availability of IT resources. To address this shortcoming, we\npresent Anna, a freely-available software solution that provides ML classifier\nresults for EHR laboratory data in real-time.\n","authors":["Chun Yin Kong","Picasso Vasquez","Makan Farhoodimoghadam","Chris Brandt","Titus C. Brown","Krystle L. Reagan","Allison Zwingenberger","Stefan M. Keller"],"pdf_url":"https://arxiv.org/pdf/2410.14625v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14623v1","updated":"2024-10-18T17:22:38Z","published":"2024-10-18T17:22:38Z","title":"syren-new: Precise formulae for the linear and nonlinear matter power\n  spectra with massive neutrinos and dynamical dark energy","summary":"  Current and future large scale structure surveys aim to constrain the\nneutrino mass and the equation of state of dark energy. We aim to construct\naccurate and interpretable symbolic approximations to the linear and nonlinear\nmatter power spectra as a function of cosmological parameters in extended\n$\\Lambda$CDM models which contain massive neutrinos and non-constant equations\nof state for dark energy. This constitutes an extension of the syren-halofit\nemulators to incorporate these two effects, which we call syren-new\n(SYmbolic-Regression-ENhanced power spectrum emulator with NEutrinos and\n$W_0-w_a$). We also obtain a simple approximation to the derived parameter\n$\\sigma_8$ as a function of the cosmological parameters for these models. Our\nresults for the linear power spectrum are designed to emulate CLASS, whereas\nfor the nonlinear case we aim to match the results of EuclidEmulator2. We\ncompare our results to existing emulators and $N$-body simulations. Our\nanalytic emulators for $\\sigma_8$, the linear and nonlinear power spectra\nachieve root mean squared errors of 0.1%, 0.3% and 1.3%, respectively, across a\nwide range of cosmological parameters, redshifts and wavenumbers. We verify\nthat emulator-related discrepancies are subdominant compared to observational\nerrors and other modelling uncertainties when computing shear power spectra for\nLSST-like surveys. Our expressions have similar accuracy to existing\n(numerical) emulators, but are at least an order of magnitude faster, both on a\nCPU and GPU. Our work greatly improves the accuracy, speed and range of\napplicability of current symbolic approximations to the linear and nonlinear\nmatter power spectra. We provide publicly available code for all symbolic\napproximations found.\n","authors":["Ce Sui","Deaglan J. Bartlett","Shivam Pandey","Harry Desmond","Pedro G. Ferreira","Benjamin D. Wandelt"],"pdf_url":"https://arxiv.org/pdf/2410.14623v1.pdf","comment":"18 pages, 15 figures"},{"id":"http://arxiv.org/abs/2410.14621v1","updated":"2024-10-18T17:21:25Z","published":"2024-10-18T17:21:25Z","title":"JAMUN: Transferable Molecular Conformational Ensemble Generation with\n  Walk-Jump Sampling","summary":"  Conformational ensembles of protein structures are immensely important both\nto understanding protein function, and for drug discovery in novel modalities\nsuch as cryptic pockets. Current techniques for sampling ensembles are\ncomputationally inefficient, or do not transfer to systems outside their\ntraining data. We present walk-Jump Accelerated Molecular ensembles with\nUniversal Noise (JAMUN), a step towards the goal of efficiently sampling the\nBoltzmann distribution of arbitrary proteins. By extending Walk-Jump Sampling\nto point clouds, JAMUN enables ensemble generation at orders of magnitude\nfaster rates than traditional molecular dynamics or state-of-the-art ML\nmethods. Further, JAMUN is able to predict the stable basins of small peptides\nthat were not seen during training.\n","authors":["Ameya Daigavane","Bodhi P. Vani","Saeed Saremi","Joseph Kleinhenz","Joshua Rackers"],"pdf_url":"https://arxiv.org/pdf/2410.14621v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10989v2","updated":"2024-10-18T17:21:17Z","published":"2024-10-14T18:17:01Z","title":"Liger Kernel: Efficient Triton Kernels for LLM Training","summary":"  Training Large Language Models (LLMs) efficiently at scale presents a\nformidable challenge, driven by their ever-increasing computational demands and\nthe need for enhanced performance. In this work, we introduce Liger-Kernel, an\nopen-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input\nchunking, our kernels achieve on average a 20% increase in training throughput\nand a 60% reduction in GPU memory usage for popular LLMs compared to\nHuggingFace implementations. In addition, Liger-Kernel is designed with\nmodularity, accessibility, and adaptability in mind, catering to both casual\nand expert users. Comprehensive benchmarks and integration tests are built in\nto ensure compatibility, performance, correctness, and convergence across\ndiverse computing environments and model architectures.\n  The source code is available under a permissive license at:\ngithub.com/linkedin/Liger-Kernel.\n","authors":["Pin-Lun Hsu","Yun Dai","Vignesh Kothapalli","Qingquan Song","Shao Tang","Siyu Zhu","Steven Shimizu","Shivam Sahni","Haowen Ning","Yanning Chen"],"pdf_url":"https://arxiv.org/pdf/2410.10989v2.pdf","comment":"17 pages, 12 figures"},{"id":"http://arxiv.org/abs/2410.10101v2","updated":"2024-10-18T17:15:09Z","published":"2024-10-14T02:41:01Z","title":"Learning Linear Attention in Polynomial Time","summary":"  Previous research has explored the computational expressivity of Transformer\nmodels in simulating Boolean circuits or Turing machines. However, the\nlearnability of these simulators from observational data has remained an open\nquestion. Our study addresses this gap by providing the first polynomial-time\nlearnability results (specifically strong, agnostic PAC learning) for\nsingle-layer Transformers with linear attention. We show that linear attention\nmay be viewed as a linear predictor in a suitably defined RKHS. As a\nconsequence, the problem of learning any linear transformer may be converted\ninto the problem of learning an ordinary linear predictor in an expanded\nfeature space, and any such predictor may be converted back into a multiheaded\nlinear transformer. Moving to generalization, we show how to efficiently\nidentify training datasets for which every empirical risk minimizer is\nequivalent (up to trivial symmetries) to the linear Transformer that generated\nthe data, thereby guaranteeing the learned model will correctly generalize\nacross all inputs. Finally, we provide examples of computations expressible via\nlinear attention and therefore polynomial-time learnable, including associative\nmemories, finite automata, and a class of Universal Turing Machine (UTMs) with\npolynomially bounded computation histories. We empirically validate our\ntheoretical findings on three tasks: learning random linear attention networks,\nkey--value associations, and learning to execute finite automata. Our findings\nbridge a critical gap between theoretical expressivity and learnability of\nTransformers, and show that flexible and general models of computation are\nefficiently learnable.\n","authors":["Morris Yau","Ekin Akyürek","Jiayuan Mao","Joshua B. Tenenbaum","Stefanie Jegelka","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2410.10101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14616v1","updated":"2024-10-18T17:14:28Z","published":"2024-10-18T17:14:28Z","title":"Benchmarking Deep Reinforcement Learning for Navigation in Denied Sensor\n  Environments","summary":"  Deep Reinforcement learning (DRL) is used to enable autonomous navigation in\nunknown environments. Most research assume perfect sensor data, but real-world\nenvironments may contain natural and artificial sensor noise and denial. Here,\nwe present a benchmark of both well-used and emerging DRL algorithms in a\nnavigation task with configurable sensor denial effects. In particular, we are\ninterested in comparing how different DRL methods (e.g. model-free PPO vs.\nmodel-based DreamerV3) are affected by sensor denial. We show that DreamerV3\noutperforms other methods in the visual end-to-end navigation task with a\ndynamic goal - and other methods are not able to learn this. Furthermore,\nDreamerV3 generally outperforms other methods in sensor-denied environments. In\norder to improve robustness, we use adversarial training and demonstrate an\nimproved performance in denied environments, although this generally comes with\na performance cost on the vanilla environments. We anticipate this benchmark of\ndifferent DRL methods and the usage of adversarial training to be a starting\npoint for the development of more elaborate navigation strategies that are\ncapable of dealing with uncertain and denied sensor readings.\n","authors":["Mariusz Wisniewski","Paraskevas Chatzithanos","Weisi Guo","Antonios Tsourdos"],"pdf_url":"https://arxiv.org/pdf/2410.14616v1.pdf","comment":"31 pages, 19 figures. For associated code, see\n  https://github.com/mazqtpopx/cranfield-navigation-gym"},{"id":"http://arxiv.org/abs/2410.14615v1","updated":"2024-10-18T17:13:29Z","published":"2024-10-18T17:13:29Z","title":"Asymptotically Optimal Change Detection for Unnormalized Pre- and\n  Post-Change Distributions","summary":"  This paper addresses the problem of detecting changes when only unnormalized\npre- and post-change distributions are accessible. This situation happens in\nmany scenarios in physics such as in ferromagnetism, crystallography,\nmagneto-hydrodynamics, and thermodynamics, where the energy models are\ndifficult to normalize.\n  Our approach is based on the estimation of the Cumulative Sum (CUSUM)\nstatistics, which is known to produce optimal performance. We first present an\nintuitively appealing approximation method. Unfortunately, this produces a\nbiased estimator of the CUSUM statistics and may cause performance degradation.\nWe then propose the Log-Partition Approximation Cumulative Sum (LPA-CUSUM)\nalgorithm based on thermodynamic integration (TI) in order to estimate the\nlog-ratio of normalizing constants of pre- and post-change distributions. It is\nproved that this approach gives an unbiased estimate of the log-partition\nfunction and the CUSUM statistics, and leads to an asymptotically optimal\nperformance. Moreover, we derive a relationship between the required sample\nsize for thermodynamic integration and the desired detection delay performance,\noffering guidelines for practical parameter selection. Numerical studies are\nprovided demonstrating the efficacy of our approach.\n","authors":["Arman Adibi","Sanjeev Kulkarni","H. Vincent Poor","Taposh Banerjee","Vahid Tarokh"],"pdf_url":"https://arxiv.org/pdf/2410.14615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.06402v2","updated":"2024-10-18T17:10:05Z","published":"2024-03-11T03:28:13Z","title":"One size doesn't fit all: Predicting the Number of Examples for\n  In-Context Learning","summary":"  In-context learning (ICL) refers to the process of adding a small number of\nlocalized examples (ones that are semantically similar to the input) from a\ntraining set of labelled data to an LLM's prompt with an objective to\neffectively control the generative process seeking to improve the downstream\ntask performance. Existing ICL approaches use an identical number of examples\n(a pre-configured hyper-parameter) for each data instance. Our work alleviates\nthe limitations of this 'one fits all' approach by dynamically predicting the\nnumber of examples for each data instance to be used in few-shot inference with\nLLMs. In particular, we employ a multi-label classifier, the parameters of\nwhich are fitted using a training set, where the label for each instance in the\ntraining set indicates if using a specific value of k (number of most similar\nexamples from 0 up to a maximum value) leads to correct k-shot downstream\npredictions. Our experiments on a number of text classification benchmarks show\nthat AICL substantially outperforms standard ICL by up to 17%.\n","authors":["Manish Chandra","Debasis Ganguly","Iadh Ounis"],"pdf_url":"https://arxiv.org/pdf/2403.06402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20601v2","updated":"2024-10-18T17:07:01Z","published":"2023-10-31T16:37:01Z","title":"Modular Boundaries in Recurrent Neural Networks","summary":"  Recent theoretical and experimental work in neuroscience has focused on the\nrepresentational and dynamical character of neural manifolds --subspaces in\nneural activity space wherein many neurons coactivate. Importantly, neural\npopulations studied under this \"neural manifold hypothesis\" are continuous and\nnot cleanly divided into separate neural populations. This perspective clashes\nwith the \"modular hypothesis\" of brain organization, wherein neural elements\nmaintain an \"all-or-nothing\" affiliation with modules. In line with this\nmodular hypothesis, recent research on recurrent neural networks suggests that\nmulti-task networks become modular across training, such that different modules\nspecialize for task-general dynamical motifs. If the modular hypothesis is\ntrue, then it would be important to use a dimensionality reduction technique\nthat captures modular structure. Here, we investigate the features of such a\nmethod. We leverage RNNs as a model system to study the character of modular\nneural populations, using a community detection method from network science\nknown as modularity maximization to partition neurons into distinct modules.\nThese partitions allow us to ask the following question: do these modular\nboundaries matter to the system? ...\n","authors":["Jacob Tanner","Sina Mansour L.","Ludovico Coletta","Alessandro Gozzi","Richard F. Betzel"],"pdf_url":"https://arxiv.org/pdf/2310.20601v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14606v1","updated":"2024-10-18T17:00:29Z","published":"2024-10-18T17:00:29Z","title":"Streaming Deep Reinforcement Learning Finally Works","summary":"  Natural intelligence processes experience as a continuous stream, sensing,\nacting, and learning moment-by-moment in real time. Streaming learning, the\nmodus operandi of classic reinforcement learning (RL) algorithms like\nQ-learning and TD, mimics natural learning by using the most recent sample\nwithout storing it. This approach is also ideal for resource-constrained,\ncommunication-limited, and privacy-sensitive applications. However, in deep RL,\nlearners almost always use batch updates and replay buffers, making them\ncomputationally expensive and incompatible with streaming learning. Although\nthe prevalence of batch deep RL is often attributed to its sample efficiency, a\nmore critical reason for the absence of streaming deep RL is its frequent\ninstability and failure to learn, which we refer to as stream barrier. This\npaper introduces the stream-x algorithms, the first class of deep RL algorithms\nto overcome stream barrier for both prediction and control and match sample\nefficiency of batch RL. Through experiments in Mujoco Gym, DM Control Suite,\nand Atari Games, we demonstrate stream barrier in existing algorithms and\nsuccessful stable learning with our stream-x algorithms: stream Q, stream AC,\nand stream TD, achieving the best model-free performance in DM Control Dog\nenvironments. A set of common techniques underlies the stream-x algorithms,\nenabling their success with a single set of hyperparameters and allowing for\neasy extension to other algorithms, thereby reviving streaming RL.\n","authors":["Mohamed Elsayed","Gautham Vasan","A. Rupam Mahmood"],"pdf_url":"https://arxiv.org/pdf/2410.14606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14604v1","updated":"2024-10-18T16:57:27Z","published":"2024-10-18T16:57:27Z","title":"Learning to Control the Smoothness of Graph Convolutional Network\n  Features","summary":"  The pioneering work of Oono and Suzuki [ICLR, 2020] and Cai and Wang\n[arXiv:2006.13318] initializes the analysis of the smoothness of graph\nconvolutional network (GCN) features. Their results reveal an intricate\nempirical correlation between node classification accuracy and the ratio of\nsmooth to non-smooth feature components. However, the optimal ratio that favors\nnode classification is unknown, and the non-smooth features of deep GCN with\nReLU or leaky ReLU activation function diminish. In this paper, we propose a\nnew strategy to let GCN learn node features with a desired smoothness --\nadapting to data and tasks -- to enhance node classification. Our approach has\nthree key steps: (1) We establish a geometric relationship between the input\nand output of ReLU or leaky ReLU. (2) Building on our geometric insights, we\naugment the message-passing process of graph convolutional layers (GCLs) with a\nlearnable term to modulate the smoothness of node features with computational\nefficiency. (3) We investigate the achievable ratio between smooth and\nnon-smooth feature components for GCNs with the augmented message-passing\nscheme. Our extensive numerical results show that the augmented message-passing\nschemes significantly improve node classification for GCN and some related\nmodels.\n","authors":["Shih-Hsin Wang","Justin Baker","Cory Hauck","Bao Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14604v1.pdf","comment":"48 pages"},{"id":"http://arxiv.org/abs/2410.14602v1","updated":"2024-10-18T16:57:05Z","published":"2024-10-18T16:57:05Z","title":"How Does Data Diversity Shape the Weight Landscape of Neural Networks?","summary":"  To enhance the generalization of machine learning models to unseen data,\ntechniques such as dropout, weight decay ($L_2$ regularization), and noise\naugmentation are commonly employed. While regularization methods (i.e., dropout\nand weight decay) are geared toward adjusting model parameters to prevent\noverfitting, data augmentation increases the diversity of the input training\nset, a method purported to improve accuracy and calibration error. In this\npaper, we investigate the impact of each of these techniques on the parameter\nspace of neural networks, with the goal of understanding how they alter the\nweight landscape in transfer learning scenarios. To accomplish this, we employ\nRandom Matrix Theory to analyze the eigenvalue distributions of pre-trained\nmodels, fine-tuned using these techniques but using different levels of data\ndiversity, for the same downstream tasks. We observe that diverse data\ninfluences the weight landscape in a similar fashion as dropout. Additionally,\nwe compare commonly used data augmentation methods with synthetic data created\nby generative models. We conclude that synthetic data can bring more diversity\ninto real input data, resulting in a better performance on out-of-distribution\ntest instances.\n","authors":["Yang Ba","Michelle V. Mancenido","Rong Pan"],"pdf_url":"https://arxiv.org/pdf/2410.14602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09639v2","updated":"2024-10-18T16:50:56Z","published":"2024-06-14T00:08:04Z","title":"TGB 2.0: A Benchmark for Learning on Temporal Knowledge Graphs and\n  Heterogeneous Graphs","summary":"  Multi-relational temporal graphs are powerful tools for modeling real-world\ndata, capturing the evolving and interconnected nature of entities over time.\nRecently, many novel models are proposed for ML on such graphs intensifying the\nneed for robust evaluation and standardized benchmark datasets. However, the\navailability of such resources remains scarce and evaluation faces added\ncomplexity due to reproducibility issues in experimental protocols. To address\nthese challenges, we introduce Temporal Graph Benchmark 2.0 (TGB 2.0), a novel\nbenchmarking framework tailored for evaluating methods for predicting future\nlinks on Temporal Knowledge Graphs and Temporal Heterogeneous Graphs with a\nfocus on large-scale datasets, extending the Temporal Graph Benchmark. TGB 2.0\nfacilitates comprehensive evaluations by presenting eight novel datasets\nspanning five domains with up to 53 million edges. TGB 2.0 datasets are\nsignificantly larger than existing datasets in terms of number of nodes, edges,\nor timestamps. In addition, TGB 2.0 provides a reproducible and realistic\nevaluation pipeline for multi-relational temporal graphs. Through extensive\nexperimentation, we observe that 1) leveraging edge-type information is crucial\nto obtain high performance, 2) simple heuristic baselines are often competitive\nwith more complex methods, 3) most methods fail to run on our largest datasets,\nhighlighting the need for research on more scalable methods.\n","authors":["Julia Gastinger","Shenyang Huang","Mikhail Galkin","Erfan Loghmani","Ali Parviz","Farimah Poursafaei","Jacob Danovitch","Emanuele Rossi","Ioannis Koutis","Heiner Stuckenschmidt","Reihaneh Rabbany","Guillaume Rabusseau"],"pdf_url":"https://arxiv.org/pdf/2406.09639v2.pdf","comment":"29 pages, 8 figures, 11 tables, accepted at NeurIPS 2024 Track on\n  Datasets and Benchmarks"},{"id":"http://arxiv.org/abs/2410.14592v1","updated":"2024-10-18T16:43:10Z","published":"2024-10-18T16:43:10Z","title":"Contractivity and linear convergence in bilinear saddle-point problems:\n  An operator-theoretic approach","summary":"  We study the convex-concave bilinear saddle-point problem $\\min_x \\max_y f(x)\n+ y^\\top Ax - g(y)$, where both, only one, or none of the functions $f$ and $g$\nare strongly convex, and suitable rank conditions on the matrix $A$ hold. The\nsolution of this problem is at the core of many machine learning tasks. By\nemploying tools from operator theory, we systematically prove the contractivity\n(in turn, the linear convergence) of several first-order primal-dual\nalgorithms, including the Chambolle-Pock method. Our approach results in\nconcise and elegant proofs, and it yields new convergence guarantees and\ntighter bounds compared to known results.\n","authors":["Colin Dirren","Mattia Bianchi","Panagiotis D. Grontas","John Lygeros","Florian Dörfler"],"pdf_url":"https://arxiv.org/pdf/2410.14592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14591v1","updated":"2024-10-18T16:41:37Z","published":"2024-10-18T16:41:37Z","title":"A Lipschitz spaces view of infinitely wide shallow neural networks","summary":"  We revisit the mean field parametrization of shallow neural networks, using\nsigned measures on unbounded parameter spaces and duality pairings that take\ninto account the regularity and growth of activation functions. This setting\ndirectly leads to the use of unbalanced Kantorovich-Rubinstein norms defined by\nduality with Lipschitz functions, and of spaces of measures dual to those of\ncontinuous functions with controlled growth. These allow to make transparent\nthe need for total variation and moment bounds or penalization to obtain\nexistence of minimizers of variational formulations, under which we prove a\ncompactness result in strong Kantorovich-Rubinstein norm, and in the absence of\nwhich we show several examples demonstrating undesirable behavior. Further, the\nKantorovich-Rubinstein setting enables us to combine the advantages of a\ncompletely linear parametrization and ensuing reproducing kernel Banach space\nframework with optimal transport insights. We showcase this synergy with\nrepresenter theorems and uniform large data limits for empirical risk\nminimization, and in proposed formulations for distillation and fusion\napplications.\n","authors":["Francesca Bartolucci","Marcello Carioni","José A. Iglesias","Yury Korolev","Emanuele Naldi","Stefano Vigogna"],"pdf_url":"https://arxiv.org/pdf/2410.14591v1.pdf","comment":"39 pages, 1 table"},{"id":"http://arxiv.org/abs/2410.14588v1","updated":"2024-10-18T16:38:55Z","published":"2024-10-18T16:38:55Z","title":"Learning With Multi-Group Guarantees For Clusterable Subpopulations","summary":"  A canonical desideratum for prediction problems is that performance\nguarantees should hold not just on average over the population, but also for\nmeaningful subpopulations within the overall population. But what constitutes a\nmeaningful subpopulation? In this work, we take the perspective that relevant\nsubpopulations should be defined with respect to the clusters that naturally\nemerge from the distribution of individuals for which predictions are being\nmade. In this view, a population refers to a mixture model whose components\nconstitute the relevant subpopulations. We suggest two formalisms for capturing\nper-subgroup guarantees: first, by attributing each individual to the component\nfrom which they were most likely drawn, given their features; and second, by\nattributing each individual to all components in proportion to their relative\nlikelihood of having been drawn from each component. Using online calibration\nas a case study, we study a \\variational algorithm that provides guarantees for\neach of these formalisms by handling all plausible underlying subpopulation\nstructures simultaneously, and achieve an $O(T^{1/2})$ rate even when the\nsubpopulations are not well-separated. In comparison, the more natural\ncluster-then-predict approach that first recovers the structure of the\nsubpopulations and then makes predictions suffers from a $O(T^{2/3})$ rate and\nrequires the subpopulations to be separable. Along the way, we prove that\nproviding per-subgroup calibration guarantees for underlying clusters can be\neasier than learning the clusters: separation between median subgroup features\nis required for the latter but not the former.\n","authors":["Jessica Dai","Nika Haghtalab","Eric Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.14588v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14587v1","updated":"2024-10-18T16:37:52Z","published":"2024-10-18T16:37:52Z","title":"Neuro-Symbolic Traders: Assessing the Wisdom of AI Crowds in Markets","summary":"  Deep generative models are becoming increasingly used as tools for financial\nanalysis. However, it is unclear how these models will influence financial\nmarkets, especially when they infer financial value in a semi-autonomous way.\nIn this work, we explore the interplay between deep generative models and\nmarket dynamics. We develop a form of virtual traders that use deep generative\nmodels to make buy/sell decisions, which we term neuro-symbolic traders, and\nexpose them to a virtual market. Under our framework, neuro-symbolic traders\nare agents that use vision-language models to discover a model of the\nfundamental value of an asset. Agents develop this model as a stochastic\ndifferential equation, calibrated to market data using gradient descent. We\ntest our neuro-symbolic traders on both synthetic data and real financial time\nseries, including an equity stock, commodity, and a foreign exchange pair. We\nthen expose several groups of neuro-symbolic traders to a virtual market\nenvironment. This market environment allows for feedback between the traders\nbelief of the underlying value to the observed price dynamics. We find that\nthis leads to price suppression compared to the historical data, highlighting a\nfuture risk to market stability. Our work is a first step towards quantifying\nthe effect of deep generative agents on markets dynamics and sets out some of\nthe potential risks and benefits of this approach in the future.\n","authors":["Namid R. Stillman","Rory Baggott"],"pdf_url":"https://arxiv.org/pdf/2410.14587v1.pdf","comment":"8 pages, 4 figures, ACM format"},{"id":"http://arxiv.org/abs/2410.14586v1","updated":"2024-10-18T16:37:28Z","published":"2024-10-18T16:37:28Z","title":"Neural Combinatorial Clustered Bandits for Recommendation Systems","summary":"  We consider the contextual combinatorial bandit setting where in each round,\nthe learning agent, e.g., a recommender system, selects a subset of \"arms,\"\ne.g., products, and observes rewards for both the individual base arms, which\nare a function of known features (called \"context\"), and the super arm (the\nsubset of arms), which is a function of the base arm rewards. The agent's goal\nis to simultaneously learn the unknown reward functions and choose the\nhighest-reward arms. For example, the \"reward\" may represent a user's\nprobability of clicking on one of the recommended products. Conventional bandit\nmodels, however, employ restrictive reward function models in order to obtain\nperformance guarantees. We make use of deep neural networks to estimate and\nlearn the unknown reward functions and propose Neural UCB Clustering\n(NeUClust), which adopts a clustering approach to select the super arm in every\nround by exploiting underlying structure in the context space. Unlike prior\nneural bandit works, NeUClust uses a neural network to estimate the super arm\nreward and select the super arm, thus eliminating the need for a known\noptimization oracle. We non-trivially extend prior neural combinatorial bandit\nworks to prove that NeUClust achieves\n$\\widetilde{O}\\left(\\widetilde{d}\\sqrt{T}\\right)$ regret, where $\\widetilde{d}$\nis the effective dimension of a neural tangent kernel matrix, $T$ the number of\nrounds. Experiments on real world recommendation datasets show that NeUClust\nachieves better regret and reward than other contextual combinatorial and\nneural bandit algorithms.\n","authors":["Baran Atalar","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2410.14586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14581v1","updated":"2024-10-18T16:32:06Z","published":"2024-10-18T16:32:06Z","title":"Optimizing Attention with Mirror Descent: Generalized Max-Margin Token\n  Selection","summary":"  Attention mechanisms have revolutionized several domains of artificial\nintelligence, such as natural language processing and computer vision, by\nenabling models to selectively focus on relevant parts of the input data. While\nrecent work has characterized the optimization dynamics of gradient descent\n(GD) in attention-based models and the structural properties of its preferred\nsolutions, less is known about more general optimization algorithms such as\nmirror descent (MD). In this paper, we investigate the convergence properties\nand implicit biases of a family of MD algorithms tailored for softmax attention\nmechanisms, with the potential function chosen as the $p$-th power of the\n$\\ell_p$-norm. Specifically, we show that these algorithms converge in\ndirection to a generalized hard-margin SVM with an $\\ell_p$-norm objective when\napplied to a classification problem using a softmax attention model. Notably,\nour theoretical results reveal that the convergence rate is comparable to that\nof traditional GD in simpler models, despite the highly nonlinear and nonconvex\nnature of the present problem. Additionally, we delve into the joint\noptimization dynamics of the key-query matrix and the decoder, establishing\nconditions under which this complex joint optimization converges to their\nrespective hard-margin SVM solutions. Lastly, our numerical experiments on real\ndata demonstrate that MD algorithms improve generalization over standard GD and\nexcel in optimal token selection.\n","authors":["Aaron Alvarado Kristanto Julistiono","Davoud Ataee Tarzanagh","Navid Azizan"],"pdf_url":"https://arxiv.org/pdf/2410.14581v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14579v1","updated":"2024-10-18T16:27:04Z","published":"2024-10-18T16:27:04Z","title":"Towards Unsupervised Validation of Anomaly-Detection Models","summary":"  Unsupervised validation of anomaly-detection models is a highly challenging\ntask. While the common practices for model validation involve a labeled\nvalidation set, such validation sets cannot be constructed when the underlying\ndatasets are unlabeled. The lack of robust and efficient unsupervised\nmodel-validation techniques presents an acute challenge in the implementation\nof automated anomaly-detection pipelines, especially when there exists no prior\nknowledge of the model's performance on similar datasets. This work presents a\nnew paradigm to automated validation of anomaly-detection models, inspired by\nreal-world, collaborative decision-making mechanisms. We focus on two\ncommonly-used, unsupervised model-validation tasks -- model selection and model\nevaluation -- and provide extensive experimental results that demonstrate the\naccuracy and robustness of our approach on both tasks.\n","authors":["Lihi Idan"],"pdf_url":"https://arxiv.org/pdf/2410.14579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14578v1","updated":"2024-10-18T16:26:45Z","published":"2024-10-18T16:26:45Z","title":"Large Language Models Are Overparameterized Text Encoders","summary":"  Large language models (LLMs) demonstrate strong performance as text embedding\nmodels when finetuned with supervised contrastive training. However, their\nlarge size balloons inference time and memory requirements. In this paper, we\nshow that by pruning the last $p\\%$ layers of an LLM before supervised training\nfor only 1000 steps, we can achieve a proportional reduction in memory and\ninference time. We evaluate four different state-of-the-art LLMs on text\nembedding tasks and find that our method can prune up to 30\\% of layers with\nnegligible impact on performance and up to 80\\% with only a modest drop. With\nonly three lines of code, our method is easily implemented in any pipeline for\ntransforming LLMs to text encoders. We also propose $\\text{L}^3 \\text{Prune}$,\na novel layer-pruning strategy based on the model's initial loss that provides\ntwo optimal pruning configurations: a large variant with negligible performance\nloss and a small variant for resource-constrained settings. On average, the\nlarge variant prunes 21\\% of the parameters with a $-0.3$ performance drop, and\nthe small variant only suffers from a $-5.1$ decrease while pruning 74\\% of the\nmodel. We consider these results strong evidence that LLMs are\noverparameterized for text embedding tasks, and can be easily pruned.\n","authors":["Thennal D K","Tim Fischer","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2410.14578v1.pdf","comment":"8 pages of content + 1 for limitations and ethical considerations, 14\n  pages in total including references and appendix, 5+1 figures"},{"id":"http://arxiv.org/abs/2410.13174v2","updated":"2024-10-18T16:26:30Z","published":"2024-10-17T02:57:35Z","title":"Scalable Drift Monitoring in Medical Imaging AI","summary":"  The integration of artificial intelligence (AI) into medical imaging has\nadvanced clinical diagnostics but poses challenges in managing model drift and\nensuring long-term reliability. To address these challenges, we develop MMC+,\nan enhanced framework for scalable drift monitoring, building upon the\nCheXstray framework that introduced real-time drift detection for medical\nimaging AI models using multi-modal data concordance. This work extends the\noriginal framework's methodologies, providing a more scalable and adaptable\nsolution for real-world healthcare settings and offers a reliable and\ncost-effective alternative to continuous performance monitoring addressing\nlimitations of both continuous and periodic monitoring methods. MMC+ introduces\ncritical improvements to the original framework, including more robust handling\nof diverse data streams, improved scalability with the integration of\nfoundation models like MedImageInsight for high-dimensional image embeddings\nwithout site-specific training, and the introduction of uncertainty bounds to\nbetter capture drift in dynamic clinical environments. Validated with\nreal-world data from Massachusetts General Hospital during the COVID-19\npandemic, MMC+ effectively detects significant data shifts and correlates them\nwith model performance changes. While not directly predicting performance\ndegradation, MMC+ serves as an early warning system, indicating when AI systems\nmay deviate from acceptable performance bounds and enabling timely\ninterventions. By emphasizing the importance of monitoring diverse data streams\nand evaluating data shifts alongside model performance, this work contributes\nto the broader adoption and integration of AI solutions in clinical settings.\n","authors":["Jameson Merkow","Felix J. Dorfner","Xiyu Yang","Alexander Ersoy","Giridhar Dasegowda","Mannudeep Kalra","Matthew P. Lungren","Christopher P. Bridge","Ivan Tarapov"],"pdf_url":"https://arxiv.org/pdf/2410.13174v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14574v1","updated":"2024-10-18T16:20:22Z","published":"2024-10-18T16:20:22Z","title":"MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts","summary":"  Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled\nscalability in deep learning. SMoE has the potential to exponentially increase\nparameter count while maintaining the efficiency of the model by only\nactivating a small subset of these parameters for a given sample. However, it\nhas been observed that SMoE suffers from unstable training and has difficulty\nadapting to new distributions, leading to the model's lack of robustness to\ndata contamination. To overcome these limitations, we first establish a\nconnection between the dynamics of the expert representations in SMoEs and\ngradient descent on a multi-objective optimization problem. Leveraging our\nframework, we then integrate momentum into SMoE and propose a new family of\nSMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate\nthat MomentumSMoE is more stable and robust than SMoE. In particular, we verify\nthe advantages of MomentumSMoE over SMoE on a variety of practical tasks\nincluding ImageNet-1K object recognition and WikiText-103 language modeling. We\ndemonstrate the applicability of MomentumSMoE to many types of SMoE models,\nincluding those in the Sparse MoE model for vision (V-MoE) and the Generalist\nLanguage Model (GLaM). We also show that other advanced momentum-based\noptimization methods, such as Adam, can be easily incorporated into the\nMomentumSMoE framework for designing new SMoE models with even better\nperformance, almost negligible additional computation cost, and simple\nimplementations.\n","authors":["Rachel S. Y. Teo","Tan M. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2410.14574v1.pdf","comment":"10 pages in the main text. Published at NeurIPS 2024. The code is\n  available at https://github.com/rachtsy/MomentumSMoE"},{"id":"http://arxiv.org/abs/2410.14573v1","updated":"2024-10-18T16:20:17Z","published":"2024-10-18T16:20:17Z","title":"Building Trust in Black-box Optimization: A Comprehensive Framework for\n  Explainability","summary":"  Optimizing costly black-box functions within a constrained evaluation budget\npresents significant challenges in many real-world applications. Surrogate\nOptimization (SO) is a common resolution, yet its proprietary nature introduced\nby the complexity of surrogate models and the sampling core (e.g., acquisition\nfunctions) often leads to a lack of explainability and transparency. While\nexisting literature has primarily concentrated on enhancing convergence to\nglobal optima, the practical interpretation of newly proposed strategies\nremains underexplored, especially in batch evaluation settings. In this paper,\nwe propose \\emph{Inclusive} Explainability Metrics for Surrogate Optimization\n(IEMSO), a comprehensive set of model-agnostic metrics designed to enhance the\ntransparency, trustworthiness, and explainability of the SO approaches. Through\nthese metrics, we provide both intermediate and post-hoc explanations to\npractitioners before and after performing expensive evaluations to gain trust.\nWe consider four primary categories of metrics, each targeting a specific\naspect of the SO process: Sampling Core Metrics, Batch Properties Metrics,\nOptimization Process Metrics, and Feature Importance. Our experimental\nevaluations demonstrate the significant potential of the proposed metrics\nacross different benchmarks.\n","authors":["Nazanin Nezami","Hadis Anahideh"],"pdf_url":"https://arxiv.org/pdf/2410.14573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14570v1","updated":"2024-10-18T16:16:52Z","published":"2024-10-18T16:16:52Z","title":"Understanding the difficulty of low-precision post-training quantization\n  of large language models","summary":"  Large language models of high parameter counts are computationally expensive,\nyet can be made much more efficient by compressing their weights to very low\nnumerical precision. This can be achieved either through post-training\nquantization by minimizing local, layer-wise quantization errors, or through\nquantization-aware fine-tuning by minimizing the global loss function. In this\nstudy, we discovered that, under the same data constraint, the former approach\nnearly always fared worse than the latter, a phenomenon particularly prominent\nwhen the numerical precision is very low. We further showed that this\ndifficulty of post-training quantization arose from stark misalignment between\noptimization of the local and global objective functions. Our findings explains\nlimited utility in minimization of local quantization error and the importance\nof direct quantization-aware fine-tuning, in the regime of large models at very\nlow precision.\n","authors":["Zifei Xu","Sayeh Sharify","Wanzin Yazar","Tristan Webb","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12616v2","updated":"2024-10-18T16:09:52Z","published":"2024-06-18T13:44:07Z","title":"Learning diffusion at lightspeed","summary":"  Diffusion regulates numerous natural processes and the dynamics of many\nsuccessful generative models. Existing models to learn the diffusion terms from\nobservational data rely on complex bilevel optimization problems and model only\nthe drift of the system. We propose a new simple model, JKOnet*, which bypasses\nthe complexity of existing architectures while presenting significantly\nenhanced representational capabilities: JKOnet* recovers the potential,\ninteraction, and internal energy components of the underlying diffusion\nprocess. JKOnet* minimizes a simple quadratic loss and outperforms other\nbaselines in terms of sample efficiency, computational complexity, and\naccuracy. Additionally, JKOnet* provides a closed-form optimal solution for\nlinearly parametrized functionals, and, when applied to predict the evolution\nof cellular processes from real-world data, it achieves state-of-the-art\naccuracy at a fraction of the computational cost of all existing methods. Our\nmethodology is based on the interpretation of diffusion processes as\nenergy-minimizing trajectories in the probability space via the so-called JKO\nscheme, which we study via its first-order optimality conditions.\n","authors":["Antonio Terpin","Nicolas Lanzetti","Martin Gadea","Florian Dörfler"],"pdf_url":"https://arxiv.org/pdf/2406.12616v2.pdf","comment":"Accepted for presentation at, and publication in the proceedings of,\n  the 38th Conference on Neural Information Processing Systems (NeurIPS 2024,\n  oral)"},{"id":"http://arxiv.org/abs/2410.14556v1","updated":"2024-10-18T15:59:54Z","published":"2024-10-18T15:59:54Z","title":"Measuring Diversity: Axioms and Challenges","summary":"  The concept of diversity is widely used in various applications: from image\nor molecule generation to recommender systems. Thus, being able to properly\nmeasure diversity is important. This paper addresses the problem of quantifying\ndiversity for a set of objects. First, we make a systematic review of existing\ndiversity measures and explore their undesirable behavior in some cases. Based\non this review, we formulate three desirable properties (axioms) of a reliable\ndiversity measure: monotonicity, uniqueness, and continuity. We show that none\nof the existing measures has all three properties and thus these measures are\nnot suitable for quantifying diversity. Then, we construct two examples of\nmeasures that have all the desirable properties, thus proving that the list of\naxioms is not self-contradicting. Unfortunately, the constructed examples are\ntoo computationally complex for practical use, thus we pose an open problem of\nconstructing a diversity measure that has all the listed properties and can be\ncomputed in practice.\n","authors":["Mikhail Mironov","Liudmila Prokhorenkova"],"pdf_url":"https://arxiv.org/pdf/2410.14556v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.15652v3","updated":"2024-10-18T15:45:39Z","published":"2024-09-24T01:29:24Z","title":"English offensive text detection using CNN based Bi-GRU model","summary":"  Over the years, the number of users of social media has increased\ndrastically. People frequently share their thoughts through social platforms,\nand this leads to an increase in hate content. In this virtual community,\nindividuals share their views, express their feelings, and post photos, videos,\nblogs, and more. Social networking sites like Facebook and Twitter provide\nplatforms to share vast amounts of content with a single click. However, these\nplatforms do not impose restrictions on the uploaded content, which may include\nabusive language and explicit images unsuitable for social media. To resolve\nthis issue, a new idea must be implemented to divide the inappropriate content.\nNumerous studies have been done to automate the process. In this paper, we\npropose a new Bi-GRU-CNN model to classify whether the text is offensive or\nnot. The combination of the Bi-GRU and CNN models outperforms the existing\nmodel.\n","authors":["Tonmoy Roy","Md Robiul Islam","Asif Ahammad Miazee","Anika Antara","Al Amin","Sunjim Hossain"],"pdf_url":"https://arxiv.org/pdf/2409.15652v3.pdf","comment":"5 pages and 6 figures"},{"id":"http://arxiv.org/abs/2410.14548v1","updated":"2024-10-18T15:43:34Z","published":"2024-10-18T15:43:34Z","title":"Boosting K-means for Big Data by Fusing Data Streaming with Global\n  Optimization","summary":"  K-means clustering is a cornerstone of data mining, but its efficiency\ndeteriorates when confronted with massive datasets. To address this limitation,\nwe propose a novel heuristic algorithm that leverages the Variable Neighborhood\nSearch (VNS) metaheuristic to optimize K-means clustering for big data. Our\napproach is based on the sequential optimization of the partial objective\nfunction landscapes obtained by restricting the Minimum Sum-of-Squares\nClustering (MSSC) formulation to random samples from the original big dataset.\nWithin each landscape, systematically expanding neighborhoods of the currently\nbest (incumbent) solution are explored by reinitializing all degenerate and a\nvarying number of additional centroids. Extensive and rigorous experimentation\non a large number of real-world datasets reveals that by transforming the\ntraditional local search into a global one, our algorithm significantly\nenhances the accuracy and efficiency of K-means clustering in big data\nenvironments, becoming the new state of the art in the field.\n","authors":["Ravil Mussabayev","Rustam Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2410.14548v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11206v2","updated":"2024-10-18T15:43:02Z","published":"2024-06-17T04:53:47Z","title":"Retraining with Predicted Hard Labels Provably Increases Model Accuracy","summary":"  The performance of a model trained with \\textit{noisy labels} is often\nimproved by simply \\textit{retraining} the model with its own predicted\n\\textit{hard} labels (i.e., $1$/$0$ labels). Yet, a detailed theoretical\ncharacterization of this phenomenon is lacking. In this paper, we theoretically\nanalyze retraining in a linearly separable setting with randomly corrupted\nlabels given to us and prove that retraining can improve the population\naccuracy obtained by initially training with the given (noisy) labels. To the\nbest of our knowledge, this is the first such theoretical result. Retraining\nfinds application in improving training with local label differential privacy\n(DP) which involves training with noisy labels. We empirically show that\nretraining selectively on the samples for which the predicted label matches the\ngiven label significantly improves label DP training at \\textit{no extra\nprivacy cost}; we call this \\textit{consensus-based retraining}. As an example,\nwhen training ResNet-18 on CIFAR-100 with $\\epsilon=3$ label DP, we obtain\n$6.4\\%$ improvement in accuracy with consensus-based retraining.\n","authors":["Rudrajit Das","Inderjit S. Dhillon","Alessandro Epasto","Adel Javanmard","Jieming Mao","Vahab Mirrokni","Sujay Sanghavi","Peilin Zhong"],"pdf_url":"https://arxiv.org/pdf/2406.11206v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15379v2","updated":"2024-10-18T15:38:16Z","published":"2024-04-23T07:16:13Z","title":"Clustering of timed sequences -- Application to the analysis of care\n  pathways","summary":"  Improving the future of healthcare starts by better understanding the current\nactual practices in hospital settings. This motivates the objective of\ndiscovering typical care pathways from patient data. Revealing typical care\npathways can be achieved through clustering. The difficulty in clustering care\npathways, represented by sequences of timestamped events, lies in defining a\nsemantically appropriate metric and clustering algorithms. In this article, we\nadapt two methods developed for time series to the clustering of timed\nsequences: the drop-DTW metric and the DBA approach for the construction of\naveraged time sequences. These methods are then applied in clustering\nalgorithms to propose original and sound clustering algorithms for timed\nsequences. This approach is experimented with and evaluated on synthetic and\nreal-world data.\n","authors":["Thomas Guyet","Pierre Pinson","Enoal Gesny"],"pdf_url":"https://arxiv.org/pdf/2404.15379v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14539v1","updated":"2024-10-18T15:29:04Z","published":"2024-10-18T15:29:04Z","title":"Diffusion-based Semi-supervised Spectral Algorithm for Regression on\n  Manifolds","summary":"  We introduce a novel diffusion-based spectral algorithm to tackle regression\nanalysis on high-dimensional data, particularly data embedded within\nlower-dimensional manifolds. Traditional spectral algorithms often fall short\nin such contexts, primarily due to the reliance on predetermined kernel\nfunctions, which inadequately address the complex structures inherent in\nmanifold-based data. By employing graph Laplacian approximation, our method\nuses the local estimation property of heat kernel, offering an adaptive,\ndata-driven approach to overcome this obstacle. Another distinct advantage of\nour algorithm lies in its semi-supervised learning framework, enabling it to\nfully use the additional unlabeled data. This ability enhances the performance\nby allowing the algorithm to dig the spectrum and curvature of the data\nmanifold, providing a more comprehensive understanding of the dataset.\nMoreover, our algorithm performs in an entirely data-driven manner, operating\ndirectly within the intrinsic manifold structure of the data, without requiring\nany predefined manifold information. We provide a convergence analysis of our\nalgorithm. Our findings reveal that the algorithm achieves a convergence rate\nthat depends solely on the intrinsic dimension of the underlying manifold,\nthereby avoiding the curse of dimensionality associated with the higher ambient\ndimension.\n","authors":["Weichun Xia","Jiaxin Jiang","Lei Shi"],"pdf_url":"https://arxiv.org/pdf/2410.14539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12874v2","updated":"2024-10-18T15:26:55Z","published":"2024-10-14T18:11:53Z","title":"On Debiasing Text Embeddings Through Context Injection","summary":"  Current advances in Natural Language Processing (NLP) have made it\nincreasingly feasible to build applications leveraging textual data. Generally,\nthe core of these applications rely on having a good semantic representation of\ntext into vectors, via embedding models. However, it has been shown that these\nembeddings capture and perpetuate biases already present in text. While a few\ntechniques have been proposed to debias embeddings, they do not take advantage\nof the recent advances in context understanding of modern embedding models. In\nthis paper, we fill this gap by conducting a review of 19 embedding models by\nquantifying their biases and how well they respond to context injection as a\nmean of debiasing. We show that higher performing models are more prone to\ncapturing biases, but are also better at incorporating context. Surprisingly,\nwe find that while models can easily embed affirmative semantics, they fail at\nembedding neutral semantics. Finally, in a retrieval task, we show that biases\nin embeddings can lead to non-desirable outcomes. We use our new-found insights\nto design a simple algorithm for top $k$ retrieval, where $k$ is dynamically\nselected. We show that our algorithm is able to retrieve all relevant gendered\nand neutral chunks.\n","authors":["Thomas Uriot"],"pdf_url":"https://arxiv.org/pdf/2410.12874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14535v1","updated":"2024-10-18T15:23:29Z","published":"2024-10-18T15:23:29Z","title":"Comparing Differentiable and Dynamic Ray Tracing: Introducing the\n  Multipath Lifetime Map","summary":"  With the increasing presence of dynamic scenarios, such as Vehicle-to-Vehicle\ncommunications, radio propagation modeling tools must adapt to the rapidly\nchanging nature of the radio channel. Recently, both Differentiable and Dynamic\nRay Tracing frameworks have emerged to address these challenges. However, there\nis often confusion about how these approaches differ and which one should be\nused in specific contexts. In this paper, we provide an overview of these two\ntechniques and a comparative analysis against two state-of-the-art tools:\n3DSCAT from UniBo and Sionna from NVIDIA. To provide a more precise\ncharacterization of the scope of these methods, we introduce a novel\nsimulation-based metric, the Multipath Lifetime Map, which enables the\nevaluation of spatial and temporal coherence in radio channels only based on\nthe geometrical description of the environment. Finally, our metrics are\nevaluated on a classic urban street canyon scenario, yielding similar results\nto those obtained from measurement campaigns.\n","authors":["Jérome Eertmans","Enrico Maria Vittuci","Vittorio Degli Esposti","Laurent Jacques","Claude Oestges"],"pdf_url":"https://arxiv.org/pdf/2410.14535v1.pdf","comment":"5 pages, 5 figures, 1 table, submitted to EuCAP 2025"},{"id":"http://arxiv.org/abs/2404.07864v2","updated":"2024-10-18T15:23:26Z","published":"2024-04-11T15:57:12Z","title":"Inferring Change Points in High-Dimensional Regression via Approximate\n  Message Passing","summary":"  We consider the problem of localizing change points in a generalized linear\nmodel (GLM), a model that covers many widely studied problems in statistical\nlearning including linear, logistic, and rectified linear regression. We\npropose a novel and computationally efficient Approximate Message Passing (AMP)\nalgorithm for estimating both the signals and the change point locations, and\nrigorously characterize its performance in the high-dimensional limit where the\nnumber of parameters $p$ is proportional to the number of samples $n$. This\ncharacterization is in terms of a state evolution recursion, which allows us to\nprecisely compute performance measures such as the asymptotic Hausdorff error\nof our change point estimates, and allows us to tailor the algorithm to take\nadvantage of any prior structural information on the signals and change points.\nMoreover, we show how our AMP iterates can be used to efficiently compute a\nBayesian posterior distribution over the change point locations in the\nhigh-dimensional limit. We validate our theory via numerical experiments, and\ndemonstrate the favorable performance of our estimators on both synthetic and\nreal data in the settings of linear, logistic, and rectified linear regression.\n","authors":["Gabriel Arpino","Xiaoqi Liu","Julia Gontarek","Ramji Venkataramanan"],"pdf_url":"https://arxiv.org/pdf/2404.07864v2.pdf","comment":"43 pages, 9 figures. A preliminary version of this paper appeared in\n  ICML 2024"},{"id":"http://arxiv.org/abs/2408.05807v3","updated":"2024-10-18T15:19:04Z","published":"2024-08-11T15:56:44Z","title":"Kernel Density Estimators in Large Dimensions","summary":"  This paper studies Kernel Density Estimation for a high-dimensional\ndistribution $\\rho(x)$. Traditional approaches have focused on the limit of\nlarge number of data points $n$ and fixed dimension $d$. We analyze instead the\nregime where both the number $n$ of data points $y_i$ and their dimensionality\n$d$ grow with a fixed ratio $\\alpha=(\\log n)/d$. Our study reveals three\ndistinct statistical regimes for the kernel-based estimate of the density $\\hat\n\\rho_h^{\\mathcal {D}}(x)=\\frac{1}{n h^d}\\sum_{i=1}^n\nK\\left(\\frac{x-y_i}{h}\\right)$, depending on the bandwidth $h$: a classical\nregime for large bandwidth where the Central Limit Theorem (CLT) holds, which\nis akin to the one found in traditional approaches. Below a certain value of\nthe bandwidth, $h_{CLT}(\\alpha)$, we find that the CLT breaks down. The\nstatistics of $\\hat\\rho_h^{\\mathcal {D}}(x)$ for a fixed $x$ drawn from\n$\\rho(x)$ is given by a heavy-tailed distribution (an alpha-stable\ndistribution). In particular below a value $h_G(\\alpha)$, we find that\n$\\hat\\rho_h^{\\mathcal {D}}(x)$ is governed by extreme value statistics: only a\nfew points in the database matter and give the dominant contribution to the\ndensity estimator. We provide a detailed analysis for high-dimensional\nmultivariate Gaussian data. We show that the optimal bandwidth threshold based\non Kullback-Leibler divergence lies in the new statistical regime identified in\nthis paper. As known by practitioners, when decreasing the bandwidth a\nKernel-estimated estimated changes from a smooth curve to a collections of\npeaks centred on the data points. Our findings reveal that this general\nphenomenon is related to sharp transitions between phases characterized by\ndifferent statistical properties, and offer new insights for Kernel density\nestimation in high-dimensional settings.\n","authors":["Giulio Biroli","Marc Mézard"],"pdf_url":"https://arxiv.org/pdf/2408.05807v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14533v1","updated":"2024-10-18T15:14:25Z","published":"2024-10-18T15:14:25Z","title":"The Traveling Bandit: A Framework for Bayesian Optimization with\n  Movement Costs","summary":"  This paper introduces a framework for Bayesian Optimization (BO) with metric\nmovement costs, addressing a critical challenge in practical applications where\ninput alterations incur varying costs. Our approach is a convenient plug-in\nthat seamlessly integrates with the existing literature on batched algorithms,\nwhere designs within batches are observed following the solution of a Traveling\nSalesman Problem. The proposed method provides a theoretical guarantee of\nconvergence in terms of movement costs for BO. Empirically, our method\neffectively reduces average movement costs over time while maintaining\ncomparable regret performance to conventional BO methods. This framework also\nshows promise for broader applications in various bandit settings with movement\ncosts.\n","authors":["Qiyuan Chen","Raed Al Kontar"],"pdf_url":"https://arxiv.org/pdf/2410.14533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14532v1","updated":"2024-10-18T15:13:07Z","published":"2024-10-18T15:13:07Z","title":"Using Sentiment and Technical Analysis to Predict Bitcoin with Machine\n  Learning","summary":"  Cryptocurrencies have gained significant attention in recent years due to\ntheir decentralized nature and potential for financial innovation. Thus, the\nability to accurately predict its price has become a subject of great interest\nfor investors, traders, and researchers. Some works in the literature show how\nBitcoin's market sentiment correlates with its price fluctuations in the\nmarket. However, papers that consider the sentiment of the market associated\nwith financial Technical Analysis indicators in order to predict Bitcoin's\nprice are still scarce. In this paper, we present a novel approach for\npredicting Bitcoin price movements by combining the Fear & Greedy Index, a\nmeasure of market sentiment, Technical Analysis indicators, and the potential\nof Machine Learning algorithms. This work represents a preliminary study on the\nimportance of sentiment metrics in cryptocurrency forecasting. Our initial\nexperiments demonstrate promising results considering investment returns,\nsurpassing the Buy & Hold baseline, and offering valuable insights about the\ncombination of indicators of sentiment and market in a cryptocurrency\nprediction model.\n","authors":["Arthur Emanuel de Oliveira Carosia"],"pdf_url":"https://arxiv.org/pdf/2410.14532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14528v1","updated":"2024-10-18T15:10:55Z","published":"2024-10-18T15:10:55Z","title":"Domain Adaptive Safety Filters via Deep Operator Learning","summary":"  Learning-based approaches for constructing Control Barrier Functions (CBFs)\nare increasingly being explored for safety-critical control systems. However,\nthese methods typically require complete retraining when applied to unseen\nenvironments, limiting their adaptability. To address this, we propose a\nself-supervised deep operator learning framework that learns the mapping from\nenvironmental parameters to the corresponding CBF, rather than learning the CBF\ndirectly. Our approach leverages the residual of a parametric Partial\nDifferential Equation (PDE), where the solution defines a parametric CBF\napproximating the maximal control invariant set. This framework accommodates\ncomplex safety constraints, higher relative degrees, and actuation limits. We\ndemonstrate the effectiveness of the method through numerical experiments on\nnavigation tasks involving dynamic obstacles.\n","authors":["Lakshmideepakreddy Manda","Shaoru Chen","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2410.14528v1.pdf","comment":"63rd IEEE Conference on Decision and Control (CDC)"},{"id":"http://arxiv.org/abs/2410.14522v1","updated":"2024-10-18T15:06:50Z","published":"2024-10-18T15:06:50Z","title":"Rethinking Distance Metrics for Counterfactual Explainability","summary":"  Counterfactual explanations have been a popular method of post-hoc\nexplainability for a variety of settings in Machine Learning. Such methods\nfocus on explaining classifiers by generating new data points that are similar\nto a given reference, while receiving a more desirable prediction. In this\nwork, we investigate a framing for counterfactual generation methods that\nconsiders counterfactuals not as independent draws from a region around the\nreference, but as jointly sampled with the reference from the underlying data\ndistribution. Through this framing, we derive a distance metric, tailored for\ncounterfactual similarity that can be applied to a broad range of settings.\nThrough both quantitative and qualitative analyses of counterfactual generation\nmethods, we show that this framing allows us to express more nuanced\ndependencies among the covariates.\n","authors":["Joshua Nathaniel Williams","Anurag Katakkar","Hoda Heidari","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2410.14522v1.pdf","comment":"13 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2410.14515v1","updated":"2024-10-18T14:54:40Z","published":"2024-10-18T14:54:40Z","title":"Efficient Annotator Reliability Assessment and Sample Weighting for\n  Knowledge-Based Misinformation Detection on Social Media","summary":"  Misinformation spreads rapidly on social media, confusing the truth and\ntargetting potentially vulnerable people. To effectively mitigate the negative\nimpact of misinformation, it must first be accurately detected before applying\na mitigation strategy, such as X's community notes, which is currently a manual\nprocess. This study takes a knowledge-based approach to misinformation\ndetection, modelling the problem similarly to one of natural language\ninference. The EffiARA annotation framework is introduced, aiming to utilise\ninter- and intra-annotator agreement to understand the reliability of each\nannotator and influence the training of large language models for\nclassification based on annotator reliability. In assessing the EffiARA\nannotation framework, the Russo-Ukrainian Conflict Knowledge-Based\nMisinformation Classification Dataset (RUC-MCD) was developed and made publicly\navailable. This study finds that sample weighting using annotator reliability\nperforms the best, utilising both inter- and intra-annotator agreement and\nsoft-label training. The highest classification performance achieved using\nLlama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.\n","authors":["Owen Cook","Charlie Grimshaw","Ben Wu","Sophie Dillon","Jack Hicks","Luke Jones","Thomas Smith","Matyas Szert","Xingyi Song"],"pdf_url":"https://arxiv.org/pdf/2410.14515v1.pdf","comment":"8 pages, 3 figures, 3 tables. Code available here:\n  https://github.com/MiniEggz/ruc-misinfo"},{"id":"http://arxiv.org/abs/2406.07361v2","updated":"2024-10-18T14:38:03Z","published":"2024-06-11T15:28:48Z","title":"Deep Implicit Optimization for Robust and Flexible Image Registration","summary":"  Deep Learning in Image Registration (DLIR) methods have been tremendously\nsuccessful in image registration due to their speed and ability to incorporate\nweak label supervision at training time. However, DLIR methods forego many of\nthe benefits of classical optimization-based methods. The functional nature of\ndeep networks do not guarantee that the predicted transformation is a local\nminima of the registration objective, the representation of the transformation\n(displacement/velocity field/affine) is fixed, and the networks are not robust\nto domain shift. Our method aims to bridge this gap between classical and\nlearning methods by incorporating optimization as a layer in a deep network. A\ndeep network is trained to predict multi-scale dense feature images that are\nregistered using a black box iterative optimization solver. This optimal warp\nis then used to minimize image and label alignment errors. By implicitly\ndifferentiating end-to-end through an iterative optimization solver, our\nlearned features are registration and label-aware, and the warp functions are\nguaranteed to be local minima of the registration objective in the feature\nspace. Our framework shows excellent performance on in-domain datasets, and is\nagnostic to domain shift such as anisotropy and varying intensity profiles. For\nthe first time, our method allows switching between arbitrary transformation\nrepresentations (free-form to diffeomorphic) at test time with zero retraining.\nEnd-to-end feature learning also facilitates interpretability of features, and\nout-of-the-box promptability using additional label-fidelity terms at\ninference.\n","authors":["Rohit Jena","Pratik Chaudhari","James C. Gee"],"pdf_url":"https://arxiv.org/pdf/2406.07361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08979v2","updated":"2024-10-18T14:35:53Z","published":"2024-10-11T16:54:07Z","title":"Overcoming Slow Decision Frequencies in Continuous Control: Model-Based\n  Sequence Reinforcement Learning for Model-Free Control","summary":"  Reinforcement learning (RL) is rapidly reaching and surpassing human-level\ncontrol capabilities. However, state-of-the-art RL algorithms often require\ntimesteps and reaction times significantly faster than human capabilities,\nwhich is impractical in real-world settings and typically necessitates\nspecialized hardware. Such speeds are difficult to achieve in the real world\nand often requires specialized hardware. We introduce Sequence Reinforcement\nLearning (SRL), an RL algorithm designed to produce a sequence of actions for a\ngiven input state, enabling effective control at lower decision frequencies.\nSRL addresses the challenges of learning action sequences by employing both a\nmodel and an actor-critic architecture operating at different temporal scales.\nWe propose a \"temporal recall\" mechanism, where the critic uses the model to\nestimate intermediate states between primitive actions, providing a learning\nsignal for each individual action within the sequence. Once training is\ncomplete, the actor can generate action sequences independently of the model,\nachieving model-free control at a slower frequency. We evaluate SRL on a suite\nof continuous control tasks, demonstrating that it achieves performance\ncomparable to state-of-the-art algorithms while significantly reducing actor\nsample complexity. To better assess performance across varying decision\nfrequencies, we introduce the Frequency-Averaged Score (FAS) metric. Our\nresults show that SRL significantly outperforms traditional RL algorithms in\nterms of FAS, making it particularly suitable for applications requiring\nvariable decision frequencies. Additionally, we compare SRL with model-based\nonline planning, showing that SRL achieves superior FAS while leveraging the\nsame model during training that online planners use for planning.\n","authors":["Devdhar Patel","Hava Siegelmann"],"pdf_url":"https://arxiv.org/pdf/2410.08979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13012v2","updated":"2024-10-18T14:32:21Z","published":"2024-10-16T20:14:02Z","title":"Sample Compression Scheme Reductions","summary":"  We present novel reductions from sample compression schemes in multiclass\nclassification, regression, and adversarially robust learning settings to\nbinary sample compression schemes. Assuming we have a compression scheme for\nbinary classes of size $f(d_\\mathrm{VC})$, where $d_\\mathrm{VC}$ is the VC\ndimension, then we have the following results: (1) If the binary compression\nscheme is a majority-vote or a stable compression scheme, then there exists a\nmulticlass compression scheme of size $O(f(d_\\mathrm{G}))$, where\n$d_\\mathrm{G}$ is the graph dimension. Moreover, for general binary compression\nschemes, we obtain a compression of size $O(f(d_\\mathrm{G})\\log|Y|)$, where $Y$\nis the label space. (2) If the binary compression scheme is a majority-vote or\na stable compression scheme, then there exists an $\\epsilon$-approximate\ncompression scheme for regression over $[0,1]$-valued functions of size\n$O(f(d_\\mathrm{P}))$, where $d_\\mathrm{P}$ is the pseudo-dimension. For general\nbinary compression schemes, we obtain a compression of size\n$O(f(d_\\mathrm{P})\\log(1/\\epsilon))$. These results would have significant\nimplications if the sample compression conjecture, which posits that any binary\nconcept class with a finite VC dimension admits a binary compression scheme of\nsize $O(d_\\mathrm{VC})$, is resolved (Littlestone and Warmuth, 1986; Floyd and\nWarmuth, 1995; Warmuth, 2003). Our results would then extend the proof of the\nconjecture immediately to other settings. We establish similar results for\nadversarially robust learning and also provide an example of a concept class\nthat is robustly learnable but has no bounded-size compression scheme,\ndemonstrating that learnability is not equivalent to having a compression\nscheme independent of the sample size, unlike in binary classification, where\ncompression of size $2^{O(d_\\mathrm{VC})}$ is attainable (Moran and Yehudayoff,\n2016).\n","authors":["Idan Attias","Steve Hanneke","Arvind Ramaswami"],"pdf_url":"https://arxiv.org/pdf/2410.13012v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14489v1","updated":"2024-10-18T14:19:13Z","published":"2024-10-18T14:19:13Z","title":"An Integrated Deep Learning Model for Skin Cancer Detection Using Hybrid\n  Feature Fusion Technique","summary":"  Skin cancer is a serious and potentially fatal disease caused by DNA damage.\nEarly detection significantly increases survival rates, making accurate\ndiagnosis crucial. In this groundbreaking study, we present a hybrid framework\nbased on Deep Learning (DL) that achieves precise classification of benign and\nmalignant skin lesions. Our approach begins with dataset preprocessing to\nenhance classification accuracy, followed by training two separate pre-trained\nDL models, InceptionV3 and DenseNet121. By fusing the results of each model\nusing the weighted sum rule, our system achieves exceptional accuracy rates.\nSpecifically, we achieve a 92.27% detection accuracy rate, 92.33% sensitivity,\n92.22% specificity, 90.81% precision, and 91.57% F1-score, outperforming\nexisting models and demonstrating the robustness and trustworthiness of our\nhybrid approach. Our study represents a significant advance in skin cancer\ndiagnosis and provides a promising foundation for further research in the\nfield. With the potential to save countless lives through earlier detection,\nour hybrid deep-learning approach is a game-changer in the fight against skin\ncancer.\n","authors":["Maksuda Akter","Rabea Khatun","Md. Alamin Talukder","Md. Manowarul Islam","Md. Ashraf Uddin"],"pdf_url":"https://arxiv.org/pdf/2410.14489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14488v1","updated":"2024-10-18T14:16:54Z","published":"2024-10-18T14:16:54Z","title":"ANT: Adaptive Noise Schedule for Time Series Diffusion Models","summary":"  Advances in diffusion models for generative artificial intelligence have\nrecently propagated to the time series (TS) domain, demonstrating\nstate-of-the-art performance on various tasks. However, prior works on TS\ndiffusion models often borrow the framework of existing works proposed in other\ndomains without considering the characteristics of TS data, leading to\nsuboptimal performance. In this work, we propose Adaptive Noise schedule for\nTime series diffusion models (ANT), which automatically predetermines proper\nnoise schedules for given TS datasets based on their statistics representing\nnon-stationarity. Our intuition is that an optimal noise schedule should\nsatisfy the following desiderata: 1) It linearly reduces the non-stationarity\nof TS data so that all diffusion steps are equally meaningful, 2) the data is\ncorrupted to the random noise at the final step, and 3) the number of steps is\nsufficiently large. The proposed method is practical for use in that it\neliminates the necessity of finding the optimal noise schedule with a small\nadditional cost to compute the statistics for given datasets, which can be done\noffline before training. We validate the effectiveness of our method across\nvarious tasks, including TS forecasting, refinement, and generation, on\ndatasets from diverse domains. Code is available at this repository:\nhttps://github.com/seunghan96/ANT.\n","authors":["Seunghan Lee","Kibok Lee","Taeyoung Park"],"pdf_url":"https://arxiv.org/pdf/2410.14488v1.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.14485v1","updated":"2024-10-18T14:10:16Z","published":"2024-10-18T14:10:16Z","title":"CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and\n  Fully-Connected Neural Networks for Causally Constrained Predictions","summary":"  Artificial Neural Networks (ANNs), including fully-connected networks and\ntransformers, are highly flexible and powerful function approximators, widely\napplied in fields like computer vision and natural language processing.\nHowever, their inability to inherently respect causal structures can limit\ntheir robustness, making them vulnerable to covariate shift and difficult to\ninterpret/explain. This poses significant challenges for their reliability in\nreal-world applications. In this paper, we introduce Causal Fully-Connected\nNeural Networks (CFCNs) and Causal Transformers (CaTs), two general model\nfamilies designed to operate under predefined causal constraints, as specified\nby a Directed Acyclic Graph (DAG). These models retain the powerful function\napproximation abilities of traditional neural networks while adhering to the\nunderlying structural constraints, improving robustness, reliability, and\ninterpretability at inference time. This approach opens new avenues for\ndeploying neural networks in more demanding, real-world scenarios where\nrobustness and explainability is critical.\n","authors":["Matthew J. Vowels","Mathieu Rochat","Sina Akbari"],"pdf_url":"https://arxiv.org/pdf/2410.14485v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14484v1","updated":"2024-10-18T14:08:41Z","published":"2024-10-18T14:08:41Z","title":"Transfer Reinforcement Learning in Heterogeneous Action Spaces using\n  Subgoal Mapping","summary":"  In this paper, we consider a transfer reinforcement learning problem\ninvolving agents with different action spaces. Specifically, for any new unseen\ntask, the goal is to use a successful demonstration of this task by an expert\nagent in its action space to enable a learner agent learn an optimal policy in\nits own different action space with fewer samples than those required if the\nlearner was learning on its own. Existing transfer learning methods across\ndifferent action spaces either require handcrafted mappings between those\naction spaces provided by human experts, which can induce bias in the learning\nprocedure, or require the expert agent to share its policy parameters with the\nlearner agent, which does not generalize well to unseen tasks. In this work, we\npropose a method that learns a subgoal mapping between the expert agent policy\nand the learner agent policy. Since the expert agent and the learner agent have\ndifferent action spaces, their optimal policies can have different subgoal\ntrajectories. We learn this subgoal mapping by training a Long Short Term\nMemory (LSTM) network for a distribution of tasks and then use this mapping to\npredict the learner subgoal sequence for unseen tasks, thereby improving the\nspeed of learning by biasing the agent's policy towards the predicted learner\nsubgoal sequence. Through numerical experiments, we demonstrate that the\nproposed learning scheme can effectively find the subgoal mapping underlying\nthe given distribution of tasks. Moreover, letting the learner agent imitate\nthe expert agent's policy with the learnt subgoal mapping can significantly\nimprove the sample efficiency and training time of the learner agent in unseen\nnew tasks.\n","authors":["Kavinayan P. Sivakumar","Yan Zhang","Zachary Bell","Scott Nivison","Michael M. Zavlanos"],"pdf_url":"https://arxiv.org/pdf/2410.14484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14483v1","updated":"2024-10-18T14:06:49Z","published":"2024-10-18T14:06:49Z","title":"Spectral Representations for Accurate Causal Uncertainty Quantification\n  with Gaussian Processes","summary":"  Accurate uncertainty quantification for causal effects is essential for\nrobust decision making in complex systems, but remains challenging in\nnon-parametric settings. One promising framework represents conditional\ndistributions in a reproducing kernel Hilbert space and places Gaussian process\npriors on them to infer posteriors on causal effects, but requires restrictive\nnuclear dominant kernels and approximations that lead to unreliable uncertainty\nestimates. In this work, we introduce a method, IMPspec, that addresses these\nlimitations via a spectral representation of the Hilbert space. We show that\nposteriors in this model can be obtained explicitly, by extending a result in\nHilbert space regression theory. We also learn the spectral representation to\noptimise posterior calibration. Our method achieves state-of-the-art\nperformance in uncertainty quantification and causal Bayesian optimisation\nacross simulations and a healthcare application.\n","authors":["Hugh Dance","Peter Orbanz","Arthur Gretton"],"pdf_url":"https://arxiv.org/pdf/2410.14483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09804v2","updated":"2024-10-18T14:03:05Z","published":"2024-10-13T11:15:38Z","title":"BlackDAN: A Black-Box Multi-Objective Approach for Effective and\n  Contextual Jailbreaking of Large Language Models","summary":"  While large language models (LLMs) exhibit remarkable capabilities across\nvarious tasks, they encounter potential security risks such as jailbreak\nattacks, which exploit vulnerabilities to bypass security measures and generate\nharmful outputs. Existing jailbreak strategies mainly focus on maximizing\nattack success rate (ASR), frequently neglecting other critical factors,\nincluding the relevance of the jailbreak response to the query and the level of\nstealthiness. This narrow focus on single objectives can result in ineffective\nattacks that either lack contextual relevance or are easily recognizable. In\nthis work, we introduce BlackDAN, an innovative black-box attack framework with\nmulti-objective optimization, aiming to generate high-quality prompts that\neffectively facilitate jailbreaking while maintaining contextual relevance and\nminimizing detectability. BlackDAN leverages Multiobjective Evolutionary\nAlgorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks\nacross multiple objectives including ASR, stealthiness, and semantic relevance.\nBy integrating mechanisms like mutation, crossover, and Pareto-dominance,\nBlackDAN provides a transparent and interpretable process for generating\njailbreaks. Furthermore, the framework allows customization based on user\npreferences, enabling the selection of prompts that balance harmfulness,\nrelevance, and other factors. Experimental results demonstrate that BlackDAN\noutperforms traditional single-objective methods, yielding higher success rates\nand improved robustness across various LLMs and multimodal LLMs, while ensuring\njailbreak responses are both relevant and less detectable.\n","authors":["Xinyuan Wang","Victor Shea-Jay Huang","Renmiao Chen","Hao Wang","Chengwei Pan","Lei Sha","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2410.09804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14479v1","updated":"2024-10-18T14:02:34Z","published":"2024-10-18T14:02:34Z","title":"Backdoored Retrievers for Prompt Injection Attacks on Retrieval\n  Augmented Generation of Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating coherent text but remain limited by the static nature of their\ntraining data. Retrieval Augmented Generation (RAG) addresses this issue by\ncombining LLMs with up-to-date information retrieval, but also expand the\nattack surface of the system. This paper investigates prompt injection attacks\non RAG, focusing on malicious objectives beyond misinformation, such as\ninserting harmful links, promoting unauthorized services, and initiating\ndenial-of-service behaviors. We build upon existing corpus poisoning techniques\nand propose a novel backdoor attack aimed at the fine-tuning process of the\ndense retriever component. Our experiments reveal that corpus poisoning can\nachieve significant attack success rates through the injection of a small\nnumber of compromised documents into the retriever corpus. In contrast,\nbackdoor attacks demonstrate even higher success rates but necessitate a more\ncomplex setup, as the victim must fine-tune the retriever using the attacker\npoisoned dataset.\n","authors":["Cody Clop","Yannick Teglia"],"pdf_url":"https://arxiv.org/pdf/2410.14479v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.14477v1","updated":"2024-10-18T14:02:06Z","published":"2024-10-18T14:02:06Z","title":"Laplace Transform Based Low-Complexity Learning of Continuous Markov\n  Semigroups","summary":"  Markov processes serve as a universal model for many real-world random\nprocesses. This paper presents a data-driven approach for learning these models\nthrough the spectral decomposition of the infinitesimal generator (IG) of the\nMarkov semigroup. The unbounded nature of IGs complicates traditional methods\nsuch as vector-valued regression and Hilbert-Schmidt operator analysis.\nExisting techniques, including physics-informed kernel regression, are\ncomputationally expensive and limited in scope, with no recovery guarantees for\ntransfer operator methods when the time-lag is small. We propose a novel method\nthat leverages the IG's resolvent, characterized by the Laplace transform of\ntransfer operators. This approach is robust to time-lag variations, ensuring\naccurate eigenvalue learning even for small time-lags. Our statistical analysis\napplies to a broader class of Markov processes than current methods while\nreducing computational complexity from quadratic to linear in the state\ndimension. Finally, we illustrate the behaviour of our method in two\nexperiments.\n","authors":["Vladimir R. Kostic","Karim Lounici","Hélène Halconruy","Timothée Devergne","Pietro Novelli","Massimiliano Pontil"],"pdf_url":"https://arxiv.org/pdf/2410.14477v1.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2410.14475v1","updated":"2024-10-18T14:00:44Z","published":"2024-10-18T14:00:44Z","title":"Enhancing Cryptocurrency Market Forecasting: Advanced Machine Learning\n  Techniques and Industrial Engineering Contributions","summary":"  Cryptocurrencies, as decentralized digital assets, have experienced rapid\ngrowth and adoption, with over 23,000 cryptocurrencies and a market\ncapitalization nearing \\$1.1 trillion (about \\$3,400 per person in the US) as\nof 2023. This dynamic market presents significant opportunities and risks,\nhighlighting the need for accurate price prediction models to manage\nvolatility. This chapter comprehensively reviews machine learning (ML)\ntechniques applied to cryptocurrency price prediction from 2014 to 2024. We\nexplore various ML algorithms, including linear models, tree-based approaches,\nand advanced deep learning architectures such as transformers and large\nlanguage models. Additionally, we examine the role of sentiment analysis in\ncapturing market sentiment from textual data like social media posts and news\narticles to anticipate price fluctuations. With expertise in optimizing complex\nsystems and processes, industrial engineers are pivotal in enhancing these\nmodels. They contribute by applying principles of process optimization,\nefficiency, and risk mitigation to improve computational performance and data\nmanagement. This chapter highlights the evolving landscape of cryptocurrency\nprice prediction, the integration of emerging technologies, and the significant\nrole of industrial engineers in refining predictive models. By addressing\ncurrent limitations and exploring future research directions, this chapter aims\nto advance the development of more accurate and robust prediction systems,\nsupporting better-informed investment decisions and more stable market\nbehavior.\n","authors":["Jannatun Nayeem Pinky","Ramya Akula"],"pdf_url":"https://arxiv.org/pdf/2410.14475v1.pdf","comment":"63 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.14470v1","updated":"2024-10-18T13:54:46Z","published":"2024-10-18T13:54:46Z","title":"How Do Training Methods Influence the Utilization of Vision Models?","summary":"  Not all learnable parameters (e.g., weights) contribute equally to a neural\nnetwork's decision function. In fact, entire layers' parameters can sometimes\nbe reset to random values with little to no impact on the model's decisions. We\nrevisit earlier studies that examined how architecture and task complexity\ninfluence this phenomenon and ask: is this phenomenon also affected by how we\ntrain the model? We conducted experimental evaluations on a diverse set of\nImageNet-1k classification models to explore this, keeping the architecture and\ntraining data constant but varying the training pipeline. Our findings reveal\nthat the training method strongly influences which layers become critical to\nthe decision function for a given task. For example, improved training regimes\nand self-supervised training increase the importance of early layers while\nsignificantly under-utilizing deeper layers. In contrast, methods such as\nadversarial training display an opposite trend. Our preliminary results extend\nprevious findings, offering a more nuanced understanding of the inner mechanics\nof neural networks.\n  Code: https://github.com/paulgavrikov/layer_criticality\n","authors":["Paul Gavrikov","Shashank Agnihotri","Margret Keuper","Janis Keuper"],"pdf_url":"https://arxiv.org/pdf/2410.14470v1.pdf","comment":"Accepted at the Interpretable AI: Past, Present and Future Workshop\n  at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.14466v1","updated":"2024-10-18T13:51:25Z","published":"2024-10-18T13:51:25Z","title":"Flow-based Sampling for Entanglement Entropy and the Machine Learning of\n  Defects","summary":"  We introduce a novel technique to numerically calculate R\\'enyi entanglement\nentropies in lattice quantum field theory using generative models. We describe\nhow flow-based approaches can be combined with the replica trick using a custom\nneural-network architecture around a lattice defect connecting two replicas.\nNumerical tests for the $\\phi^4$ scalar field theory in two and three\ndimensions demonstrate that our technique outperforms state-of-the-art Monte\nCarlo calculations, and exhibit a promising scaling with the defect size.\n","authors":["Andrea Bulgarelli","Elia Cellini","Karl Jansen","Stefan Kühn","Alessandro Nada","Shinichi Nakajima","Kim A. Nicoli","Marco Panero"],"pdf_url":"https://arxiv.org/pdf/2410.14466v1.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2404.18550v4","updated":"2024-10-18T13:50:10Z","published":"2024-04-29T09:45:46Z","title":"IncidentResponseGPT: Generating Traffic Incident Response Plans with\n  Generative Artificial Intelligence","summary":"  The proposed IncidentResponseGPT framework - a novel system that applies\ngenerative artificial intelligence (AI) to potentially enhance the efficiency\nand effectiveness of traffic incident response. This model allows for synthesis\nof region-specific incident response guidelines and generates incident response\nplans adapted to specific area, aiming to expedite decision-making for traffic\nmanagement authorities. This approach aims to accelerate incident resolution\ntimes by suggesting various recommendations (e.g. optimal rerouting strategies,\nestimating resource needs) to minimize the overall impact on the urban traffic\nnetwork. The system suggests specific actions, including dynamic lane closures,\noptimized rerouting and dispatching appropriate emergency resources. We utilize\nthe Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) to\nrank generated response plans based on criteria like impact minimization and\nresource efficiency based on their proximity to an human-proposed solution.\n","authors":["Artur Grigorev","Adriana-Simona Mihaita Khaled Saleh","Yuming Ou"],"pdf_url":"https://arxiv.org/pdf/2404.18550v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14464v1","updated":"2024-10-18T13:48:01Z","published":"2024-10-18T13:48:01Z","title":"Electrocardiogram-Language Model for Few-Shot Question Answering with\n  Meta Learning","summary":"  Electrocardiogram (ECG) interpretation requires specialized expertise, often\ninvolving synthesizing insights from ECG signals with complex clinical queries\nposed in natural language. The scarcity of labeled ECG data coupled with the\ndiverse nature of clinical inquiries presents a significant challenge for\ndeveloping robust and adaptable ECG diagnostic systems. This work introduces a\nnovel multimodal meta-learning method for few-shot ECG question answering,\naddressing the challenge of limited labeled data while leveraging the rich\nknowledge encoded within large language models (LLMs). Our LLM-agnostic\napproach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA\nand Gemma) via a trainable fusion module, enabling the language model to reason\nabout ECG data and generate clinically meaningful answers. Extensive\nexperiments demonstrate superior generalization to unseen diagnostic tasks\ncompared to supervised baselines, achieving notable performance even with\nlimited ECG leads. For instance, in a 5-way 5-shot setting, our method using\nLLaMA-3.1-8B achieves accuracy of 84.6%, 77.3%, and 69.6% on single verify,\nchoose and query question types, respectively. These results highlight the\npotential of our method to enhance clinical ECG interpretation by combining\nsignal processing with the nuanced language understanding capabilities of LLMs,\nparticularly in data-constrained scenarios.\n","authors":["Jialu Tang","Tong Xia","Yuan Lu","Cecilia Mascolo","Aaqib Saeed"],"pdf_url":"https://arxiv.org/pdf/2410.14464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14461v1","updated":"2024-10-18T13:40:44Z","published":"2024-10-18T13:40:44Z","title":"The Propensity for Density in Feed-forward Models","summary":"  Does the process of training a neural network to solve a task tend to use all\nof the available weights even when the task could be solved with fewer weights?\nTo address this question we study the effects of pruning fully connected,\nconvolutional and residual models while varying their widths. We find that the\nproportion of weights that can be pruned without degrading performance is\nlargely invariant to model size. Increasing the width of a model has little\neffect on the density of the pruned model relative to the increase in absolute\nsize of the pruned network. In particular, we find substantial prunability\nacross a large range of model sizes, where our biggest model is 50 times as\nwide as our smallest model. We explore three hypotheses that could explain\nthese findings.\n","authors":["Nandi Schoots","Alex Jackson","Ali Kholmovaia","Peter McBurney","Murray Shanahan"],"pdf_url":"https://arxiv.org/pdf/2410.14461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13663v4","updated":"2024-10-18T13:16:57Z","published":"2024-06-19T16:10:26Z","title":"Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation","summary":"  Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.\n","authors":["Jirui Qi","Gabriele Sarti","Raquel Fernández","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2406.13663v4.pdf","comment":"Accepted by EMNLP 2024 Main Conference. Code and data released at\n  https://github.com/Betswish/MIRAGE"},{"id":"http://arxiv.org/abs/2410.12804v2","updated":"2024-10-18T13:15:50Z","published":"2024-09-30T12:05:07Z","title":"Hip Fracture Patient Pathways and Agent-based Modelling","summary":"  Increased healthcare demand is significantly straining European services.\nDigital solutions including advanced modelling techniques offer a promising\nsolution to optimising patient flow without impacting day-to-day healthcare\nprovision. In this work we outline an ongoing project that aims to optimise\nhealthcare resources using agent-based simulations.\n","authors":["Alison N. O'Connor","Stephen E. Ryan","Gauri Vaidya","Paul Harford","Meghana Kshirsagar"],"pdf_url":"https://arxiv.org/pdf/2410.12804v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.11443v2","updated":"2024-10-18T13:09:00Z","published":"2024-10-15T09:47:49Z","title":"Are High-Degree Representations Really Unnecessary in Equivariant Graph\n  Neural Networks?","summary":"  Equivariant Graph Neural Networks (GNNs) that incorporate E(3) symmetry have\nachieved significant success in various scientific applications. As one of the\nmost successful models, EGNN leverages a simple scalarization technique to\nperform equivariant message passing over only Cartesian vectors (i.e.,\n1st-degree steerable vectors), enjoying greater efficiency and efficacy\ncompared to equivariant GNNs using higher-degree steerable vectors. This\nsuccess suggests that higher-degree representations might be unnecessary. In\nthis paper, we disprove this hypothesis by exploring the expressivity of\nequivariant GNNs on symmetric structures, including $k$-fold rotations and\nregular polyhedra. We theoretically demonstrate that equivariant GNNs will\nalways degenerate to a zero function if the degree of the output\nrepresentations is fixed to 1 or other specific values. Based on this\ntheoretical insight, we propose HEGNN, a high-degree version of EGNN to\nincrease the expressivity by incorporating high-degree steerable vectors while\nmaintaining EGNN's efficiency through the scalarization trick. Our extensive\nexperiments demonstrate that HEGNN not only aligns with our theoretical\nanalyses on toy datasets consisting of symmetric structures, but also shows\nsubstantial improvements on more complicated datasets such as $N$-body and\nMD17. Our theoretical findings and empirical results potentially open up new\npossibilities for the research of equivariant GNNs.\n","authors":["Jiacheng Cen","Anyi Li","Ning Lin","Yuxiang Ren","Zihe Wang","Wenbing Huang"],"pdf_url":"https://arxiv.org/pdf/2410.11443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11877v5","updated":"2024-10-18T13:03:05Z","published":"2024-05-20T08:41:15Z","title":"A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:\n  The First Romanian Natural Language Inference Corpus","summary":"  Natural language inference (NLI), the task of recognizing the entailment\nrelationship in sentence pairs, is an actively studied topic serving as a proxy\nfor natural language understanding. Despite the relevance of the task in\nbuilding conversational agents and improving text classification, machine\ntranslation and other NLP tasks, to the best of our knowledge, there is no\npublicly available NLI corpus for the Romanian language. To this end, we\nintroduce the first Romanian NLI corpus (RoNLI) comprising 58K training\nsentence pairs, which are obtained via distant supervision, and 6K validation\nand test sentence pairs, which are manually annotated with the correct labels.\nWe conduct experiments with multiple machine learning methods based on distant\nlearning, ranging from shallow models based on word embeddings to\ntransformer-based neural networks, to establish a set of competitive baselines.\nFurthermore, we improve on the best model by employing a new curriculum\nlearning strategy based on data cartography. Our dataset and code to reproduce\nthe baselines are available at https://github.com/Eduard6421/RONLI.\n","authors":["Eduard Poesina","Cornelia Caragea","Radu Tudor Ionescu"],"pdf_url":"https://arxiv.org/pdf/2405.11877v5.pdf","comment":"Accepted at ACL 2024 (Main)"},{"id":"http://arxiv.org/abs/2410.14436v1","updated":"2024-10-18T12:53:23Z","published":"2024-10-18T12:53:23Z","title":"Learning to refine domain knowledge for biological network inference","summary":"  Perturbation experiments allow biologists to discover causal relationships\nbetween variables of interest, but the sparsity and high dimensionality of\nthese data pose significant challenges for causal structure learning\nalgorithms. Biological knowledge graphs can bootstrap the inference of causal\nstructures in these situations, but since they compile vastly diverse\ninformation, they can bias predictions towards well-studied systems.\nAlternatively, amortized causal structure learning algorithms encode inductive\nbiases through data simulation and train supervised models to recapitulate\nthese synthetic graphs. However, realistically simulating biology is arguably\neven harder than understanding a specific system. In this work, we take\ninspiration from both strategies and propose an amortized algorithm for\nrefining domain knowledge, based on data observations. On real and synthetic\ndatasets, we show that our approach outperforms baselines in recovering ground\ntruth causal graphs and identifying errors in the prior knowledge with limited\ninterventional data.\n","authors":["Peiwen Li","Menghua Wu"],"pdf_url":"https://arxiv.org/pdf/2410.14436v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14433v1","updated":"2024-10-18T12:51:19Z","published":"2024-10-18T12:51:19Z","title":"A Bioinformatic Approach Validated Utilizing Machine Learning Algorithms\n  to Identify Relevant Biomarkers and Crucial Pathways in Gallbladder Cancer","summary":"  Gallbladder cancer (GBC) is the most frequent cause of disease among biliary\ntract neoplasms. Identifying the molecular mechanisms and biomarkers linked to\nGBC progression has been a significant challenge in scientific research. Few\nrecent studies have explored the roles of biomarkers in GBC. Our study aimed to\nidentify biomarkers in GBC using machine learning (ML) and bioinformatics\ntechniques. We compared GBC tumor samples with normal samples to identify\ndifferentially expressed genes (DEGs) from two microarray datasets (GSE100363,\nGSE139682) obtained from the NCBI GEO database. A total of 146 DEGs were found,\nwith 39 up-regulated and 107 down-regulated genes. Functional enrichment\nanalysis of these DEGs was performed using Gene Ontology (GO) terms and\nREACTOME pathways through DAVID. The protein-protein interaction network was\nconstructed using the STRING database. To identify hub genes, we applied three\nranking algorithms: Degree, MNC, and Closeness Centrality. The intersection of\nhub genes from these algorithms yielded 11 hub genes. Simultaneously, two\nfeature selection methods (Pearson correlation and recursive feature\nelimination) were used to identify significant gene subsets. We then developed\nML models using SVM and RF on the GSE100363 dataset, with validation on\nGSE139682, to determine the gene subset that best distinguishes GBC samples.\nThe hub genes outperformed the other gene subsets. Finally, NTRK2, COL14A1,\nSCN4B, ATP1A2, SLC17A7, SLIT3, COL7A1, CLDN4, CLEC3B, ADCYAP1R1, and MFAP4 were\nidentified as crucial genes, with SLIT3, COL7A1, and CLDN4 being strongly\nlinked to GBC development and prediction.\n","authors":["Rabea Khatun","Wahia Tasnim","Maksuda Akter","Md Manowarul Islam","Md. Ashraf Uddin","Md. Zulfiker Mahmud","Saurav Chandra Das"],"pdf_url":"https://arxiv.org/pdf/2410.14433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14429v1","updated":"2024-10-18T12:48:22Z","published":"2024-10-18T12:48:22Z","title":"FashionR2R: Texture-preserving Rendered-to-Real Image Translation with\n  Diffusion Models","summary":"  Modeling and producing lifelike clothed human images has attracted\nresearchers' attention from different areas for decades, with the complexity\nfrom highly articulated and structured content. Rendering algorithms decompose\nand simulate the imaging process of a camera, while are limited by the accuracy\nof modeled variables and the efficiency of computation. Generative models can\nproduce impressively vivid human images, however still lacking in\ncontrollability and editability. This paper studies photorealism enhancement of\nrendered images, leveraging generative power from diffusion models on the\ncontrolled basis of rendering. We introduce a novel framework to translate\nrendered images into their realistic counterparts, which consists of two\nstages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG).\nIn DKI, we adopt positive (real) domain finetuning and negative (rendered)\ndomain embedding to inject knowledge into a pretrained Text-to-image (T2I)\ndiffusion model. In RIG, we generate the realistic image corresponding to the\ninput rendered image, with a Texture-preserving Attention Control (TAC) to\npreserve fine-grained clothing textures, exploiting the decoupled features\nencoded in the UNet structure. Additionally, we introduce SynFashion dataset,\nfeaturing high-quality digital clothing images with diverse textures. Extensive\nexperimental results demonstrate the superiority and effectiveness of our\nmethod in rendered-to-real image translation.\n","authors":["Rui Hu","Qian He","Gaofeng He","Jiedong Zhuang","Huang Chen","Huafeng Liu","Huamin Wang"],"pdf_url":"https://arxiv.org/pdf/2410.14429v1.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.14426v1","updated":"2024-10-18T12:41:41Z","published":"2024-10-18T12:41:41Z","title":"Predicting time-varying flux and balance in metabolic systems using\n  structured neural-ODE processes","summary":"  We develop a novel data-driven framework as an alternative to dynamic flux\nbalance analysis, bypassing the demand for deep domain knowledge and manual\nefforts to formulate the optimization problem. The proposed framework is\nend-to-end, which trains a structured neural ODE process (SNODEP) model to\nestimate flux and balance samples using gene-expression time-series data.\nSNODEP is designed to circumvent the limitations of the standard neural ODE\nprocess model, including restricting the latent and decoder sampling\ndistributions to be normal and lacking structure between context points for\ncalculating the latent, thus more suitable for modeling the underlying dynamics\nof a metabolic system. Through comprehensive experiments ($156$ in total), we\ndemonstrate that SNODEP not only predicts the unseen time points of real-world\ngene-expression data and the flux and balance estimates well but can even\ngeneralize to more challenging unseen knockout configurations and irregular\ndata sampling scenarios, all essential for metabolic pathway analysis. We hope\nour work can serve as a catalyst for building more scalable and powerful models\nfor genome-scale metabolic analysis. Our code is available at:\n\\url{https://github.com/TrustMLRG/SNODEP}.\n","authors":["Santanu Rathod","Pietro Lio","Xiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.14426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03546v2","updated":"2024-10-18T12:41:23Z","published":"2023-10-05T13:57:53Z","title":"Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior\n  Models","summary":"  Posterior sampling has been shown to be a powerful Bayesian approach for\nsolving imaging inverse problems. The recent plug-and-play unadjusted Langevin\nalgorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling\nand minimum mean squared error (MMSE) estimation by combining physical\nmeasurement models with deep-learning priors specified using image denoisers.\nHowever, the intricate relationship between the sampling distribution of\nPnP-ULA and the mismatched data-fidelity and denoiser has not been\ntheoretically analyzed. We address this gap by proposing a posterior-L2\npseudometric and using it to quantify an explicit error bound for PnP-ULA under\nmismatched posterior distribution. We numerically validate our theory on\nseveral inverse problems such as sampling from Gaussian mixture models and\nimage deblurring. Our results suggest that the sensitivity of the sampling\ndistribution of PnP-ULA to a mismatch in the measurement model and the denoiser\ncan be precisely characterized.\n","authors":["Marien Renaud","Jiaming Liu","Valentin de Bortoli","Andrés Almansa","Ulugbek S. Kamilov"],"pdf_url":"https://arxiv.org/pdf/2310.03546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14423v1","updated":"2024-10-18T12:37:51Z","published":"2024-10-18T12:37:51Z","title":"Integrating Deep Learning with Fundus and Optical Coherence Tomography\n  for Cardiovascular Disease Prediction","summary":"  Early identification of patients at risk of cardiovascular diseases (CVD) is\ncrucial for effective preventive care, reducing healthcare burden, and\nimproving patients' quality of life. This study demonstrates the potential of\nretinal optical coherence tomography (OCT) imaging combined with fundus\nphotographs for identifying future adverse cardiac events. We used data from\n977 patients who experienced CVD within a 5-year interval post-image\nacquisition, alongside 1,877 control participants without CVD, totaling 2,854\nsubjects. We propose a novel binary classification network based on a\nMulti-channel Variational Autoencoder (MCVAE), which learns a latent embedding\nof patients' fundus and OCT images to classify individuals into two groups:\nthose likely to develop CVD in the future and those who are not. Our model,\ntrained on both imaging modalities, achieved promising results (AUROC 0.78 +/-\n0.02, accuracy 0.68 +/- 0.002, precision 0.74 +/- 0.02, sensitivity 0.73 +/-\n0.02, and specificity 0.68 +/- 0.01), demonstrating its efficacy in identifying\npatients at risk of future CVD events based on their retinal images. This study\nhighlights the potential of retinal OCT imaging and fundus photographs as\ncost-effective, non-invasive alternatives for predicting cardiovascular disease\nrisk. The widespread availability of these imaging techniques in optometry\npractices and hospitals further enhances their potential for large-scale CVD\nrisk screening. Our findings contribute to the development of standardized,\naccessible methods for early CVD risk identification, potentially improving\npreventive care strategies and patient outcomes.\n","authors":["Cynthia Maldonado-Garcia","Arezoo Zakeri","Alejandro F Frangi","Nishant Ravikumar"],"pdf_url":"https://arxiv.org/pdf/2410.14423v1.pdf","comment":"Part of the book series: Lecture Notes in Computer Science\n  ((LNCS,volume 15155))"},{"id":"http://arxiv.org/abs/2410.14420v1","updated":"2024-10-18T12:33:10Z","published":"2024-10-18T12:33:10Z","title":"Asymptotic non-linear shrinkage formulas for weighted sample covariance","summary":"  We compute asymptotic non-linear shrinkage formulas for covariance and\nprecision matrix estimators for weighted sample covariances, in the spirit of\nLedoit and P\\'ech\\'e. We detail explicitly the formulas for\nexponentially-weighted sample covariances. Those new tools pave a way for\napplying non-linear shrinkage methods on weighted sample covariance. We show\nexperimentally the performance of the asymptotic shrinkage formulas. Finally,\nwe test the robustness of the theory to a heavy-tailed distributions.\n","authors":["Benoit Oriol"],"pdf_url":"https://arxiv.org/pdf/2410.14420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14416v1","updated":"2024-10-18T12:29:10Z","published":"2024-10-18T12:29:10Z","title":"An explainable machine learning approach for energy forecasting at the\n  household level","summary":"  Electricity forecasting has been a recurring research topic, as it is key to\nfinding the right balance between production and consumption. While most papers\nare focused on the national or regional scale, few are interested in the\nhousehold level. Desegregated forecast is a common topic in Machine Learning\n(ML) literature but lacks explainability that household energy forecasts\nrequire. This paper specifically targets the challenges of forecasting\nelectricity use at the household level. This paper confronts common Machine\nLearning algorithms to electricity household forecasts, weighing the pros and\ncons, including accuracy and explainability with well-known key metrics.\nFurthermore, we also confront them in this paper with the business challenges\nspecific to this sector such as explainability or outliers resistance. We\nintroduce a custom decision tree, aiming at providing a fair estimate of the\nenergy consumption, while being explainable and consistent with human\nintuition. We show that this novel method allows greater explainability without\nsacrificing much accuracy. The custom tree methodology can be used in various\nbusiness use cases but is subject to limitations, such as a lack of resilience\nwith outliers.\n","authors":["Pauline Béraud","Margaux Rioux","Michel Babany","Philippe de La Chevasnerie","Damien Theis","Giacomo Teodori","Chloé Pinguet","Romane Rigaud","François Leclerc"],"pdf_url":"https://arxiv.org/pdf/2410.14416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10918v5","updated":"2024-10-18T12:27:07Z","published":"2024-06-16T12:46:40Z","title":"Multi-LLM QA with Embodied Exploration","summary":"  Large language models (LLMs) have grown in popularity due to their natural\nlanguage interface and pre trained knowledge, leading to rapidly increasing\nsuccess in question-answering (QA) tasks. More recently, multi-agent systems\nwith LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.\nIn these scenarios, the models may each answer the question and reach a\nconsensus or each model is specialized to answer different domain questions.\nHowever, most prior work dealing with Multi-LLM QA has focused on scenarios\nwhere the models are asked in a zero-shot manner or are given information\nsources to extract the answer. For question answering of an unknown\nenvironment, embodied exploration of the environment is first needed to answer\nthe question. This skill is necessary for personalizing embodied AI to\nenvironments such as households. There is a lack of insight into whether a\nMulti-LLM system can handle question-answering based on observations from\nembodied exploration. In this work, we address this gap by investigating the\nuse of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.\nMultiple LLM-based agents independently explore and then answer queries about a\nhousehold environment. We analyze different aggregation methods to generate a\nsingle, final answer for each query: debating, majority voting, and training a\ncentral answer module (CAM). Using CAM, we observe a $46\\%$ higher accuracy\ncompared against the other non-learning-based aggregation methods. We provide\ncode and the query dataset for further research.\n","authors":["Bhrij Patel","Vishnu Sashank Dorbala","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2406.10918v5.pdf","comment":"16 pages, 9 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2410.14413v1","updated":"2024-10-18T12:26:51Z","published":"2024-10-18T12:26:51Z","title":"WeSpeR: Population spectrum retrieval and spectral density estimation of\n  weighted sample covariance","summary":"  The spectrum of the weighted sample covariance shows a asymptotic non random\nbehavior when the dimension grows with the number of samples. In this setting,\nwe prove that the asymptotic spectral distribution $F$ of the weighted sample\ncovariance has a continuous density on $\\mathbb{R}^*$. We address then the\npractical problem of numerically finding this density. We propose a procedure\nto compute it, to determine the support of $F$ and define an efficient grid on\nit. We use this procedure to design the $\\textit{WeSpeR}$ algorithm, which\nestimates the spectral density and retrieves the true spectral covariance\nspectrum. Empirical tests confirm the good properties of the $\\textit{WeSpeR}$\nalgorithm.\n","authors":["Benoit Oriol"],"pdf_url":"https://arxiv.org/pdf/2410.14413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10547v2","updated":"2024-10-18T12:25:46Z","published":"2024-07-15T08:57:02Z","title":"Learning Social Cost Functions for Human-Aware Path Planning","summary":"  Achieving social acceptance is one of the main goals of Social Robotic\nNavigation. Despite this topic has received increasing interest in recent\nyears, most of the research has focused on driving the robotic agent along\nobstacle-free trajectories, planning around estimates of future human motion to\nrespect personal distances and optimize navigation. However, social\ninteractions in everyday life are also dictated by norms that do not strictly\ndepend on movement, such as when standing at the end of a queue rather than\ncutting it. In this paper, we propose a novel method to recognize common social\nscenarios and modify a traditional planner's cost function to adapt to them.\nThis solution enables the robot to carry out different social navigation\nbehaviors that would not arise otherwise, maintaining the robustness of\ntraditional navigation. Our approach allows the robot to learn different social\nnorms with a single learned model, rather than having different modules for\neach task. As a proof of concept, we consider the tasks of queuing and respect\ninteraction spaces of groups of people talking to one another, but the method\ncan be extended to other human activities that do not involve motion.\n","authors":["Andrea Eirale","Matteo Leonetti","Marcello Chiaberge"],"pdf_url":"https://arxiv.org/pdf/2407.10547v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14411v1","updated":"2024-10-18T12:24:05Z","published":"2024-10-18T12:24:05Z","title":"SNAC: Multi-Scale Neural Audio Codec","summary":"  Neural audio codecs have recently gained popularity because they can\nrepresent audio signals with high fidelity at very low bitrates, making it\nfeasible to use language modeling approaches for audio generation and\nunderstanding. Residual Vector Quantization (RVQ) has become the standard\ntechnique for neural audio compression using a cascade of VQ codebooks. This\npaper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ\nwhere the quantizers can operate at different temporal resolutions. By applying\na hierarchy of quantizers at variable frame rates, the codec adapts to the\naudio structure across multiple timescales. This leads to more efficient\ncompression, as demonstrated by extensive objective and subjective evaluations.\nThe code and model weights are open-sourced at\nhttps://github.com/hubertsiuzdak/snac.\n","authors":["Hubert Siuzdak","Florian Grötschla","Luca A. Lanzendörfer"],"pdf_url":"https://arxiv.org/pdf/2410.14411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.14437v3","updated":"2024-10-18T12:20:54Z","published":"2022-12-29T19:21:33Z","title":"An algorithm for clustering with confidence-based must-link and\n  cannot-link constraints","summary":"  We study here the semi-supervised $k$-clustering problem where information is\navailable on whether pairs of objects are in the same or in different clusters.\nThis information is either available with certainty or with a limited level of\nconfidence. We introduce the PCCC (Pairwise-Confidence-Constraints-Clustering)\nalgorithm, which iteratively assigns objects to clusters while accounting for\nthe information provided on the pairs of objects. Our algorithm uses integer\nprogramming for the assignment of objects which allows to include relationships\nas hard constraints that are guaranteed to be satisfied or as soft constraints\nthat can be violated subject to a penalty. This flexibility distinguishes our\nalgorithm from the state-of-the-art in which all pairwise constraints are\neither considered hard, or all are considered soft. We developed an enhanced\nmulti-start approach and a model-size reduction technique for the integer\nprogram that contributes to the effectiveness and the efficiency of the\nalgorithm. Unlike existing algorithms, our algorithm scales to large-scale\ninstances with up to 60,000 objects, 100 clusters, and millions of cannot-link\nconstraints (which are the most challenging constraints to incorporate). We\ncompare the PCCC algorithm with state-of-the-art approaches in an extensive\ncomputational study. Even though the PCCC algorithm is more general than the\nstate-of-the-art approaches in its applicability, it outperforms the\nstate-of-the-art approaches on instances with all hard or all soft constraints\nboth in terms of runtime and various metrics of solution quality. The code of\nthe PCCC algorithm is publicly available on GitHub.\n","authors":["Philipp Baumann","Dorit S. Hochbaum"],"pdf_url":"https://arxiv.org/pdf/2212.14437v3.pdf","comment":"To appear in INFORMS Journal on Computing"},{"id":"http://arxiv.org/abs/2405.16504v2","updated":"2024-10-18T12:20:11Z","published":"2024-05-26T09:57:45Z","title":"Explaining Modern Gated-Linear RNNs via a Unified Implicit Attention\n  Formulation","summary":"  Recent advances in efficient sequence modeling have led to attention-free\nlayers, such as Mamba, RWKV, and various gated RNNs, all featuring\nsub-quadratic complexity in sequence length and excellent scaling properties,\nenabling the construction of a new type of foundation models. In this paper, we\npresent a unified view of these models, formulating such layers as implicit\ncausal self-attention layers. The formulation includes most of their\nsub-components and is not limited to a specific part of the architecture. The\nframework compares the underlying mechanisms on similar grounds for different\nlayers and provides a direct means for applying explainability methods. Our\nexperiments show that our attention matrices and attribution method outperform\nan alternative and a more limited formulation that was recently proposed for\nMamba. For the other architectures for which our method is the first to provide\nsuch a view, our method is effective and competitive in the relevant metrics\ncompared to the results obtained by state-of-the-art Transformer explainability\nmethods. Our code is publicly available.\n","authors":["Itamar Zimerman","Ameen Ali","Lior Wolf"],"pdf_url":"https://arxiv.org/pdf/2405.16504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12950v2","updated":"2024-10-18T12:19:41Z","published":"2024-06-18T12:54:47Z","title":"MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular\n  Property Prediction","summary":"  Molecular property prediction (MPP) is a fundamental and crucial task in drug\ndiscovery. However, prior methods are limited by the requirement for a large\nnumber of labeled molecules and their restricted ability to generalize for\nunseen and new tasks, both of which are essential for real-world applications.\nTo address these challenges, we present MolecularGPT for few-shot MPP. From a\nperspective on instruction tuning, we fine-tune large language models (LLMs)\nbased on curated molecular instructions spanning over 1000 property prediction\ntasks. This enables building a versatile and specialized LLM that can be\nadapted to novel MPP tasks without any fine-tuning through zero- and few-shot\nin-context learning (ICL). MolecularGPT exhibits competitive in-context\nreasoning capabilities across 10 downstream evaluation datasets, setting new\nbenchmarks for few-shot molecular prediction tasks. More importantly, with just\ntwo-shot examples, MolecularGPT can outperform standard supervised graph neural\nnetwork methods on 4 out of 7 datasets. It also excels state-of-the-art LLM\nbaselines by up to 15.7% increase on classification accuracy and decrease of\n17.9 on regression metrics (e.g., RMSE) under zero-shot. This study\ndemonstrates the potential of LLMs as effective few-shot molecular property\npredictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.\n","authors":["Yuyan Liu","Sirui Ding","Sheng Zhou","Wenqi Fan","Qiaoyu Tan"],"pdf_url":"https://arxiv.org/pdf/2406.12950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09567v2","updated":"2024-10-18T12:18:01Z","published":"2024-10-12T15:29:18Z","title":"Timeseria: an object-oriented time series processing library","summary":"  Timeseria is an object-oriented time series processing library implemented in\nPython, which aims at making it easier to manipulate time series data and to\nbuild statistical and machine learning models on top of it. Unlike common data\nanalysis frameworks, it builds up from well defined and reusable logical units\n(objects), which can be easily combined together in order to ensure a high\nlevel of consistency. Thanks to this approach, Timeseria can address by design\nseveral non-trivial issues often underestimated, such as handling data losses,\nnon-uniform sampling rates, differences between aggregated data and punctual\nobservations, time zones, daylight saving times, and more. Timeseria comes with\na comprehensive set of base data structures, common data manipulation\noperations, and extensible models for data reconstruction, forecasting and\nanomaly detection. It also integrates a powerful plotting engine capable of\nhandling even millions of data points.\n","authors":["Stefano Alberto Russo","Giuliano Taffoni","Luca Bortolussi"],"pdf_url":"https://arxiv.org/pdf/2410.09567v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14393v1","updated":"2024-10-18T11:55:34Z","published":"2024-10-18T11:55:34Z","title":"Debug Smarter, Not Harder: AI Agents for Error Resolution in\n  Computational Notebooks","summary":"  Computational notebooks became indispensable tools for research-related\ndevelopment, offering unprecedented interactivity and flexibility in the\ndevelopment process. However, these benefits come at the cost of\nreproducibility and an increased potential for bugs. With the rise of\ncode-fluent Large Language Models empowered with agentic techniques, smart\nbug-fixing tools with a high level of autonomy have emerged. However, those\ntools are tuned for classical script programming and still struggle with\nnon-linear computational notebooks. In this paper, we present an AI agent\ndesigned specifically for error resolution in a computational notebook. We have\ndeveloped an agentic system capable of exploring a notebook environment by\ninteracting with it -- similar to how a user would -- and integrated the system\ninto the JetBrains service for collaborative data science called Datalore. We\nevaluate our approach against the pre-existing single-action solution by\ncomparing costs and conducting a user study. Users rate the error resolution\ncapabilities of the agentic system higher but experience difficulties with UI.\nWe share the results of the study and consider them valuable for further\nimproving user-agent collaboration.\n","authors":["Konstantin Grotov","Artem Borzilov","Maksim Krivobok","Timofey Bryksin","Yaroslav Zharov"],"pdf_url":"https://arxiv.org/pdf/2410.14393v1.pdf","comment":"Accepted to EMNLP 2024 System Demonstrations"},{"id":"http://arxiv.org/abs/2410.14390v1","updated":"2024-10-18T11:50:54Z","published":"2024-10-18T11:50:54Z","title":"Personalizing Low-Rank Bayesian Neural Networks Via Federated Learning","summary":"  To support real-world decision-making, it is crucial for models to be\nwell-calibrated, i.e., to assign reliable confidence estimates to their\npredictions. Uncertainty quantification is particularly important in\npersonalized federated learning (PFL), as participating clients typically have\nsmall local datasets, making it difficult to unambiguously determine optimal\nmodel parameters. Bayesian PFL (BPFL) methods can potentially enhance\ncalibration, but they often come with considerable computational and memory\nrequirements due to the need to track the variances of all the individual model\nparameters. Furthermore, different clients may exhibit heterogeneous\nuncertainty levels owing to varying local dataset sizes and distributions. To\naddress these challenges, we propose LR-BPFL, a novel BPFL method that learns a\nglobal deterministic model along with personalized low-rank Bayesian\ncorrections. To tailor the local model to each client's inherent uncertainty\nlevel, LR-BPFL incorporates an adaptive rank selection mechanism. We evaluate\nLR-BPFL across a variety of datasets, demonstrating its advantages in terms of\ncalibration, accuracy, as well as computational and memory requirements.\n","authors":["Boning Zhang","Dongzhu Liu","Osvaldo Simeone","Guanchu Wang","Dimitrios Pezaros","Guangxu Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.14390v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14389v1","updated":"2024-10-18T11:49:40Z","published":"2024-10-18T11:49:40Z","title":"SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task\n  Learning with Deep Representation Surgery","summary":"  Model merging-based multitask learning (MTL) offers a promising approach for\nperforming MTL by merging multiple expert models without requiring access to\nraw training data. However, in this paper, we examine the merged model's\nrepresentation distribution and uncover a critical issue of \"representation\nbias\". This bias arises from a significant distribution gap between the\nrepresentations of the merged and expert models, leading to the suboptimal\nperformance of the merged MTL model. To address this challenge, we first\npropose a representation surgery solution called Surgery. Surgery is a\nlightweight, task-specific module that aligns the final layer representations\nof the merged model with those of the expert models, effectively alleviating\nbias and improving the merged model's performance. Despite these improvements,\na performance gap remains compared to the traditional MTL method. Further\nanalysis reveals that representation bias phenomena exist at each layer of the\nmerged model, and aligning representations only in the last layer is\ninsufficient for fully reducing systemic bias because biases introduced at each\nlayer can accumulate and interact in complex ways. To tackle this, we then\npropose a more comprehensive solution, deep representation surgery (also called\nSurgeryV2), which mitigates representation bias across all layers, and thus\nbridges the performance gap between model merging-based MTL and traditional\nMTL. Finally, we design an unsupervised optimization objective to optimize both\nthe Surgery and SurgeryV2 modules. Our experimental results show that\nincorporating these modules into state-of-the-art (SOTA) model merging schemes\nleads to significant performance gains. Notably, our SurgeryV2 scheme reaches\nalmost the same level as individual expert models or the traditional MTL model.\nThe code is available at \\url{https://github.com/EnnengYang/SurgeryV2}.\n","authors":["Enneng Yang","Li Shen","Zhenyi Wang","Guibing Guo","Xingwei Wang","Xiaocun Cao","Jie Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2410.14389v1.pdf","comment":"This paper is an extended version of our previous work\n  [arXiv:2402.02705] presented at ICML 2024"},{"id":"http://arxiv.org/abs/2410.06927v2","updated":"2024-10-18T11:47:40Z","published":"2024-10-09T14:21:59Z","title":"Spectral and Rhythm Features for Audio Classification with Deep\n  Convolutional Neural Networks","summary":"  Convolutional neural networks (CNNs) are widely used in computer vision. They\ncan be used not only for conventional digital image material to recognize\npatterns, but also for feature extraction from digital imagery representing\nspectral and rhythm features extracted from time-domain digital audio signals\nfor the acoustic classification of sounds. Different spectral and rhythm\nfeature representations like mel-scaled spectrograms, mel-frequency cepstral\ncoefficients (MFCCs), cyclic tempograms, short-time Fourier transform (STFT)\nchromagrams, constant-Q transform (CQT) chromagrams and chroma energy\nnormalized statistics (CENS) chromagrams are investigated in terms of the audio\nclassification performance using a deep convolutional neural network. It can be\nclearly shown that the mel-scaled spectrograms and the mel-frequency cepstral\ncoefficients (MFCCs) perform significantly better than the other spectral and\nrhythm features investigated in this research for audio classification tasks\nusing deep CNNs. The experiments were carried out with the aid of the ESC-50\ndataset with 2,000 labeled environmental audio recordings.\n","authors":["Friedrich Wolf-Monheim"],"pdf_url":"https://arxiv.org/pdf/2410.06927v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14388v1","updated":"2024-10-18T11:44:29Z","published":"2024-10-18T11:44:29Z","title":"Unscrambling disease progression at scale: fast inference of event\n  permutations with optimal transport","summary":"  Disease progression models infer group-level temporal trajectories of change\nin patients' features as a chronic degenerative condition plays out. They\nprovide unique insight into disease biology and staging systems with\nindividual-level clinical utility. Discrete models consider disease progression\nas a latent permutation of events, where each event corresponds to a feature\nbecoming measurably abnormal. However, permutation inference using traditional\nmaximum likelihood approaches becomes prohibitive due to combinatoric\nexplosion, severely limiting model dimensionality and utility. Here we leverage\nideas from optimal transport to model disease progression as a latent\npermutation matrix of events belonging to the Birkhoff polytope, facilitating\nfast inference via optimisation of the variational lower bound. This enables a\nfactor of 1000 times faster inference than the current state of the art and,\ncorrespondingly, supports models with several orders of magnitude more features\nthan the current state of the art can consider. Experiments demonstrate the\nincrease in speed, accuracy and robustness to noise in simulation. Further\nexperiments with real-world imaging data from two separate datasets, one from\nAlzheimer's disease patients, the other age-related macular degeneration,\nshowcase, for the first time, pixel-level disease progression events in the\nbrain and eye, respectively. Our method is low compute, interpretable and\napplicable to any progressive condition and data modality, giving it broad\npotential clinical utility.\n","authors":["Peter A. Wijeratne","Daniel C. Alexander"],"pdf_url":"https://arxiv.org/pdf/2410.14388v1.pdf","comment":"Pre-print of version accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2407.13625v2","updated":"2024-10-18T11:43:28Z","published":"2024-07-18T15:59:37Z","title":"Distributionally and Adversarially Robust Logistic Regression via\n  Intersecting Wasserstein Balls","summary":"  Adversarially robust optimization (ARO) has become the de facto standard for\ntraining models to defend against adversarial attacks during testing. However,\ndespite their robustness, these models often suffer from severe overfitting. To\nmitigate this issue, several successful approaches have been proposed,\nincluding replacing the empirical distribution in training with: (i) a\nworst-case distribution within an ambiguity set, leading to a distributionally\nrobust (DR) counterpart of ARO; or (ii) a mixture of the empirical distribution\nwith one derived from an auxiliary dataset (e.g., synthetic, external, or\nout-of-domain). Building on the first approach, we explore the Wasserstein DR\ncounterpart of ARO for logistic regression and show it admits a tractable\nconvex optimization reformulation. Adopting the second approach, we enhance the\nDR framework by intersecting its ambiguity set with one constructed from an\nauxiliary dataset, which yields significant improvements when the Wasserstein\ndistance between the data-generating and auxiliary distributions can be\nestimated. We analyze the resulting optimization problem, develop efficient\nsolutions, and show that our method outperforms benchmark approaches on\nstandard datasets.\n","authors":["Aras Selvi","Eleonora Kreacic","Mohsen Ghassemi","Vamsi Potluru","Tucker Balch","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2407.13625v2.pdf","comment":"33 pages, 3 color figures, under review at a conference"},{"id":"http://arxiv.org/abs/2410.14386v1","updated":"2024-10-18T11:38:29Z","published":"2024-10-18T11:38:29Z","title":"Investigating the Capabilities of Deep Learning for Processing and\n  Interpreting One-Shot Multi-offset GPR Data: A Numerical Case Study for Lunar\n  and Martian Environments","summary":"  Ground-penetrating radar (GPR) is a mature geophysical method that has gained\nincreasing popularity in planetary science over the past decade. GPR has been\nutilised both for Lunar and Martian missions providing pivotal information\nregarding the near surface geology of Terrestrial planets. Within that context,\nnumerous processing pipelines have been suggested to address the unique\nchallenges present in planetary setups. These processing pipelines often\nrequire manual tuning resulting to ambiguous outputs open to non-unique\ninterpretations. These pitfalls combined with the large number of planetary GPR\ndata (kilometers in magnitude), highlight the necessity for automatic,\nobjective and advanced processing and interpretation schemes. The current paper\ninvestigates the potential of deep learning for interpreting and processing GPR\ndata. The one-shot multi-offset configuration is investigated via a coherent\nnumerical case study, showcasing the potential of deep learning for A)\nreconstructing the dielectric distribution of the the near surface of\nTerrestrial planets, and B) filling missing or bad-quality traces. Special care\nwas taken for the numerical data to be both realistic and challenging.\nMoreover, the generated synthetic data are properly labelled and made publicly\navailable for training future data-driven pipelines and contributing towards\ndeveloping pre-trained foundation models for GPR.\n","authors":["Iraklis Giannakis","Craig Warren","Antonios Giannopoulos","Georgios Leontidis","Yan Su","Feng Zhou","Javier Martin-Torres","Nectaria Diamanti"],"pdf_url":"https://arxiv.org/pdf/2410.14386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14659v2","updated":"2024-10-18T11:32:20Z","published":"2023-10-23T07:53:47Z","title":"Predicting Accurate Lagrangian Multipliers for Mixed Integer Linear\n  Programs","summary":"  Lagrangian relaxation stands among the most efficient approaches for solving\na Mixed Integer Linear Programs (MILP) with difficult constraints. Given any\nduals for these constraints, called Lagrangian Multipliers (LMs), it returns a\nbound on the optimal value of the MILP, and Lagrangian methods seek the LMs\ngiving the best such bound. But these methods generally rely on iterative\nalgorithms resembling gradient descent to maximize the concave piecewise linear\ndual function: the computational burden grows quickly with the number of\nrelaxed constraints. We introduce a deep learning approach that bypasses the\ndescent, effectively amortizing the local, per instance, optimization. A\nprobabilistic encoder based on a graph convolutional network computes\nhigh-dimensional representations of relaxed constraints in MILP instances. A\ndecoder then turns these representations into LMs. We train the encoder and\ndecoder jointly by directly optimizing the bound obtained from the predicted\nmultipliers. Numerical experiments show that our approach closes up to 85~\\% of\nthe gap between the continuous relaxation and the best Lagrangian bound, and\nprovides a high quality warm-start for descent based Lagrangian methods.\n","authors":["Francesco Demelas","Joseph Le Roux","Mathieu Lacroix","Axel Parmentier"],"pdf_url":"https://arxiv.org/pdf/2310.14659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09388v2","updated":"2024-10-18T11:23:18Z","published":"2024-10-12T06:39:31Z","title":"3-D Magnetotelluric Deep Learning Inversion Guided by Pseudo-Physical\n  Information","summary":"  Magnetotelluric deep learning (DL) inversion methods based on joint\ndata-driven and physics-driven have become a hot topic in recent years. When\nmapping observation data (or forward modeling data) to the resistivity model\nusing neural networks (NNs), incorporating the error (loss) term of the\ninversion resistivity's forward modeling response--which introduces physical\ninformation about electromagnetic field propagation--can significantly enhance\nthe inversion accuracy. To efficiently achieve data-physical dual-driven MT\ndeep learning inversion for large-scale 3-D MT data, we propose using DL\nforward modeling networks to compute this portion of the loss. This approach\nintroduces pseudo-physical information through the forward modeling of NN\nsimulation, further guiding the inversion network fitting. Specifically, we\nfirst pre-train the forward modeling networks as fixed forward modeling\noperators, then transfer and integrate them into the inversion network\ntraining, and finally optimize the inversion network by minimizing the\nmultinomial loss. Theoretical experimental results indicate that despite some\nsimulation errors in DL forward modeling, the introduced pseudo-physical\ninformation still enhances inversion accuracy and significantly mitigates the\noverfitting problem during training. Additionally, we propose a new input mode\nthat involves masking and adding noise to the data, simulating the field data\nenvironment of 3-D MT inversion, thereby making the method more flexible and\neffective for practical applications.\n","authors":["Peifan Jiang","Xuben Wang","Shuang Wang","Fei Deng","Kunpeng Wang","Bin Wang","Yuhan Yang","Islam Fadel"],"pdf_url":"https://arxiv.org/pdf/2410.09388v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14380v1","updated":"2024-10-18T11:07:26Z","published":"2024-10-18T11:07:26Z","title":"Dual-Label LearningWith Irregularly Present Labels","summary":"  In multi-task learning, we often encounter the case when the presence of\nlabels across samples exhibits irregular patterns: samples can be fully\nlabeled, partially labeled or unlabeled. Taking drug analysis as an example,\nmultiple toxicity properties of a drug molecule may not be concurrently\navailable due to experimental limitations. It triggers a demand for a new\ntraining and inference mechanism that could accommodate irregularly present\nlabels and maximize the utility of any available label information. In this\nwork, we focus on the two-label learning task, and propose a novel training and\ninference framework, Dual-Label Learning (DLL). The DLL framework formulates\nthe problem into a dual-function system, in which the two functions should\nsimultaneously satisfy standard supervision, structural duality and\nprobabilistic duality. DLL features a dual-tower model architecture that\nexplicitly captures the information exchange between labels, aimed at\nmaximizing the utility of partially available labels in understanding label\ncorrelation. During training, label imputation for missing labels is conducted\nas part of the forward propagation process, while during inference, labels are\nregarded as unknowns of a bivariate system of equations and are solved jointly.\nTheoretical analysis guarantees the feasibility of DLL, and extensive\nexperiments are conducted to verify that by explicitly modeling label\ncorrelation and maximizing the utility of available labels, our method makes\nconsistently better predictions than baseline approaches by up to a 10% gain in\nF1-score or MAPE. Remarkably, our method provided with data at a label missing\nrate as high as 60% can achieve similar or even better results than baseline\napproaches at a label missing rate of only 10%.\n","authors":["Mingqian Li","Qiao Han","Yiteng Zhai","Ruifeng Li","Yao Yang","Hongyang Chen"],"pdf_url":"https://arxiv.org/pdf/2410.14380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14375v1","updated":"2024-10-18T11:06:23Z","published":"2024-10-18T11:06:23Z","title":"Fine-Tuning Pre-trained Language Models for Robust Causal Representation\n  Learning","summary":"  The fine-tuning of pre-trained language models (PLMs) has been shown to be\neffective across various domains. By using domain-specific supervised data, the\ngeneral-purpose representation derived from PLMs can be transformed into a\ndomain-specific representation. However, these methods often fail to generalize\nto out-of-domain (OOD) data due to their reliance on non-causal\nrepresentations, often described as spurious features. Existing methods either\nmake use of adjustments with strong assumptions about lack of hidden common\ncauses, or mitigate the effect of spurious features using multi-domain data. In\nthis work, we investigate how fine-tuned pre-trained language models aid\ngeneralizability from single-domain scenarios under mild assumptions, targeting\nmore general and practical real-world scenarios. We show that a robust\nrepresentation can be derived through a so-called causal front-door adjustment,\nbased on a decomposition assumption, using fine-tuned representations as a\nsource of data augmentation. Comprehensive experiments in both synthetic and\nreal-world settings demonstrate the superior generalizability of the proposed\nmethod compared to existing approaches. Our work thus sheds light on the domain\ngeneralization problem by introducing links between fine-tuning and causal\nmechanisms into representation learning.\n","authors":["Jialin Yu","Yuxiang Zhou","Yulan He","Nevin L. Zhang","Ricardo Silva"],"pdf_url":"https://arxiv.org/pdf/2410.14375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01129v4","updated":"2024-10-18T10:46:43Z","published":"2024-08-02T09:18:41Z","title":"A Survey of Mamba","summary":"  As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations.\n","authors":["Haohao Qu","Liangbo Ning","Rui An","Wenqi Fan","Tyler Derr","Hui Liu","Xin Xu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2408.01129v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13458v5","updated":"2024-10-18T10:31:27Z","published":"2023-03-23T17:26:12Z","title":"Optimization Dynamics of Equivariant and Augmented Neural Networks","summary":"  We investigate the optimization of neural networks on symmetric data, and\ncompare the strategy of constraining the architecture to be equivariant to that\nof using data augmentation. Our analysis reveals that that the relative\ngeometry of the admissible and the equivariant layers, respectively, plays a\nkey role. Under natural assumptions on the data, network, loss, and group of\nsymmetries, we show that compatibility of the spaces of admissible layers and\nequivariant layers, in the sense that the corresponding orthogonal projections\ncommute, implies that the sets of equivariant stationary points are identical\nfor the two strategies. If the linear layers of the network also are given a\nunitary parametrization, the set of equivariant layers is even invariant under\nthe gradient flow for augmented models. Our analysis however also reveals that\neven in the latter situation, stationary points may be unstable for augmented\ntraining although they are stable for the manifestly equivariant models.\n","authors":["Oskar Nordenfors","Fredrik Ohlsson","Axel Flinth"],"pdf_url":"https://arxiv.org/pdf/2303.13458v5.pdf","comment":"v4: Some discussions added, along with an updated experiment section.\n  v3: Completely revised manuscript: New framework for neural nets, new main\n  result (involving compability condition), new experiments, new author. v2:\n  Revised manuscript. Mostly small edits, apart from new experiments (see\n  Appendix E)"},{"id":"http://arxiv.org/abs/2310.11244v4","updated":"2024-10-18T10:21:31Z","published":"2023-10-17T13:12:32Z","title":"Entity Matching using Large Language Models","summary":"  Entity matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity. Entity matching is a central step in most data\nintegration pipelines. Many state-of-the-art entity matching methods rely on\npre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks\nof these models for entity matching are that (i) the models require significant\namounts of task-specific training data and (ii) the fine-tuned models are not\nrobust concerning out-of-distribution entities. This paper investigates using\ngenerative large language models (LLMs) as a less task-specific training\ndata-dependent and more robust alternative to PLM-based matchers. The study\ncovers hosted and open-source LLMs which can be run locally. We evaluate these\nmodels in a zero-shot scenario and a scenario where task-specific training data\nis available. We compare different prompt designs and the prompt sensitivity of\nthe models. We show that there is no single best prompt but that the prompt\nneeds to be tuned for each model/dataset combination. We further investigate\n(i) the selection of in-context demonstrations, (ii) the generation of matching\nrules, as well as (iii) fine-tuning LLMs using the same pool of training data.\nOur experiments show that the best LLMs require no or only a few training\nexamples to perform comparably to PLMs that were fine-tuned using thousands of\nexamples. LLM-based matchers further exhibit higher robustness to unseen\nentities. We show that GPT4 can generate structured explanations for matching\ndecisions and can automatically identify potential causes of matching errors by\nanalyzing explanations of wrong decisions. We demonstrate that the model can\ngenerate meaningful textual descriptions of the identified error classes, which\ncan help data engineers to improve entity matching pipelines.\n","authors":["Ralph Peeters","Aaron Steiner","Christian Bizer"],"pdf_url":"https://arxiv.org/pdf/2310.11244v4.pdf","comment":"Published in Proceedings of the 28th International Conference on\n  Extending Database Technology (EDBT), 25th March-28th March, 2025, ISBN\n  978-3-89318-098-1 on OpenProceedings.org"},{"id":"http://arxiv.org/abs/2203.08975v2","updated":"2024-10-18T10:14:58Z","published":"2022-03-16T22:39:46Z","title":"A Survey of Multi-Agent Deep Reinforcement Learning with Communication","summary":"  Communication is an effective mechanism for coordinating the behaviors of\nmultiple agents, broadening their views of the environment, and to support\ntheir collaborations. In the field of multi-agent deep reinforcement learning\n(MADRL), agents can improve the overall learning performance and achieve their\nobjectives by communication. Agents can communicate various types of messages,\neither to all agents or to specific agent groups, or conditioned on specific\nconstraints. With the growing body of research work in MADRL with communication\n(Comm-MADRL), there is a lack of a systematic and structural approach to\ndistinguish and classify existing Comm-MADRL approaches. In this paper, we\nsurvey recent works in the Comm-MADRL field and consider various aspects of\ncommunication that can play a role in designing and developing multi-agent\nreinforcement learning systems. With these aspects in mind, we propose 9\ndimensions along which Comm-MADRL approaches can be analyzed, developed, and\ncompared. By projecting existing works into the multi-dimensional space, we\ndiscover interesting trends. We also propose some novel directions for\ndesigning future Comm-MADRL systems through exploring possible combinations of\nthe dimensions.\n","authors":["Changxi Zhu","Mehdi Dastani","Shihan Wang"],"pdf_url":"https://arxiv.org/pdf/2203.08975v2.pdf","comment":"34 pages, 5 figures, 13 tables; published on Autonomous Agents and\n  Multi-Agent Systems"},{"id":"http://arxiv.org/abs/2410.14347v1","updated":"2024-10-18T09:57:59Z","published":"2024-10-18T09:57:59Z","title":"A Scientific Machine Learning Approach for Predicting and Forecasting\n  Battery Degradation in Electric Vehicles","summary":"  Carbon emissions are rising at an alarming rate, posing a significant threat\nto global efforts to mitigate climate change. Electric vehicles have emerged as\na promising solution, but their reliance on lithium-ion batteries introduces\nthe critical challenge of battery degradation. Accurate prediction and\nforecasting of battery degradation over both short and long time spans are\nessential for optimizing performance, extending battery life, and ensuring\neffective long-term energy management. This directly influences the\nreliability, safety, and sustainability of EVs, supporting their widespread\nadoption and aligning with key UN SDGs. In this paper, we present a novel\napproach to the prediction and long-term forecasting of battery degradation\nusing Scientific Machine Learning framework which integrates domain knowledge\nwith neural networks, offering more interpretable and scientifically grounded\nsolutions for both predicting short-term battery health and forecasting\ndegradation over extended periods. This hybrid approach captures both known and\nunknown degradation dynamics, improving predictive accuracy while reducing data\nrequirements. We incorporate ground-truth data to inform our models, ensuring\nthat both the predictions and forecasts reflect practical conditions. The model\nachieved MSE of 9.90 with the UDE and 11.55 with the NeuralODE, in experimental\ndata, a loss of 1.6986 with the UDE, and a MSE of 2.49 in the NeuralODE,\ndemonstrating the enhanced precision of our approach. This integration of\ndata-driven insights with SciML's strengths in interpretability and scalability\nallows for robust battery management. By enhancing battery longevity and\nminimizing waste, our approach contributes to the sustainability of energy\nsystems and accelerates the global transition toward cleaner, more responsible\nenergy solutions, aligning with the UN's SDG agenda.\n","authors":["Sharv Murgai","Hrishikesh Bhagwat","Raj Abhijit Dandekar","Rajat Dandekar","Sreedath Panat"],"pdf_url":"https://arxiv.org/pdf/2410.14347v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14334v1","updated":"2024-10-18T09:44:35Z","published":"2024-10-18T09:44:35Z","title":"Evaluating the evaluators: Towards human-aligned metrics for missing\n  markers reconstruction","summary":"  Animation data is often obtained through optical motion capture systems,\nwhich utilize a multitude of cameras to establish the position of optical\nmarkers. However, system errors or occlusions can result in missing markers,\nthe manual cleaning of which can be time-consuming. This has sparked interest\nin machine learning-based solutions for missing marker reconstruction in the\nacademic community. Most academic papers utilize a simplistic mean square error\nas the main metric. In this paper, we show that this metric does not correlate\nwith subjective perception of the fill quality. We introduce and evaluate a set\nof better-correlated metrics that can drive progress in the field.\n","authors":["Taras Kucherenko","Derek Peristy","Judith Bütepage"],"pdf_url":"https://arxiv.org/pdf/2410.14334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11677v2","updated":"2024-10-18T09:41:53Z","published":"2024-10-15T15:14:22Z","title":"Understanding Likelihood Over-optimisation in Direct Alignment\n  Algorithms","summary":"  Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation\n(DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives\nto online Reinforcement Learning from Human Feedback (RLHF) algorithms such as\nProximal Policy Optimisation (PPO) for aligning language models to human\npreferences, without the need for explicit reward modelling. These methods\ngenerally aim to increase the likelihood of generating better (preferred)\ncompletions while discouraging worse (non-preferred) ones, while staying close\nto the original model's behaviour. In this work, we explore the relationship\nbetween completion likelihood and model performance in state-of-the-art DAAs,\nand identify a critical issue of likelihood over-optimisation. Contrary to\nexpectations, we find that higher likelihood of better completions and larger\nmargins between better and worse completion likelihoods do not necessarily lead\nto better performance, and may even degrade it. Our analysis reveals that while\nhigher likelihood correlates with better memorisation of factual knowledge\npatterns, a slightly lower completion likelihood tends to improve output\ndiversity, thus leading to better generalisation to unseen scenarios. Moreover,\nwe identify two key indicators that signal when over-optimised output diversity\nbegins to harm performance: Decreasing Entropy over Top-k Tokens and\nDiminishing Top-k Probability Mass. Our experimental results validate that\nthese indicators are reliable signs of declining performance under different\nregularisations, helping prevent over-optimisation and improve alignment with\nhuman preferences.\n","authors":["Zhengyan Shi","Sander Land","Acyr Locatelli","Matthieu Geist","Max Bartolo"],"pdf_url":"https://arxiv.org/pdf/2410.11677v2.pdf","comment":"Preprint Version"},{"id":"http://arxiv.org/abs/2410.14326v1","updated":"2024-10-18T09:37:38Z","published":"2024-10-18T09:37:38Z","title":"Fast proxy centers for Jeffreys centroids: The Jeffreys-Fisher-Rao and\n  the inductive Gauss-Bregman centers","summary":"  The symmetric Kullback-Leibler centroid also called the Jeffreys centroid of\na set of mutually absolutely continuous probability distributions on a measure\nspace provides a notion of centrality which has proven useful in many tasks\nincluding information retrieval, information fusion, and clustering in image,\nvideo and sound processing. However, the Jeffreys centroid is not available in\nclosed-form for sets of categorical or normal distributions, two widely used\nstatistical models, and thus need to be approximated numerically in practice.\nIn this paper, we first propose the new Jeffreys-Fisher-Rao center defined as\nthe Fisher-Rao midpoint of the sided Kullback-Leibler centroids as a plug-in\nreplacement of the Jeffreys centroid. This Jeffreys-Fisher-Rao center admits a\ngeneric formula for uni-parameter exponential family distributions, and\nclosed-form formula for categorical and normal distributions, matches exactly\nthe Jeffreys centroid for same-mean normal distributions, and is experimentally\nobserved in practice to be close to the Jeffreys centroid. Second, we define a\nnew type of inductive centers generalizing the principle of Gauss\narithmetic-geometric double sequence mean for pairs of densities of any given\nexponential family. This center is shown experimentally to approximate very\nwell the Jeffreys centroid and is suggested to use when the Jeffreys-Fisher-Rao\ncenter is not available in closed form. Moreover, this Gauss-Bregman inductive\ncenter always converges and matches the Jeffreys centroid for sets of same-mean\nnormal distributions. We report on our experiments demonstrating the use of the\nJeffreys-Fisher-Rao and Gauss-Bregman centers instead of the Jeffreys centroid.\nFinally, we conclude this work by reinterpreting these fast proxy centers of\nJeffreys centroids under the lens of dually flat spaces in information\ngeometry.\n","authors":["Frank Nielsen"],"pdf_url":"https://arxiv.org/pdf/2410.14326v1.pdf","comment":"35 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.14325v1","updated":"2024-10-18T09:37:05Z","published":"2024-10-18T09:37:05Z","title":"Debiasing Mini-Batch Quadratics for Applications in Deep Learning","summary":"  Quadratic approximations form a fundamental building block of machine\nlearning methods. E.g., second-order optimizers try to find the Newton step\ninto the minimum of a local quadratic proxy to the objective function; and the\nsecond-order approximation of a network's loss function can be used to quantify\nthe uncertainty of its outputs via the Laplace approximation. When computations\non the entire training set are intractable - typical for deep learning - the\nrelevant quantities are computed on mini-batches. This, however, distorts and\nbiases the shape of the associated stochastic quadratic approximations in an\nintricate way with detrimental effects on applications. In this paper, we (i)\nshow that this bias introduces a systematic error, (ii) provide a theoretical\nexplanation for it, (iii) explain its relevance for second-order optimization\nand uncertainty quantification via the Laplace approximation in deep learning,\nand (iv) develop and evaluate debiasing strategies.\n","authors":["Lukas Tatzel","Bálint Mucsányi","Osane Hackel","Philipp Hennig"],"pdf_url":"https://arxiv.org/pdf/2410.14325v1.pdf","comment":"Main text (including references): 13 pages, 6 figures; Supplements:\n  25 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.14315v1","updated":"2024-10-18T09:21:10Z","published":"2024-10-18T09:21:10Z","title":"Optimizing importance weighting in the presence of sub-population shifts","summary":"  A distribution shift between the training and test data can severely harm\nperformance of machine learning models. Importance weighting addresses this\nissue by assigning different weights to data points during training. We argue\nthat existing heuristics for determining the weights are suboptimal, as they\nneglect the increase of the variance of the estimated model due to the finite\nsample size of the training data. We interpret the optimal weights in terms of\na bias-variance trade-off, and propose a bi-level optimization procedure in\nwhich the weights and model parameters are optimized simultaneously. We apply\nthis optimization to existing importance weighting techniques for last-layer\nretraining of deep neural networks in the presence of sub-population shifts and\nshow empirically that optimizing weights significantly improves generalization\nperformance.\n","authors":["Floris Holstege","Bram Wouters","Noud van Giersbergen","Cees Diks"],"pdf_url":"https://arxiv.org/pdf/2410.14315v1.pdf","comment":"Preprint. Currently under review"},{"id":"http://arxiv.org/abs/2405.18921v2","updated":"2024-10-18T09:05:18Z","published":"2024-05-29T09:24:25Z","title":"GLANCE: Global Actions in a Nutshell for Counterfactual Explainability","summary":"  The widespread deployment of machine learning systems in critical real-world\ndecision-making applications has highlighted the urgent need for counterfactual\nexplainability methods that operate effectively. Global counterfactual\nexplanations, expressed as actions to offer recourse, aim to provide succinct\nexplanations and insights applicable to large population subgroups.\nEffectiveness is measured by the fraction of the population that is provided\nrecourse, ensuring that the actions benefit as many individuals as possible.\nKeeping the cost of actions low ensures the proposed recourse actions remain\npractical and actionable. Limiting the number of actions that provide global\ncounterfactuals is essential to maximize interpretability. The primary\nchallenge, therefore, is balancing these trade-offs, i.e., maximizing\neffectiveness, minimizing cost, while maintaining a small number of actions. We\nintroduce GLANCE, a versatile and adaptive framework, comprising two\nalgorithms, that allows the careful balancing of the trade-offs among the three\nkey objectives, with the size objective functioning as a tunable parameter to\nkeep the actions few and easy to interpret. C-GLANCE employs a clustering\napproach that considers both the feature space and the space of counterfactual\nactions, thereby accounting for the distribution of points in a way that aligns\nwith the structure of the model. T-GLANCE provides additional features to\nenhance flexibility. It employs a tree-based approach, that allows users to\nspecify split features, to build a decision tree with a single counterfactual\naction at each node that can be used as a subgroup policy. Our extensive\nexperimental evaluation demonstrates that our method consistently shows greater\nrobustness and performance compared to existing methods across various datasets\nand models.\n","authors":["Loukas Kavouras","Eleni Psaroudaki","Konstantinos Tsopelas","Dimitrios Rontogiannis","Nikolaos Theologitis","Dimitris Sacharidis","Giorgos Giannopoulos","Dimitrios Tomaras","Kleopatra Markou","Dimitrios Gunopulos","Dimitris Fotakis","Ioannis Emiris"],"pdf_url":"https://arxiv.org/pdf/2405.18921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12013v2","updated":"2024-10-18T08:57:30Z","published":"2024-06-26T12:33:34Z","title":"Dating ancient manuscripts using radiocarbon and AI-based writing style\n  analysis","summary":"  Determining the chronology of ancient handwritten manuscripts is essential\nfor reconstructing the evolution of ideas. For the Dead Sea Scrolls, this is\nparticularly important. However, there is an almost complete lack of\ndate-bearing manuscripts evenly distributed across the timeline and written in\nsimilar scripts available for palaeographic comparison. Here, we present Enoch,\na state-of-the-art AI-based date-prediction model, trained on the basis of new\nradiocarbon-dated samples of the scrolls. Enoch uses established\nhandwriting-style descriptors and applies Bayesian ridge regression. The\nchallenge of this study is that the number of radiocarbon-dated manuscripts is\nsmall, while current machine learning requires an abundance of training data.\nWe show that by using combined angular and allographic writing style feature\nvectors and applying Bayesian ridge regression, Enoch could predict the\nradiocarbon-based dates from style, supported by leave-one-out validation, with\nvaried MAEs of 27.9 to 30.7 years relative to the radiocarbon dating. Enoch was\nthen used to estimate the dates of 135 unseen manuscripts, revealing that 79\nper cent of the samples were considered 'realistic' upon palaeographic post-hoc\nevaluation. We present a new chronology of the scrolls. The radiocarbon ranges\nand Enoch's style-based predictions are often older than the traditionally\nassumed palaeographic estimates. In the range of 300-50 BCE, Enoch's date\nprediction provides an improved granularity. The study is in line with current\ndevelopments in multimodal machine-learning techniques, and the methods can be\nused for date prediction in other partially-dated manuscript collections. This\nresearch shows how Enoch's quantitative, probability-based approach can be a\ntool for palaeographers and historians, re-dating ancient Jewish key texts and\ncontributing to current debates on Jewish and Christian origins.\n","authors":["Mladen Popović","Maruf A. Dhali","Lambert Schomaker","Johannes van der Plicht","Kaare Lund Rasmussen","Jacopo La Nasa","Ilaria Degano","Maria Perla Colombini","Eibert Tigchelaar"],"pdf_url":"https://arxiv.org/pdf/2407.12013v2.pdf","comment":"16 pages of main article, 103 pages of supplementary materials; the\n  first version of this article is originally prepared in July 2023 after the\n  completion of all the experiments"},{"id":"http://arxiv.org/abs/2410.13754v2","updated":"2024-10-18T08:56:52Z","published":"2024-10-17T16:52:28Z","title":"MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures","summary":"  Perceiving and generating diverse modalities are crucial for AI models to\neffectively learn from and engage with real-world signals, necessitating\nreliable evaluations for their development. We identify two major issues in\ncurrent evaluations: (1) inconsistent standards, shaped by different\ncommunities with varying protocols and maturity levels; and (2) significant\nquery, grading, and generalization biases. To address these, we introduce\nMixEval-X, the first any-to-any, real-world benchmark designed to optimize and\nstandardize evaluations across diverse input and output modalities. We propose\nmulti-modal benchmark mixture and adaptation-rectification pipelines to\nreconstruct real-world task distributions, ensuring evaluations generalize\neffectively to real-world use cases. Extensive meta-evaluations show our\napproach effectively aligns benchmark samples with real-world task\ndistributions. Meanwhile, MixEval-X's model rankings correlate strongly with\nthat of crowd-sourced real-world evaluations (up to 0.98) while being much more\nefficient. We provide comprehensive leaderboards to rerank existing models and\norganizations and offer insights to enhance understanding of multi-modal\nevaluations and inform future research.\n","authors":["Jinjie Ni","Yifan Song","Deepanway Ghosal","Bo Li","David Junhao Zhang","Xiang Yue","Fuzhao Xue","Zian Zheng","Kaichen Zhang","Mahir Shah","Kabir Jain","Yang You","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2410.13754v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.05172v3","updated":"2024-10-18T08:44:31Z","published":"2023-06-08T13:11:20Z","title":"FLEdge: Benchmarking Federated Machine Learning Applications in Edge\n  Computing Systems","summary":"  Federated Learning (FL) has become a viable technique for realizing\nprivacy-enhancing distributed deep learning on the network edge. Heterogeneous\nhardware, unreliable client devices, and energy constraints often characterize\nedge computing systems. In this paper, we propose FLEdge, which complements\nexisting FL benchmarks by enabling a systematic evaluation of client\ncapabilities. We focus on computational and communication bottlenecks, client\nbehavior, and data security implications. Our experiments with models varying\nfrom 14K to 80M trainable parameters are carried out on dedicated hardware with\nemulated network characteristics and client behavior. We find that\nstate-of-the-art embedded hardware has significant memory bottlenecks, leading\nto 4x longer processing times than on modern data center GPUs.\n","authors":["Herbert Woisetschläger","Alexander Isenko","Ruben Mayer","Shiqiang Wang","Hans-Arno Jacobsen"],"pdf_url":"https://arxiv.org/pdf/2306.05172v3.pdf","comment":"Paper accepted for publication at the ACM/IFIP Middleware Conference\n  2024. Please cite the published version via\n  https://doi.org/10.1145/3652892.3700751"},{"id":"http://arxiv.org/abs/2410.14281v1","updated":"2024-10-18T08:38:12Z","published":"2024-10-18T08:38:12Z","title":"PTR: A Pre-trained Language Model for Trajectory Recovery","summary":"  Spatiotemporal trajectory data is vital for web-of-things services and is\nextensively collected and analyzed by web-based hardware and platforms.\nHowever, issues such as service interruptions and network instability often\nlead to sparsely recorded trajectories, resulting in a loss of detailed\nmovement data. As a result, recovering these trajectories to restore missing\ninformation becomes essential. Despite progress, several challenges remain\nunresolved. First, the lack of large-scale dense trajectory data hampers the\nperformance of existing deep learning methods, which rely heavily on abundant\ndata for supervised training. Second, current methods struggle to generalize\nacross sparse trajectories with varying sampling intervals, necessitating\nseparate re-training for each interval and increasing computational costs.\nThird, external factors crucial for the recovery of missing points are not\nfully incorporated.\n  To address these challenges, we propose a framework called PTR. This\nframework mitigates the issue of limited dense trajectory data by leveraging\nthe capabilities of pre-trained language models (PLMs). PTR incorporates an\nexplicit trajectory prompt and is trained on datasets with multiple sampling\nintervals, enabling it to generalize effectively across different intervals in\nsparse trajectories. To capture external factors, we introduce an implicit\ntrajectory prompt that models road conditions, providing richer information for\nrecovering missing points. Additionally, we present a trajectory embedder that\nencodes trajectory points and transforms the embeddings of both observed and\nmissing points into a format comprehensible to PLMs. Experimental results on\ntwo public trajectory datasets with three sampling intervals demonstrate the\nefficacy and scalability of PTR.\n","authors":["Tonglong Wei","Yan Lin","Youfang Lin","Shengnan Guo","Jilin Hu","Gao Cong","Huaiyu Wan"],"pdf_url":"https://arxiv.org/pdf/2410.14281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13196v2","updated":"2024-10-18T08:33:19Z","published":"2024-10-17T03:56:12Z","title":"Context-Enhanced Multi-View Trajectory Representation Learning: Bridging\n  the Gap through Self-Supervised Models","summary":"  Modeling trajectory data with generic-purpose dense representations has\nbecome a prevalent paradigm for various downstream applications, such as\ntrajectory classification, travel time estimation and similarity computation.\nHowever, existing methods typically rely on trajectories from a single spatial\nview, limiting their ability to capture the rich contextual information that is\ncrucial for gaining deeper insights into movement patterns across different\ngeospatial contexts. To this end, we propose MVTraj, a novel multi-view\nmodeling method for trajectory representation learning. MVTraj integrates\ndiverse contextual knowledge, from GPS to road network and points-of-interest\nto provide a more comprehensive understanding of trajectory data. To align the\nlearning process across multiple views, we utilize GPS trajectories as a bridge\nand employ self-supervised pretext tasks to capture and distinguish movement\npatterns across different spatial views. Following this, we treat trajectories\nfrom different views as distinct modalities and apply a hierarchical\ncross-modal interaction module to fuse the representations, thereby enriching\nthe knowledge derived from multiple sources. Extensive experiments on\nreal-world datasets demonstrate that MVTraj significantly outperforms existing\nbaselines in tasks associated with various spatial views, validating its\neffectiveness and practical utility in spatio-temporal modeling.\n","authors":["Tangwen Qian","Junhe Li","Yile Chen","Gao Cong","Tao Sun","Fei Wang","Yongjun Xu"],"pdf_url":"https://arxiv.org/pdf/2410.13196v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14270v1","updated":"2024-10-18T08:25:28Z","published":"2024-10-18T08:25:28Z","title":"Stochastic Quasi-Newton Optimization in Large Dimensions Including Deep\n  Network Training","summary":"  Our proposal is on a new stochastic optimizer for non-convex and possibly\nnon-smooth objective functions typically defined over large dimensional design\nspaces. Towards this, we have tried to bridge noise-assisted global search and\nfaster local convergence, the latter being the characteristic feature of a\nNewton-like search. Our specific scheme -- acronymed FINDER (Filtering Informed\nNewton-like and Derivative-free Evolutionary Recursion), exploits the nonlinear\nstochastic filtering equations to arrive at a derivative-free update that has\nresemblance with the Newton search employing the inverse Hessian of the\nobjective function. Following certain simplifications of the update to enable a\nlinear scaling with dimension and a few other enhancements, we apply FINDER to\na range of problems, starting with some IEEE benchmark objective functions to a\ncouple of archetypal data-driven problems in deep networks to certain cases of\nphysics-informed deep networks. The performance of the new method vis-\\'a-vis\nthe well-known Adam and a few others bears evidence to its promise and\npotentialities for large dimensional optimization problems of practical\ninterest.\n","authors":["Uttam Suman","Mariya Mamajiwala","Mukul Saxena","Ankit Tyagi","Debasish Roy"],"pdf_url":"https://arxiv.org/pdf/2410.14270v1.pdf","comment":"19 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.14269v1","updated":"2024-10-18T08:24:07Z","published":"2024-10-18T08:24:07Z","title":"On time series clustering with k-means","summary":"  There is a long history of research into time series clustering using\ndistance-based partitional clustering. Many of the most popular algorithms\nadapt k-means (also known as Lloyd's algorithm) to exploit time dependencies in\nthe data by specifying a time series distance function. However, these\nalgorithms are often presented with k-means configured in various ways,\naltering key parameters such as the initialisation strategy. This variability\nmakes it difficult to compare studies because k-means is known to be highly\nsensitive to its configuration. To address this, we propose a standard\nLloyd's-based model for TSCL that adopts an end-to-end approach, incorporating\na specialised distance function not only in the assignment step but also in the\ninitialisation and stopping criteria. By doing so, we create a unified\nstructure for comparing seven popular Lloyd's-based TSCL algorithms. This\ncommon framework enables us to more easily attribute differences in clustering\nperformance to the distance function itself, rather than variations in the\nk-means configuration.\n","authors":["Christopher Holder","Anthony Bagnall","Jason Lines"],"pdf_url":"https://arxiv.org/pdf/2410.14269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14268v1","updated":"2024-10-18T08:22:07Z","published":"2024-10-18T08:22:07Z","title":"MoDification: Mixture of Depths Made Easy","summary":"  Long-context efficiency has recently become a trending topic in serving large\nlanguage models (LLMs). And mixture of depths (MoD) is proposed as a perfect\nfit to bring down both latency and memory. In this paper, however, we discover\nthat MoD can barely transform existing LLMs without costly training over an\nextensive number of tokens. To enable the transformations from any LLMs to MoD\nones, we showcase top-k operator in MoD should be promoted to threshold-p\noperator, and refinement to architecture and data should also be crafted along.\nAll these designs form our method termed MoDification. Through a comprehensive\nset of experiments covering model scales from 3B to 70B, we exhibit\nMoDification strikes an excellent balance between efficiency and effectiveness.\nMoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in\nmemory compared to original LLMs especially in long-context applications.\n","authors":["Chen Zhang","Meizhi Zhong","Qimeng Wang","Xuantao Lu","Zheyu Ye","Chengqiang Lu","Yan Gao","Yao Hu","Kehai Chen","Min Zhang","Dawei Song"],"pdf_url":"https://arxiv.org/pdf/2410.14268v1.pdf","comment":"12 pages, 9 figures, 5 tables, work in progress"},{"id":"http://arxiv.org/abs/2403.13784v6","updated":"2024-10-18T08:20:22Z","published":"2024-03-20T17:47:08Z","title":"The Model Openness Framework: Promoting Completeness and Openness for\n  Reproducibility, Transparency, and Usability in Artificial Intelligence","summary":"  Generative artificial intelligence (AI) offers numerous opportunities for\nresearch and innovation, but its commercialization has raised concerns about\nthe transparency and safety of frontier AI models. Most models lack the\nnecessary components for full understanding, auditing, and reproducibility, and\nsome model producers use restrictive licenses whilst claiming that their models\nare \"open source\". To address these concerns, we introduce the Model Openness\nFramework (MOF), a three-tiered ranked classification system that rates machine\nlearning models based on their completeness and openness, following open\nscience principles. For each MOF class, we specify code, data, and\ndocumentation components of the model development lifecycle that must be\nreleased and under which open licenses. In addition, the Model Openness Tool\n(MOT) provides a user-friendly reference implementation to evaluate the\nopenness and completeness of models against the MOF classification system.\nTogether, the MOF and MOT provide timely practical guidance for (i) model\nproducers to enhance the openness and completeness of their publicly-released\nmodels, and (ii) model consumers to identify open models and their constituent\ncomponents that can be permissively used, studied, modified, and redistributed.\nThrough the MOF, we seek to establish completeness and openness as core tenets\nof responsible AI research and development, and to promote best practices in\nthe burgeoning open AI ecosystem.\n","authors":["Matt White","Ibrahim Haddad","Cailean Osborne","Xiao-Yang Yanglet Liu","Ahmed Abdelmonsef","Sachin Varghese","Arnaud Le Hors"],"pdf_url":"https://arxiv.org/pdf/2403.13784v6.pdf","comment":"28 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2406.00125v3","updated":"2024-10-18T08:18:36Z","published":"2024-05-31T18:32:46Z","title":"TotalVibeSegmentator: Full Body MRI Segmentation for the NAKO and UK\n  Biobank","summary":"  Objectives: To present a publicly available torso segmentation network for\nlarge epidemiology datasets on volumetric interpolated breath-hold examination\n(VIBE) images. Materials & Methods: We extracted preliminary segmentations from\nTotalSegmentator, spine, and body composition networks for VIBE images, then\nimproved them iteratively and retrained a nnUNet network. Using subsets of NAKO\n(85 subjects) and UK Biobank (16 subjects), we evaluated with Dice-score on a\nholdout set (12 subjects) and existing organ segmentation approach (1000\nsubjects), generating 71 semantic segmentation types for VIBE images. We\nprovide an additional network for the vertebra segments 22 individual vertebra\ntypes. Results: We achieved an average Dice score of 0.89 +- 0.07 overall 71\nsegmentation labels. We scored > 0.90 Dice-score on the abdominal organs except\nfor the pancreas with a Dice of 0.70. Conclusion: Our work offers a detailed\nand refined publicly available full torso segmentation on VIBE images.\n","authors":["Robert Graf","Paul-Sören Platzek","Evamaria Olga Riedel","Constanze Ramschütz","Sophie Starck","Hendrik Kristian Möller","Matan Atad","Henry Völzke","Robin Bülow","Carsten Oliver Schmidt","Julia Rüdebusch","Matthias Jung","Marco Reisert","Jakob Weiss","Maximilian Löffler","Fabian Bamberg","Bene Wiestler","Johannes C. Paetzold","Daniel Rueckert","Jan Stefan Kirschke"],"pdf_url":"https://arxiv.org/pdf/2406.00125v3.pdf","comment":"https://github.com/robert-graf/TotalVibeSegmentator"},{"id":"http://arxiv.org/abs/2402.16346v3","updated":"2024-10-18T08:09:14Z","published":"2024-02-26T07:00:24Z","title":"Boosting Graph Pooling with Persistent Homology","summary":"  Recently, there has been an emerging trend to integrate persistent homology\n(PH) into graph neural networks (GNNs) to enrich expressive power. However,\nnaively plugging PH features into GNN layers always results in marginal\nimprovement with low interpretability. In this paper, we investigate a novel\nmechanism for injecting global topological invariance into pooling layers using\nPH, motivated by the observation that filtration operation in PH naturally\naligns graph pooling in a cut-off manner. In this fashion, message passing in\nthe coarsened graph acts along persistent pooled topology, leading to improved\nperformance. Experimentally, we apply our mechanism to a collection of graph\npooling methods and observe consistent and substantial performance gain over\nseveral popular datasets, demonstrating its wide applicability and flexibility.\n","authors":["Chaolong Ying","Xinjian Zhao","Tianshu Yu"],"pdf_url":"https://arxiv.org/pdf/2402.16346v3.pdf","comment":"Published at NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.14257v1","updated":"2024-10-18T08:05:37Z","published":"2024-10-18T08:05:37Z","title":"Revisiting SLO and Goodput Metrics in LLM Serving","summary":"  Large language models (LLMs) have achieved remarkable performance and are\nwidely deployed in various applications, while the serving of LLM inference has\nraised concerns about user experience and serving throughput. Accordingly,\nservice level objectives (SLOs) and goodput-the number of requests that meet\nSLOs per second-are introduced to evaluate the performance of LLM serving.\nHowever, existing metrics fail to capture the nature of user experience. We\nobserve two ridiculous phenomena in existing metrics: 1) delaying token\ndelivery can smooth the tail time between tokens (tail TBT) of a request and 2)\ndropping the request that fails to meet the SLOs midway can improve goodput.\n  In this paper, we revisit SLO and goodput metrics in LLM serving and propose\na unified metric framework smooth goodput including SLOs and goodput to reflect\nthe nature of user experience in LLM serving. The framework can adapt to\nspecific goals of different tasks by setting parameters. We re-evaluate the\nperformance of different LLM serving systems under multiple workloads based on\nthis unified framework and provide possible directions for future optimization\nof existing strategies. We hope that this framework can provide a unified\nstandard for evaluating LLM serving and foster researches in the field of LLM\nserving optimization to move in a cohesive direction.\n","authors":["Zhibin Wang","Shipeng Li","Yuhang Zhou","Xue Li","Rong Gu","Nguyen Cam-Tu","Chen Tian","Sheng Zhong"],"pdf_url":"https://arxiv.org/pdf/2410.14257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20988v3","updated":"2024-10-18T08:05:18Z","published":"2024-05-31T16:34:11Z","title":"Communication-Efficient Distributed Deep Learning via Federated Dynamic\n  Averaging","summary":"  Driven by the ever-growing volume and decentralized nature of data, coupled\nwith the need to harness this data and generate knowledge from it, has led to\nthe extensive use of distributed deep learning (DDL) techniques for training.\nThese techniques rely on local training that is performed at the distributed\nnodes based on locally collected data, followed by a periodic synchronization\nprocess that combines these models to create a global model. However, frequent\nsynchronization of DL models, encompassing millions to many billions of\nparameters, creates a communication bottleneck, severely hindering scalability.\nWorse yet, DDL algorithms typically waste valuable bandwidth, and make\nthemselves less practical in bandwidth-constrained federated settings, by\nrelying on overly simplistic, periodic, and rigid synchronization schedules.\nThese drawbacks also have a direct impact on the time required for the training\nprocess, necessitating excessive time for data communication. To address these\nshortcomings, we propose Federated Dynamic Averaging (FDA), a\ncommunication-efficient DDL strategy that dynamically triggers synchronization\nbased on the value of the model variance. In essence, the costly\nsynchronization step is triggered only if the local models, which are\ninitialized from a common global model after each synchronization, have\nsignificantly diverged. This decision is facilitated by the communication of a\nsmall local state from each distributed node/worker. Through extensive\nexperiments across a wide range of learning tasks we demonstrate that FDA\nreduces communication cost by orders of magnitude, compared to both traditional\nand cutting-edge communication-efficient algorithms. Additionally, we show that\nFDA maintains robust performance across diverse data heterogeneity settings.\n","authors":["Michail Theologitis","Georgios Frangias","Georgios Anestis","Vasilis Samoladas","Antonios Deligiannakis"],"pdf_url":"https://arxiv.org/pdf/2405.20988v3.pdf","comment":"Accepted as research paper at EDBT 2025"},{"id":"http://arxiv.org/abs/2311.16984v4","updated":"2024-10-18T08:04:36Z","published":"2023-11-28T17:35:38Z","title":"FedECA: A Federated External Control Arm Method for Causal Inference\n  with Time-To-Event Data in Distributed Settings","summary":"  External control arms (ECA) can inform the early clinical development of\nexperimental drugs and provide efficacy evidence for regulatory approval.\nHowever, the main challenge in implementing ECA lies in accessing real-world or\nhistorical clinical trials data. Indeed, regulations protecting patients'\nrights by strictly controlling data processing make pooling data from multiple\nsources in a central server often difficult. To address these limitations, we\ndevelop a new method, 'FedECA' that leverages federated learning (FL) to enable\ninverse probability of treatment weighting (IPTW) for time-to-event outcomes on\nseparate cohorts without needing to pool data. To showcase the potential of\nFedECA, we apply it in different settings of increasing complexity culminating\nwith a real-world use-case in which FedECA provides evidence for a differential\neffect between two drugs that would have otherwise gone unnoticed. By sharing\nour code, we hope FedECA will foster the creation of federated research\nnetworks and thus accelerate drug development.\n","authors":["Jean Ogier du Terrail","Quentin Klopfenstein","Honghao Li","Imke Mayer","Nicolas Loiseau","Mohammad Hallal","Michael Debouver","Thibault Camalon","Thibault Fouqueray","Jorge Arellano Castro","Zahia Yanes","Laetitia Dahan","Julien Taïeb","Pierre Laurent-Puig","Jean-Baptiste Bachet","Shulin Zhao","Remy Nicolle","Jérome Cros","Daniel Gonzalez","Robert Carreras-Torres","Adelaida Garcia Velasco","Kawther Abdilleh","Sudheer Doss","Félix Balazard","Mathieu Andreux"],"pdf_url":"https://arxiv.org/pdf/2311.16984v4.pdf","comment":"code available at: https://github.com/owkin/fedeca, bug in SMD\n  computation present in v1 and v2 has been fixed, many experiments on real\n  data have been added + fix in YODA experiments using imputed data instead of\n  raw data as well as typos and affiliations fix"},{"id":"http://arxiv.org/abs/2410.14254v1","updated":"2024-10-18T08:04:31Z","published":"2024-10-18T08:04:31Z","title":"RAZOR: Refining Accuracy by Zeroing Out Redundancies","summary":"  In many application domains, the proliferation of sensors and devices is\ngenerating vast volumes of data, imposing significant pressure on existing data\nanalysis and data mining techniques. Nevertheless, an increase in data volume\ndoes not inherently imply an increase in informational content, as a\nsubstantial portion may be redundant or represent noise. This challenge is\nparticularly evident in the deep learning domain, where the utility of\nadditional data is contingent on its informativeness. In the absence of such,\nlarger datasets merely exacerbate the computational cost and complexity of the\nlearning process. To address these challenges, we propose RAZOR, a novel\ninstance selection technique designed to extract a significantly smaller yet\nsufficiently informative subset from a larger set of instances without\ncompromising the learning process. RAZOR has been specifically engineered to be\nrobust, efficient, and scalable, making it suitable for large-scale datasets.\nUnlike many techniques in the literature, RAZOR is capable of operating in both\nsupervised and unsupervised settings. Experimental results demonstrate that\nRAZOR outperforms recent state-of-the-art techniques in terms of both\neffectiveness and efficiency.\n","authors":["Daniel Riccio","Genoveffa Tortora","Mara Sangiovanni"],"pdf_url":"https://arxiv.org/pdf/2410.14254v1.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2306.08670v5","updated":"2024-10-18T08:00:31Z","published":"2023-06-14T17:59:15Z","title":"Simple Opinion Dynamics for No-Regret Learning","summary":"  We study a cooperative multi-agent bandit setting in the distributed GOSSIP\nmodel: in every round, each of $n$ agents chooses an action from a common set,\nobserves the action's corresponding reward, and subsequently exchanges\ninformation with a single randomly chosen neighbor, which may inform its choice\nin the next round. We introduce and analyze families of memoryless and\ntime-independent protocols for this setting, inspired by opinion dynamics that\nare well-studied for other algorithmic tasks in the GOSSIP model. For\nstationary reward settings, we prove for the first time that these simple\nprotocols exhibit best-of-both-worlds behavior, simultaneously obtaining\nconstant cumulative regret scaling like $R(T)/T = \\widetilde O(1/T)$, and also\nreaching consensus on the highest-mean action within $\\widetilde O(\\sqrt{n})$\nrounds. We obtain these results by showing a new connection between the global\nevolution of these decentralized protocols and a class of zero-sum\nmultiplicative weights update} processes. Using this connection, we establish a\ngeneral framework for analyzing the population-level regret and other\nproperties of our protocols. Finally, we show our protocols are also\nsurprisingly robust to adversarial rewards, and in this regime we obtain\nsublinear regret scaling like $R(T)/T = \\widetilde O(1/\\sqrt{T})$ as long as\nthe number of rounds does not grow too fast as a function of $n$.\n","authors":["John Lazarsfeld","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2306.08670v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14242v1","updated":"2024-10-18T07:47:59Z","published":"2024-10-18T07:47:59Z","title":"Pseudo-label Refinement for Improving Self-Supervised Learning Systems","summary":"  Self-supervised learning systems have gained significant attention in recent\nyears by leveraging clustering-based pseudo-labels to provide supervision\nwithout the need for human annotations. However, the noise in these\npseudo-labels caused by the clustering methods poses a challenge to the\nlearning process leading to degraded performance. In this work, we propose a\npseudo-label refinement (SLR) algorithm to address this issue. The cluster\nlabels from the previous epoch are projected to the current epoch\ncluster-labels space and a linear combination of the new label and the\nprojected label is computed as a soft refined label containing the information\nfrom the previous epoch clusters as well as from the current epoch. In contrast\nto the common practice of using the maximum value as a cluster/class indicator,\nwe employ hierarchical clustering on these soft pseudo-labels to generate\nrefined hard-labels. This approach better utilizes the information embedded in\nthe soft labels, outperforming the simple maximum value approach for hard label\ngeneration. The effectiveness of the proposed SLR algorithm is evaluated in the\ncontext of person re-identification (Re-ID) using unsupervised domain\nadaptation (UDA). Experimental results demonstrate that the modified Re-ID\nbaseline, incorporating the SLR algorithm, achieves significantly improved mean\nAverage Precision (mAP) performance in various UDA tasks, including\nreal-to-synthetic, synthetic-to-real, and different real-to-real scenarios.\nThese findings highlight the efficacy of the SLR algorithm in enhancing the\nperformance of self-supervised learning systems.\n","authors":[" Zia-ur-Rehman","Arif Mahmood","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2410.14242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14240v1","updated":"2024-10-18T07:44:12Z","published":"2024-10-18T07:44:12Z","title":"Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in\n  Dynamical Systems Reconstruction","summary":"  Dynamical systems (DS) theory is fundamental for many areas of science and\nengineering. It can provide deep insights into the behavior of systems evolving\nin time, as typically described by differential or recursive equations. A\ncommon approach to facilitate mathematical tractability and interpretability of\nDS models involves decomposing nonlinear DS into multiple linear DS separated\nby switching manifolds, i.e. piecewise linear (PWL) systems. PWL models are\npopular in engineering and a frequent choice in mathematics for analyzing the\ntopological properties of DS. However, hand-crafting such models is tedious and\nonly possible for very low-dimensional scenarios, while inferring them from\ndata usually gives rise to unnecessarily complex representations with very many\nlinear subregions. Here we introduce Almost-Linear Recurrent Neural Networks\n(AL-RNNs) which automatically and robustly produce most parsimonious PWL\nrepresentations of DS from time series data, using as few PWL nonlinearities as\npossible. AL-RNNs can be efficiently trained with any SOTA algorithm for\ndynamical systems reconstruction (DSR), and naturally give rise to a symbolic\nencoding of the underlying DS that provably preserves important topological\nproperties. We show that for the Lorenz and R\\\"ossler systems, AL-RNNs\ndiscover, in a purely data-driven way, the known topologically minimal PWL\nrepresentations of the corresponding chaotic attractors. We further illustrate\non two challenging empirical datasets that interpretable symbolic encodings of\nthe dynamics can be achieved, tremendously facilitating mathematical and\ncomputational analysis of the underlying systems.\n","authors":["Manuel Brenner","Christoph Jürgen Hemmer","Zahra Monfared","Daniel Durstewitz"],"pdf_url":"https://arxiv.org/pdf/2410.14240v1.pdf","comment":"38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)"},{"id":"http://arxiv.org/abs/2410.14237v1","updated":"2024-10-18T07:37:36Z","published":"2024-10-18T07:37:36Z","title":"Unified Convergence Analysis for Score-Based Diffusion Models with\n  Deterministic Samplers","summary":"  Score-based diffusion models have emerged as powerful techniques for\ngenerating samples from high-dimensional data distributions. These models\ninvolve a two-phase process: first, injecting noise to transform the data\ndistribution into a known prior distribution, and second, sampling to recover\nthe original data distribution from noise. Among the various sampling methods,\ndeterministic samplers stand out for their enhanced efficiency. However,\nanalyzing these deterministic samplers presents unique challenges, as they\npreclude the use of established techniques such as Girsanov's theorem, which\nare only applicable to stochastic samplers. Furthermore, existing analysis for\ndeterministic samplers usually focuses on specific examples, lacking a\ngeneralized approach for general forward processes and various deterministic\nsamplers. Our paper addresses these limitations by introducing a unified\nconvergence analysis framework. To demonstrate the power of our framework, we\nanalyze the variance-preserving (VP) forward process with the exponential\nintegrator (EI) scheme, achieving iteration complexity of $\\tilde\nO(d^2/\\epsilon)$. Additionally, we provide a detailed analysis of Denoising\nDiffusion Implicit Models (DDIM)-type samplers, which have been underexplored\nin previous research, achieving polynomial iteration complexity.\n","authors":["Runjia Li","Qiwei Di","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2410.14237v1.pdf","comment":"68 pages"},{"id":"http://arxiv.org/abs/2305.01661v2","updated":"2024-10-18T07:15:51Z","published":"2023-05-02T08:28:55Z","title":"Integrating spoken instructions into flight trajectory prediction to\n  optimize automation in air traffic control","summary":"  The booming air transportation industry inevitably burdens air traffic\ncontrollers' workload, causing unexpected human factor-related incidents.\nCurrent air traffic control systems fail to consider spoken instructions for\ntraffic prediction, bringing significant challenges in detecting human errors\nduring real-time traffic operations. Here, we present an automation paradigm\nintegrating controlling intent into the information processing loop through the\nspoken instruction-aware flight trajectory prediction framework. A 3-stage\nprogressive multi-modal learning paradigm is proposed to address the modality\ngap between the trajectory and spoken instructions, as well as minimize the\ndata requirements. Experiments on a real-world dataset show the proposed\nframework achieves flight trajectory prediction with high predictability and\ntimeliness, obtaining over 20% relative reduction in mean deviation error.\nMoreover, the generalizability of the proposed framework is also confirmed by\nvarious model architectures. The proposed framework can formulate\nfull-automated information processing in real-world air traffic applications,\nsupporting human error detection and enhancing aviation safety.\n","authors":["Dongyue Guo","Zheng Zhang","Bo Yang","Jianwei Zhang","Hongyu Yang","Yi Lin"],"pdf_url":"https://arxiv.org/pdf/2305.01661v2.pdf","comment":"This paper has been accepted in principle by Nature Communications"},{"id":"http://arxiv.org/abs/2410.14223v1","updated":"2024-10-18T07:14:08Z","published":"2024-10-18T07:14:08Z","title":"G-NeuroDAVIS: A Neural Network model for generalized embedding, data\n  visualization and sample generation","summary":"  Visualizing high-dimensional datasets through a generalized embedding has\nbeen a challenge for a long time. Several methods have shown up for the same,\nbut still, they have not been able to generate a generalized embedding, which\nnot only can reveal the hidden patterns present in the data but also generate\nrealistic high-dimensional samples from it. Motivated by this aspect, in this\nstudy, a novel generative model, called G-NeuroDAVIS, has been developed, which\nis capable of visualizing high-dimensional data through a generalized\nembedding, and thereby generating new samples. The model leverages advanced\ngenerative techniques to produce high-quality embedding that captures the\nunderlying structure of the data more effectively than existing methods.\nG-NeuroDAVIS can be trained in both supervised and unsupervised settings. We\nrigorously evaluated our model through a series of experiments, demonstrating\nsuperior performance in classification tasks, which highlights the robustness\nof the learned representations. Furthermore, the conditional sample generation\ncapability of the model has been described through qualitative assessments,\nrevealing a marked improvement in generating realistic and diverse samples.\nG-NeuroDAVIS has outperformed the Variational Autoencoder (VAE) significantly\nin multiple key aspects, including embedding quality, classification\nperformance, and sample generation capability. These results underscore the\npotential of our generative model to serve as a powerful tool in various\napplications requiring high-quality data generation and representation\nlearning.\n","authors":["Chayan Maitra","Rajat K. De"],"pdf_url":"https://arxiv.org/pdf/2410.14223v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.09785v3","updated":"2024-10-18T07:11:35Z","published":"2024-09-15T16:32:49Z","title":"Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition","summary":"  Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.\n","authors":["Chao-Han Huck Yang","Taejin Park","Yuan Gong","Yuanchao Li","Zhehuai Chen","Yen-Ting Lin","Chen Chen","Yuchen Hu","Kunal Dhawan","Piotr Żelasko","Chao Zhang","Yun-Nung Chen","Yu Tsao","Jagadeesh Balam","Boris Ginsburg","Sabato Marco Siniscalchi","Eng Siong Chng","Peter Bell","Catherine Lai","Shinji Watanabe","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2409.09785v3.pdf","comment":"IEEE SLT 2024. The initial draft version has been done in December\n  2023. Post-ASR Text Processing and Understanding Community and LlaMA-7B\n  pre-training correction model:\n  https://huggingface.co/GenSEC-LLM/SLT-Task1-Llama2-7b-HyPo-baseline"},{"id":"http://arxiv.org/abs/2410.14219v1","updated":"2024-10-18T07:08:31Z","published":"2024-10-18T07:08:31Z","title":"Formal Explanations for Neuro-Symbolic AI","summary":"  Despite the practical success of Artificial Intelligence (AI), current neural\nAI algorithms face two significant issues. First, the decisions made by neural\narchitectures are often prone to bias and brittleness. Second, when a chain of\nreasoning is required, neural systems often perform poorly. Neuro-symbolic\nartificial intelligence is a promising approach that tackles these (and other)\nweaknesses by combining the power of neural perception and symbolic reasoning.\nMeanwhile, the success of AI has made it critical to understand its behaviour,\nleading to the development of explainable artificial intelligence (XAI). While\nneuro-symbolic AI systems have important advantages over purely neural AI, we\nstill need to explain their actions, which are obscured by the interactions of\nthe neural and symbolic components. To address the issue, this paper proposes a\nformal approach to explaining the decisions of neuro-symbolic systems. The\napproach hinges on the use of formal abductive explanations and on solving the\nneuro-symbolic explainability problem hierarchically. Namely, it first computes\na formal explanation for the symbolic component of the system, which serves to\nidentify a subset of the individual parts of neural information that needs to\nbe explained. This is followed by explaining only those individual neural\ninputs, independently of each other, which facilitates succinctness of\nhierarchical formal explanations and helps to increase the overall performance\nof the approach. Experimental results for a few complex reasoning tasks\ndemonstrate practical efficiency of the proposed approach, in comparison to\npurely neural systems, from the perspective of explanation size, explanation\ntime, training time, model sizes, and the quality of explanations reported.\n","authors":["Sushmita Paul","Jinqiang Yu","Jip J. Dekker","Alexey Ignatiev","Peter J. Stuckey"],"pdf_url":"https://arxiv.org/pdf/2410.14219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04808v3","updated":"2024-10-18T07:05:57Z","published":"2024-03-06T10:55:30Z","title":"WaterMax: breaking the LLM watermark detectability-robustness-quality\n  trade-off","summary":"  Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.\n","authors":["Eva Giboulot","Teddy Furon"],"pdf_url":"https://arxiv.org/pdf/2403.04808v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13602v2","updated":"2024-10-18T07:04:25Z","published":"2024-10-17T14:36:58Z","title":"Towards Satellite Non-IID Imagery: A Spectral Clustering-Assisted\n  Federated Learning Approach","summary":"  Low Earth orbit (LEO) satellites are capable of gathering abundant Earth\nobservation data (EOD) to enable different Internet of Things (IoT)\napplications. However, to accomplish an effective EOD processing mechanism, it\nis imperative to investigate: 1) the challenge of processing the observed data\nwithout transmitting those large-size data to the ground because the connection\nbetween the satellites and the ground stations is intermittent, and 2) the\nchallenge of processing the non-independent and identically distributed\n(non-IID) satellite data. In this paper, to cope with those challenges, we\npropose an orbit-based spectral clustering-assisted clustered federated\nself-knowledge distillation (OSC-FSKD) approach for each orbit of an LEO\nsatellite constellation, which retains the advantage of FL that the observed\ndata does not need to be sent to the ground. Specifically, we introduce\nnormalized Laplacian-based spectral clustering (NLSC) into federated learning\n(FL) to create clustered FL in each round to address the challenge resulting\nfrom non-IID data. Particularly, NLSC is adopted to dynamically group clients\ninto several clusters based on cosine similarities calculated by model updates.\nIn addition, self-knowledge distillation is utilized to construct each local\nclient, where the most recent updated local model is used to guide current\nlocal model training. Experiments demonstrate that the observation accuracy\nobtained by the proposed method is separately 1.01x, 2.15x, 1.10x, and 1.03x\nhigher than that of pFedSD, FedProx, FedAU, and FedALA approaches using the\nSAT4 dataset. The proposed method also shows superiority when using other\ndatasets.\n","authors":["Luyao Zou","Yu Min Park","Chu Myaet Thwal","Yan Kyaw Tun","Zhu Han","Choong Seon Hong"],"pdf_url":"https://arxiv.org/pdf/2410.13602v2.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.14212v1","updated":"2024-10-18T07:01:56Z","published":"2024-10-18T07:01:56Z","title":"Comparative Evaluation of Clustered Federated Learning Method","summary":"  Over recent years, Federated Learning (FL) has proven to be one of the most\npromising methods of distributed learning which preserves data privacy. As the\nmethod evolved and was confronted to various real-world scenarios, new\nchallenges have emerged. One such challenge is the presence of highly\nheterogeneous (often referred as non-IID) data distributions among participants\nof the FL protocol. A popular solution to this hurdle is Clustered Federated\nLearning (CFL), which aims to partition clients into groups where the\ndistribution are homogeneous. In the literature, state-of-the-art CFL\nalgorithms are often tested using a few cases of data heterogeneities, without\nsystematically justifying the choices. Further, the taxonomy used for\ndifferentiating the different heterogeneity scenarios is not always\nstraightforward. In this paper, we explore the performance of two\nstate-of-theart CFL algorithms with respect to a proposed taxonomy of data\nheterogeneities in federated learning (FL). We work with three image\nclassification datasets and analyze the resulting clusters against the\nheterogeneity classes using extrinsic clustering metrics. Our objective is to\nprovide a clearer understanding of the relationship between CFL performances\nand data heterogeneity scenarios.\n","authors":["Michael Ben Ali","Omar El-Rifai","Imen Megdiche","André Peninou","Olivier Teste"],"pdf_url":"https://arxiv.org/pdf/2410.14212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16435v2","updated":"2024-10-18T06:56:10Z","published":"2024-05-26T05:22:38Z","title":"Node Identifiers: Compact, Discrete Representations for Efficient Graph\n  Learning","summary":"  We present a novel end-to-end framework that generates highly compact\n(typically 6-15 dimensions), discrete (int4 type), and interpretable node\nrepresentations, termed node identifiers (node IDs), to tackle inference\nchallenges on large-scale graphs. By employing vector quantization, we compress\ncontinuous node embeddings from multiple layers of a Graph Neural Network (GNN)\ninto discrete codes, applicable under both self-supervised and supervised\nlearning paradigms. These node IDs capture high-level abstractions of graph\ndata and offer interpretability that traditional GNN embeddings lack. Extensive\nexperiments on 34 datasets, encompassing node classification, graph\nclassification, link prediction, and attributed graph clustering tasks,\ndemonstrate that the generated node IDs significantly enhance speed and memory\nefficiency while achieving competitive performance compared to current\nstate-of-the-art methods.\n","authors":["Yuankai Luo","Hongkang Li","Qijiong Liu","Lei Shi","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2405.16435v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14208v1","updated":"2024-10-18T06:50:15Z","published":"2024-10-18T06:50:15Z","title":"Montessori-Instruct: Generate Influential Training Data Tailored for\n  Student Learning","summary":"  Synthetic data has been widely used to train large language models, but their\ngenerative nature inevitably introduces noisy, non-informative, and misleading\nlearning signals. In this paper, we propose Montessori-Instruct, a novel data\nsynthesis framework that tailors the data synthesis ability of the teacher\nlanguage model toward the student language model's learning process.\nSpecifically, we utilize local data influence of synthetic training data points\non students to characterize students' learning preferences. Then, we train the\nteacher model with Direct Preference Optimization (DPO) to generate synthetic\ndata tailored toward student learning preferences. Experiments with\nLlama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and\nMT-Bench demonstrate that Montessori-Instruct significantly outperforms\nstandard synthesis methods by 18.35\\% and 46.24\\% relatively. Our method also\nbeats data synthesized by a stronger teacher model, GPT-4o. Further analysis\nconfirms the benefits of teacher's learning to generate more influential\ntraining data in the student's improved learning, the advantages of local data\ninfluence in accurately measuring student preferences, and the robustness of\nMontessori-Instruct across different student models. Our code and data are\nopen-sourced at https://github.com/cxcscmu/Montessori-Instruct.\n","authors":["Xiaochuan Li","Zichun Yu","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.14208v1.pdf","comment":"Codes and data are open-sourced at\n  https://github.com/cxcscmu/Montessori-Instruct"},{"id":"http://arxiv.org/abs/2410.14207v1","updated":"2024-10-18T06:47:39Z","published":"2024-10-18T06:47:39Z","title":"Flexi-Fuzz least squares SVM for Alzheimer's diagnosis: Tackling noise,\n  outliers, and class imbalance","summary":"  Alzheimer's disease (AD) is a leading neurodegenerative condition and the\nprimary cause of dementia, characterized by progressive cognitive decline and\nmemory loss. Its progression, marked by shrinkage in the cerebral cortex, is\nirreversible. Numerous machine learning algorithms have been proposed for the\nearly diagnosis of AD. However, they often struggle with the issues of noise,\noutliers, and class imbalance. To tackle the aforementioned limitations, in\nthis article, we introduce a novel, robust, and flexible membership scheme\ncalled Flexi-Fuzz. This scheme integrates a novel flexible weighting mechanism,\nclass probability, and imbalance ratio. The proposed flexible weighting\nmechanism assigns the maximum weight to samples within a specific proximity to\nthe center, with a gradual decrease in weight beyond a certain threshold. This\napproach ensures that samples near the class boundary still receive significant\nweight, maintaining their influence in the classification process. Class\nprobability is used to mitigate the impact of noisy samples, while the\nimbalance ratio addresses class imbalance. Leveraging this, we incorporate the\nproposed Flexi-Fuzz membership scheme into the least squares support vector\nmachines (LSSVM) framework, resulting in a robust and flexible model termed\nFlexi-Fuzz-LSSVM. We determine the class-center using two methods: the\nconventional mean approach and an innovative median approach, leading to two\nmodel variants, Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II. To validate the\neffectiveness of the proposed Flexi-Fuzz-LSSVM models, we evaluated them on\nbenchmark UCI and KEEL datasets, both with and without label noise.\nAdditionally, we tested the models on the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset for AD diagnosis. Experimental results demonstrate\nthe superiority of the Flexi-Fuzz-LSSVM models over baseline models.\n","authors":["Mushir Akhtar","A. Quadir","M. Tanveer","Mohd. Arshad"],"pdf_url":"https://arxiv.org/pdf/2410.14207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19619v2","updated":"2024-10-18T06:40:37Z","published":"2024-06-28T03:02:25Z","title":"ScoreFusion: fusing score-based generative models via Kullback-Leibler\n  barycenters","summary":"  We introduce ScoreFusion, a theoretically grounded method for fusing multiple\npre-trained diffusion models that are assumed to generate from auxiliary\npopulations. ScoreFusion is particularly useful for enhancing the generative\nmodeling of a target population with limited observed data. Our starting point\nconsiders the family of KL barycenters of the auxiliary populations, which is\nproven to be an optimal parametric class in the KL sense, but difficult to\nlearn. Nevertheless, by recasting the learning problem as score matching in\ndenoising diffusion, we obtain a tractable way of computing the optimal KL\nbarycenter weights. We prove a dimension-free sample complexity bound in total\nvariation distance, provided that the auxiliary models are well fitted for\ntheir own task and the auxiliary tasks combined capture the target well. We\nalso explain a connection of the practice of checkpoint merging in AI art\ncreation to an approximation of our KL-barycenter-based fusion approach.\nHowever, our fusion method differs in key aspects, allowing generation of new\npopulations, as we illustrate in experiments.\n","authors":["Hao Liu","Junze Tony Ye","Jose Blanchet","Nian Si"],"pdf_url":"https://arxiv.org/pdf/2406.19619v2.pdf","comment":"53 pages, 15 figures"},{"id":"http://arxiv.org/abs/2311.01483v5","updated":"2024-10-18T06:38:11Z","published":"2023-11-02T14:47:06Z","title":"FedSN: A Federated Learning Framework over Heterogeneous LEO Satellite\n  Networks","summary":"  Recently, a large number of Low Earth Orbit (LEO) satellites have been\nlaunched and deployed successfully in space by commercial companies, such as\nSpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve\nnot only for communication but also for various machine learning applications,\nsuch as space modulation recognition, remote sensing image classification, etc.\nHowever, the ground station (GS) may be incapable of downloading such a large\nvolume of raw sensing data for centralized model training due to the limited\ncontact time with LEO satellites (e.g. 5 minutes). Therefore, federated\nlearning (FL) has emerged as the promising solution to address this problem via\non-device training. Unfortunately, to enable FL on LEO satellites, we still\nface three critical challenges that are i) heterogeneous computing and memory\ncapabilities, ii) limited uplink rate, and iii) model staleness. To this end,\nwe propose FedSN as a general FL framework to tackle the above challenges, and\nfully explore data diversity on LEO satellites. Specifically, we first present\na novel sub-structure scheme to enable heterogeneous local model training\nconsidering different computing, memory, and communication constraints on LEO\nsatellites. Additionally, we propose a pseudo-synchronous model aggregation\nstrategy to dynamically schedule model aggregation for compensating model\nstaleness. To further demonstrate the effectiveness of the FedSN, we evaluate\nit using space modulation recognition and remote sensing image classification\ntasks by leveraging the data from real-world satellite networks. Extensive\nexperimental results demonstrate that FedSN framework achieves higher accuracy,\nlower computing, and communication overhead than the state-of-the-art\nbenchmarks and the effectiveness of each components in FedSN.\n","authors":["Zheng Lin","Zhe Chen","Zihan Fang","Xianhao Chen","Xiong Wang","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2311.01483v5.pdf","comment":"15 pages, 17 figures"},{"id":"http://arxiv.org/abs/2404.02175v2","updated":"2024-10-18T06:33:19Z","published":"2024-04-01T11:23:31Z","title":"Social Dynamics of Consumer Response: A Unified Framework Integrating\n  Statistical Physics and Marketing Dynamics","summary":"  Understanding how consumers react to advertising inputs is essential for\nmarketers aiming to optimize advertising strategies and improve campaign\neffectiveness. This study examines the complex nature of consumer behaviour by\napplying theoretical frameworks derived from physics and social psychology. We\npresent an innovative equation that captures the relation between spending on\nadvertising and consumer response, using concepts such as symmetries, scaling\nlaws, and phase transitions. By validating our equation against well-known\nmodels such as the Michaelis-Menten and Hill equations, we prove its\neffectiveness in accurately representing the complexity of consumer response\ndynamics. The analysis emphasizes the importance of key model parameters, such\nas marketing effectiveness, response sensitivity, and behavioural sensitivity,\nin influencing consumer behaviour. The work explores the practical implications\nfor advertisers and marketers, as well as discussing the limitations and future\nresearch directions. In summary, this study provides a thorough framework for\ncomprehending and forecasting consumer reactions to advertising, which has\nimplications for optimizing advertising strategies and allocating resources.\n","authors":["Javier Marin"],"pdf_url":"https://arxiv.org/pdf/2404.02175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16922v2","updated":"2024-10-18T06:15:42Z","published":"2024-05-27T08:13:39Z","title":"Theories of synaptic memory consolidation and intelligent plasticity for\n  continual learning","summary":"  Humans and animals learn throughout life. Such continual learning is crucial\nfor intelligence. In this chapter, we examine the pivotal role plasticity\nmechanisms with complex internal synaptic dynamics could play in enabling this\nability in neural networks. By surveying theoretical research, we highlight two\nfundamental enablers for continual learning. First, synaptic plasticity\nmechanisms must maintain and evolve an internal state over several behaviorally\nrelevant timescales. Second, plasticity algorithms must leverage the internal\nstate to intelligently regulate plasticity at individual synapses to facilitate\nthe seamless integration of new memories while avoiding detrimental\ninterference with existing ones. Our chapter covers successful applications of\nthese principles to deep neural networks and underscores the significance of\nsynaptic metaplasticity in sustaining continual learning capabilities. Finally,\nwe outline avenues for further research to understand the brain's superb\ncontinual learning abilities and harness similar mechanisms for artificial\nintelligence systems.\n","authors":["Friedemann Zenke","Axel Laborieux"],"pdf_url":"https://arxiv.org/pdf/2405.16922v2.pdf","comment":"An introductory-level book chapter. 35 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.14193v1","updated":"2024-10-18T06:07:22Z","published":"2024-10-18T06:07:22Z","title":"xPerT: Extended Persistence Transformer","summary":"  A persistence diagram provides a compact summary of persistent homology,\nwhich captures the topological features of a space at different scales.\nHowever, due to its nature as a set, incorporating it as a feature into a\nmachine learning framework is challenging. Several methods have been proposed\nto use persistence diagrams as input for machine learning models, but they\noften require complex preprocessing steps and extensive hyperparameter tuning.\nIn this paper, we propose a novel transformer architecture called the\n\\textit{Extended Persistence Transformer (xPerT)}, which is highly scalable\nthan the compared to Persformer, an existing transformer for persistence\ndiagrams. xPerT reduces GPU memory usage by over 90\\% and improves accuracy on\nmultiple datasets. Additionally, xPerT does not require complex preprocessing\nsteps or extensive hyperparameter tuning, making it easy to use in practice.\nOur code is available at https://github.com/sehunfromdaegu/ECG_JEPA.\n","authors":["Sehun Kim"],"pdf_url":"https://arxiv.org/pdf/2410.14193v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.14634v1","updated":"2024-10-18T17:35:33Z","published":"2024-10-18T17:35:33Z","title":"Parallel Backpropagation for Inverse of a Convolution with Application\n  to Normalizing Flows","summary":"  Inverse of an invertible convolution is an important operation that comes up\nin Normalizing Flows, Image Deblurring, etc. The naive algorithm for\nbackpropagation of this operation using Gaussian elimination has running time\n$O(n^3)$ where $n$ is the number of pixels in the image. We give a fast\nparallel backpropagation algorithm with running time $O(\\sqrt{n})$ for a square\nimage and provide a GPU implementation of the same. Inverse Convolutions are\nusually used in Normalizing Flows in the sampling pass, making them slow. We\npropose to use Inverse Convolutions in the forward (image to latent vector)\npass of the Normalizing flow. Since the sampling pass is the inverse of the\nforward pass, it will use convolutions only, resulting in efficient sampling\ntimes. We use our parallel backpropagation algorithm for optimizing the inverse\nconvolution layer resulting in fast training times also. We implement this\napproach in various Normalizing Flow backbones, resulting in our Inverse-Flow\nmodels. We benchmark Inverse-Flow on standard datasets and show significantly\nimproved sampling times with similar bits per dimension compared to previous\nmodels.\n","authors":["Sandeep Nagar","Girish Varma"],"pdf_url":"https://arxiv.org/pdf/2410.14634v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2404.13370v2","updated":"2024-10-18T16:44:05Z","published":"2024-04-20T13:15:27Z","title":"Movie101v2: Improved Movie Narration Benchmark","summary":"  Automatic movie narration aims to generate video-aligned plot descriptions to\nassist visually impaired audiences. Unlike standard video captioning, it\ninvolves not only describing key visual details but also inferring plots that\nunfold across multiple movie shots, presenting distinct and complex challenges.\nTo advance this field, we introduce Movie101v2, a large-scale, bilingual\ndataset with enhanced data quality specifically designed for movie narration.\nRevisiting the task, we propose breaking down the ultimate goal of automatic\nmovie narration into three progressive stages, offering a clear roadmap with\ncorresponding evaluation metrics. Based on our new benchmark, we baseline a\nrange of large vision-language models, including GPT-4V, and conduct an\nin-depth analysis of the challenges in narration generation. Our findings\nhighlight that achieving applicable movie narration generation is a fascinating\ngoal that requires significant research.\n","authors":["Zihao Yue","Yepeng Zhang","Ziheng Wang","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2404.13370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06729v2","updated":"2024-10-18T13:45:09Z","published":"2024-10-09T09:55:51Z","title":"Perceptual Quality Assessment of Octree-RAHT Encoded 3D Point Clouds","summary":"  No-reference bitstream-layer point cloud quality assessment (PCQA) can be\ndeployed without full decoding at any network node to achieve real-time quality\nmonitoring. In this work, we focus on the PCQA problem dedicated to Octree-RAHT\nencoding mode. First, to address the issue that existing PCQA databases have a\nsmall scale and limited distortion levels, we establish the WPC5.0 database\nwhich is the first one dedicated to Octree-RAHT encoding mode with a scale of\n400 distorted point clouds (PCs) including 4 geometric multiplied by 5 attitude\ndistortion levels. Then, we propose the first PCQA model dedicated to\nOctree-RAHT encoding mode by parsing PC bitstreams without full decoding. The\nmodel introduces texture bitrate (TBPP) to predict texture complexity (TC) and\nfurther derives the texture distortion factor. In addition, the Geometric\nQuantization Parameter (PQS) is used to estimate the geometric distortion\nfactor, which is then integrated into the model along with the texture\ndistortion factor to obtain the proposed PCQA model named streamPCQ-OR. The\nproposed model has been compared with other advanced PCQA methods on the\nWPC5.0, BASICS and M-PCCD databases, and experimental results show that our\nmodel has excellent performance while having very low computational complexity,\nproviding a reliable choice for time-critical applications. To facilitate\nsubsequent research, the database and source code will be publicly released at\nhttps://github.com/qdushl/Waterloo-Point-Cloud-Database-5.0.\n","authors":["Dongshuai Duan","Honglei Su","Qi Liu","Hui Yuan","Wei Gao","Jiarun Song","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2410.06729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10291v2","updated":"2024-10-18T09:26:46Z","published":"2024-10-14T08:45:35Z","title":"Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal\n  Perspective","summary":"  Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench .\n","authors":["Xiangru Zhu","Penglei Sun","Yaoxian Song","Yanghua Xiao","Zhixu Li","Chengyu Wang","Jun Huang","Bei Yang","Xiaoxiao Xu"],"pdf_url":"https://arxiv.org/pdf/2410.10291v2.pdf","comment":"The only change in the current version update is the replacement of\n  the template with a more precise one"},{"id":"http://arxiv.org/abs/2410.13754v2","updated":"2024-10-18T08:56:52Z","published":"2024-10-17T16:52:28Z","title":"MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures","summary":"  Perceiving and generating diverse modalities are crucial for AI models to\neffectively learn from and engage with real-world signals, necessitating\nreliable evaluations for their development. We identify two major issues in\ncurrent evaluations: (1) inconsistent standards, shaped by different\ncommunities with varying protocols and maturity levels; and (2) significant\nquery, grading, and generalization biases. To address these, we introduce\nMixEval-X, the first any-to-any, real-world benchmark designed to optimize and\nstandardize evaluations across diverse input and output modalities. We propose\nmulti-modal benchmark mixture and adaptation-rectification pipelines to\nreconstruct real-world task distributions, ensuring evaluations generalize\neffectively to real-world use cases. Extensive meta-evaluations show our\napproach effectively aligns benchmark samples with real-world task\ndistributions. Meanwhile, MixEval-X's model rankings correlate strongly with\nthat of crowd-sourced real-world evaluations (up to 0.98) while being much more\nefficient. We provide comprehensive leaderboards to rerank existing models and\norganizations and offer insights to enhance understanding of multi-modal\nevaluations and inform future research.\n","authors":["Jinjie Ni","Yifan Song","Deepanway Ghosal","Bo Li","David Junhao Zhang","Xiang Yue","Fuzhao Xue","Zian Zheng","Kaichen Zhang","Mahir Shah","Kabir Jain","Yang You","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2410.13754v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14154v1","updated":"2024-10-18T03:45:19Z","published":"2024-10-18T03:45:19Z","title":"RA-BLIP: Multimodal Adaptive Retrieval-Augmented Bootstrapping\n  Language-Image Pre-training","summary":"  Multimodal Large Language Models (MLLMs) have recently received substantial\ninterest, which shows their emerging potential as general-purpose models for\nvarious vision-language tasks. MLLMs involve significant external knowledge\nwithin their parameters; however, it is challenging to continually update these\nmodels with the latest knowledge, which involves huge computational costs and\npoor interpretability. Retrieval augmentation techniques have proven to be\neffective plugins for both LLMs and MLLMs. In this study, we propose multimodal\nadaptive Retrieval-Augmented Bootstrapping Language-Image Pre-training\n(RA-BLIP), a novel retrieval-augmented framework for various MLLMs. Considering\nthe redundant information within vision modality, we first leverage the\nquestion to instruct the extraction of visual information through interactions\nwith one set of learnable queries, minimizing irrelevant interference during\nretrieval and generation. Besides, we introduce a pre-trained multimodal\nadaptive fusion module to achieve question text-to-multimodal retrieval and\nintegration of multimodal knowledge by projecting visual and language\nmodalities into a unified semantic space. Furthermore, we present an Adaptive\nSelection Knowledge Generation (ASKG) strategy to train the generator to\nautonomously discern the relevance of retrieved knowledge, which realizes\nexcellent denoising performance. Extensive experiments on open multimodal\nquestion-answering datasets demonstrate that RA-BLIP achieves significant\nperformance and surpasses the state-of-the-art retrieval-augmented models.\n","authors":["Muhe Ding","Yang Ma","Pengda Qin","Jianlong Wu","Yuhong Li","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2410.14154v1.pdf","comment":"10 pages, 6 figures, Journal"},{"id":"http://arxiv.org/abs/2402.07640v3","updated":"2024-10-18T02:50:53Z","published":"2024-02-12T13:27:22Z","title":"Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image\n  Data","summary":"  The ability to generate sentiment-controlled feedback in response to\nmultimodal inputs comprising text and images addresses a critical gap in\nhuman-computer interaction. This capability allows systems to provide\nempathetic, accurate, and engaging responses, with useful applications in\neducation, healthcare, marketing, and customer service. To this end, we have\nconstructed a large-scale Controllable Multimodal Feedback Synthesis (CMFeed)\ndataset and propose a controllable feedback synthesis system. The system\nfeatures an encoder, decoder, and controllability block for textual and visual\ninputs. It extracts features using a transformer and Faster R-CNN networks,\ncombining them to generate feedback. The CMFeed dataset includes images, texts,\nreactions to the posts, human comments with relevance scores, and reactions to\nthese comments. These reactions train the model to produce feedback with\nspecified sentiments, achieving a sentiment classification accuracy of 77.23\\%,\nwhich is 18.82\\% higher than the accuracy without controllability. The system\nalso incorporates a similarity module for assessing feedback relevance through\nrank-based metrics and an interpretability technique to analyze the\ncontributions of textual and visual features during feedback generation. Access\nto the CMFeed dataset and the system's code is available at\nhttps://github.com/MIntelligence-Group/CMFeed.\n","authors":["Puneet Kumar","Sarthak Malik","Balasubramanian Raman","Xiaobai Li"],"pdf_url":"https://arxiv.org/pdf/2402.07640v3.pdf","comment":null}]},"2024-10-17T00:00:00Z":{"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2410.14089v1","updated":"2024-10-17T23:52:39Z","published":"2024-10-17T23:52:39Z","title":"MMAD-Purify: A Precision-Optimized Framework for Efficient and Scalable\n  Multi-Modal Attacks","summary":"  Neural networks have achieved remarkable performance across a wide range of\ntasks, yet they remain susceptible to adversarial perturbations, which pose\nsignificant risks in safety-critical applications. With the rise of\nmultimodality, diffusion models have emerged as powerful tools not only for\ngenerative tasks but also for various applications such as image editing,\ninpainting, and super-resolution. However, these models still lack robustness\ndue to limited research on attacking them to enhance their resilience.\nTraditional attack techniques, such as gradient-based adversarial attacks and\ndiffusion model-based methods, are hindered by computational inefficiencies and\nscalability issues due to their iterative nature. To address these challenges,\nwe introduce an innovative framework that leverages the distilled backbone of\ndiffusion models and incorporates a precision-optimized noise predictor to\nenhance the effectiveness of our attack framework. This approach not only\nenhances the attack's potency but also significantly reduces computational\ncosts. Our framework provides a cutting-edge solution for multi-modal\nadversarial attacks, ensuring reduced latency and the generation of\nhigh-fidelity adversarial examples with superior success rates. Furthermore, we\ndemonstrate that our framework achieves outstanding transferability and\nrobustness against purification defenses, outperforming existing gradient-based\nattack models in both effectiveness and efficiency.\n","authors":["Xinxin Liu","Zhongliang Guo","Siyuan Huang","Chun Pong Lau"],"pdf_url":"https://arxiv.org/pdf/2410.14089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14087v1","updated":"2024-10-17T23:37:58Z","published":"2024-10-17T23:37:58Z","title":"Your Interest, Your Summaries: Query-Focused Long Video Summarization","summary":"  Generating a concise and informative video summary from a long video is\nimportant, yet subjective due to varying scene importance. Users' ability to\nspecify scene importance through text queries enhances the relevance of such\nsummaries. This paper introduces an approach for query-focused video\nsummarization, aiming to align video summaries closely with user queries. To\nthis end, we propose the Fully Convolutional Sequence Network with Attention\n(FCSNA-QFVS), a novel approach designed for this task. Leveraging temporal\nconvolutional and attention mechanisms, our model effectively extracts and\nhighlights relevant content based on user-specified queries. Experimental\nvalidation on a benchmark dataset for query-focused video summarization\ndemonstrates the effectiveness of our approach.\n","authors":["Nirav Patel","Payal Prajapati","Maitrik Shah"],"pdf_url":"https://arxiv.org/pdf/2410.14087v1.pdf","comment":"To appear at the 18th International Conference on Control,\n  Automation, Robotics and Vision (ICARCV), December 2024, Dubai, UAE"},{"id":"http://arxiv.org/abs/2410.14084v1","updated":"2024-10-17T23:26:55Z","published":"2024-10-17T23:26:55Z","title":"Self Supervised Deep Learning for Robot Grasping","summary":"  Learning Based Robot Grasping currently involves the use of labeled data.\nThis approach has two major disadvantages. Firstly, labeling data for grasp\npoints and angles is a strenuous process, so the dataset remains limited.\nSecondly, human labeling is prone to bias due to semantics.\n  In order to solve these problems we propose a simpler self-supervised robotic\nsetup, that will train a Convolutional Neural Network (CNN). The robot will\nlabel and collect the data during the training process. The idea is to make a\nrobot that is less costly, small and easily maintainable in a lab setup. The\nrobot will be trained on a large data set for several hundred hours and then\nthe trained Neural Network can be mapped onto a larger grasping robot.\n","authors":["Danyal Saqib","Wajahat Hussain"],"pdf_url":"https://arxiv.org/pdf/2410.14084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14083v1","updated":"2024-10-17T23:23:48Z","published":"2024-10-17T23:23:48Z","title":"SAMReg: SAM-enabled Image Registration with ROI-based Correspondence","summary":"  This paper describes a new spatial correspondence representation based on\npaired regions-of-interest (ROIs), for medical image registration. The distinct\nproperties of the proposed ROI-based correspondence are discussed, in the\ncontext of potential benefits in clinical applications following image\nregistration, compared with alternative correspondence-representing approaches,\nsuch as those based on sampled displacements and spatial transformation\nfunctions. These benefits include a clear connection between learning-based\nimage registration and segmentation, which in turn motivates two cases of image\nregistration approaches using (pre-)trained segmentation networks. Based on the\nsegment anything model (SAM), a vision foundation model for segmentation, we\ndevelop a new registration algorithm SAMReg, which does not require any\ntraining (or training data), gradient-based fine-tuning or prompt engineering.\nThe proposed SAMReg models are evaluated across five real-world applications,\nincluding intra-subject registration tasks with cardiac MR and lung CT,\nchallenging inter-subject registration scenarios with prostate MR and retinal\nimaging, and an additional evaluation with a non-clinical example with aerial\nimage registration. The proposed methods outperform both intensity-based\niterative algorithms and DDF-predicting learning-based networks across tested\nmetrics including Dice and target registration errors on anatomical structures,\nand further demonstrates competitive performance compared to weakly-supervised\nregistration approaches that rely on fully-segmented training data. Open source\ncode and examples are available at: https://github.com/sqhuang0103/SAMReg.git.\n","authors":["Shiqi Huang","Tingfa Xu","Ziyi Shen","Shaheer Ullah Saeed","Wen Yan","Dean Barratt","Yipeng Hu"],"pdf_url":"https://arxiv.org/pdf/2410.14083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10551v3","updated":"2024-10-17T23:07:02Z","published":"2024-10-14T14:32:05Z","title":"Preserving Cardiac Integrity: A Topology-Infused Approach to Whole Heart\n  Segmentation","summary":"  Whole heart segmentation (WHS) supports cardiovascular disease (CVD)\ndiagnosis, disease monitoring, treatment planning, and prognosis. Deep learning\nhas become the most widely used method for WHS applications in recent years.\nHowever, segmentation of whole-heart structures faces numerous challenges\nincluding heart shape variability during the cardiac cycle, clinical artifacts\nlike motion and poor contrast-to-noise ratio, domain shifts in multi-center\ndata, and the distinct modalities of CT and MRI. To address these limitations\nand improve segmentation quality, this paper introduces a new\ntopology-preserving module that is integrated into deep neural networks. The\nimplementation achieves anatomically plausible segmentation by using learned\ntopology-preserving fields, which are based entirely on 3D convolution and are\ntherefore very effective for 3D voxel data. We incorporate natural constraints\nbetween structures into the end-to-end training and enrich the feature\nrepresentation of the neural network. The effectiveness of the proposed method\nis validated on an open-source medical heart dataset, specifically using the\nWHS++ data. The results demonstrate that the architecture performs\nexceptionally well, achieving a Dice coefficient of 0.939 during testing. This\nindicates full topology preservation for individual structures and\nsignificantly outperforms other baselines in preserving the overall scene\ntopology.\n","authors":["Chenyu Zhang","Wenxue Guan","Xiaodan Xing","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10551v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09087v2","updated":"2024-10-17T23:02:17Z","published":"2024-06-13T13:13:17Z","title":"Suitability of KANs for Computer Vision: A preliminary investigation","summary":"  Kolmogorov-Arnold Networks (KANs) introduce a paradigm of neural modeling\nthat implements learnable functions on the edges of the networks, diverging\nfrom the traditional node-centric activations in neural networks. This work\nassesses the applicability and efficacy of KANs in visual modeling, focusing on\nfundamental recognition and segmentation tasks. We mainly analyze the\nperformance and efficiency of different network architectures built using KAN\nconcepts along with conventional building blocks of convolutional and linear\nlayers, enabling a comparative analysis with the conventional models. Our\nfindings are aimed at contributing to understanding the potential of KANs in\ncomputer vision, highlighting both their strengths and areas for further\nresearch. Our evaluation point toward the fact that while KAN-based\narchitectures perform in line with the original claims, it may often be\nimportant to employ more complex functions on the network edges to retain the\nperformance advantage of KANs on more complex visual data.\n","authors":["Basim Azam","Naveed Akhtar"],"pdf_url":"https://arxiv.org/pdf/2406.09087v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14072v1","updated":"2024-10-17T22:45:13Z","published":"2024-10-17T22:45:13Z","title":"Efficient Vision-Language Models by Summarizing Visual Tokens into\n  Compact Registers","summary":"  Recent advancements in vision-language models (VLMs) have expanded their\npotential for real-world applications, enabling these models to perform complex\nreasoning on images. In the widely used fully autoregressive transformer-based\nmodels like LLaVA, projected visual tokens are prepended to textual tokens.\nOftentimes, visual tokens are significantly more than prompt tokens, resulting\nin increased computational overhead during both training and inference. In this\npaper, we propose Visual Compact Token Registers (Victor), a method that\nreduces the number of visual tokens by summarizing them into a smaller set of\nregister tokens. Victor adds a few learnable register tokens after the visual\ntokens and summarizes the visual information into these registers using the\nfirst few layers in the language tower of VLMs. After these few layers, all\nvisual tokens are discarded, significantly improving computational efficiency\nfor both training and inference. Notably, our method is easy to implement and\nrequires a small number of new trainable parameters with minimal impact on\nmodel performance. In our experiment, with merely 8 visual registers--about 1%\nof the original tokens--Victor shows less than a 4% accuracy drop while\nreducing the total training time by 43% and boosting the inference throughput\nby 3.3X.\n","authors":["Yuxin Wen","Qingqing Cao","Qichen Fu","Sachin Mehta","Mahyar Najibi"],"pdf_url":"https://arxiv.org/pdf/2410.14072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14070v1","updated":"2024-10-17T22:36:52Z","published":"2024-10-17T22:36:52Z","title":"FaceSaliencyAug: Mitigating Geographic, Gender and Stereotypical Biases\n  via Saliency-Based Data Augmentation","summary":"  Geographical, gender and stereotypical biases in computer vision models pose\nsignificant challenges to their performance and fairness. {In this study, we\npresent an approach named FaceSaliencyAug aimed at addressing the gender bias\nin} {Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs).\nLeveraging the salient regions} { of faces detected by saliency, the propose\napproach mitigates geographical and stereotypical biases } {in the datasets.\nFaceSaliencyAug} randomly selects masks from a predefined search space and\napplies them to the salient region of face images, subsequently restoring the\noriginal image with masked salient region. {The proposed} augmentation strategy\nenhances data diversity, thereby improving model performance and debiasing\neffects. We quantify dataset diversity using Image Similarity Score (ISS)\nacross five datasets, including Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled\nFaces in the Wild (LFW), UTK Faces, and Diverse Dataset. The proposed approach\ndemonstrates superior diversity metrics, as evaluated by ISS-intra and\nISS-inter algorithms. Furthermore, we evaluate the effectiveness of our\napproach in mitigating gender bias on CEO, Engineer, Nurse, and School Teacher\ndatasets. We use the Image-Image Association Score (IIAS) to measure gender\nbias in these occupations. Our experiments reveal a reduction in gender bias\nfor both CNNs and ViTs, indicating the efficacy of our method in promoting\nfairness and inclusivity in computer vision models.\n","authors":["Teerath Kumar","Alessandra Mileo","Malika Bendechache"],"pdf_url":"https://arxiv.org/pdf/2410.14070v1.pdf","comment":"Accepted at Image Signal and Video processing"},{"id":"http://arxiv.org/abs/2410.14060v1","updated":"2024-10-17T22:06:34Z","published":"2024-10-17T22:06:34Z","title":"On Partial Prototype Collapse in the DINO Family of Self-Supervised\n  Methods","summary":"  A prominent self-supervised learning paradigm is to model the representations\nas clusters, or more generally as a mixture model. Learning to map the data\nsamples to compact representations and fitting the mixture model simultaneously\nleads to the representation collapse problem. Regularizing the distribution of\ndata points over the clusters is the prevalent strategy to avoid this issue.\nWhile this is sufficient to prevent full representation collapse, we show that\na partial prototype collapse problem still exists in the DINO family of\nmethods, that leads to significant redundancies in the prototypes. Such\nprototype redundancies serve as shortcuts for the method to achieve a marginal\nlatent class distribution that matches the prescribed prior. We show that by\nencouraging the model to use diverse prototypes, the partial prototype collapse\ncan be mitigated. Effective utilization of the prototypes enables the methods\nto learn more fine-grained clusters, encouraging more informative\nrepresentations. We demonstrate that this is especially beneficial when\npre-training on a long-tailed fine-grained dataset.\n","authors":["Hariprasath Govindarajan","Per Sidén","Jacob Roll","Fredrik Lindsten"],"pdf_url":"https://arxiv.org/pdf/2410.14060v1.pdf","comment":"First version of the paper appeared in OpenReview on 22 Sep 2023.\n  Accepted to BMVC 2024"},{"id":"http://arxiv.org/abs/2111.14259v5","updated":"2024-10-17T21:57:15Z","published":"2021-11-28T22:58:00Z","title":"Performance of a GPU- and Time-Efficient Pseudo 3D Network for Magnetic\n  Resonance Image Super-Resolution and Motion Artifact Reduction","summary":"  Shortening acquisition time and reducing motion artifacts are the most\ncritical challenges in magnetic resonance imaging (MRI). Deep learning-based\nimage restoration has emerged as a promising solution capable of generating\nhigh-resolution and motion-artifact-free MRI images from low-resolution images\nacquired with shortened acquisition times or from motion-artifact-corrupted\nimages. To facilitate clinical integration, a time- and GPU-efficient network\nwith reliable accuracy is essential. In this study, we adopted a unified 2D\ndeep learning framework for pseudo-3D MRI image super-resolution reconstruction\n(SRR) and motion artifact reduction (MAR). The optimal down-sampling factors to\noptimize the acquisition time in SRR were identified. Training for MAR was\nperformed using publicly available in vivo data, employing a novel standardized\nmethod to induce motion artifacts of varying severity in a controlled way. The\naccuracy of the network was evaluated through a pixel-wise uncertainty map, and\nperformance was benchmarked against state-of-the-art methods. The results\ndemonstrated that the down-sampling factor of 1x1x2 for x2 acceleration and\n2x2x2 for x4 acceleration was optimal. For SRR, the proposed TS-RCAN\noutperformed the 3D networks of mDCSRN and ReCNN, with an improvement of more\nthan 0.01 in SSIM and 1.5 dB in PSNR while reducing GPU load by up to and\ninference time by up to 90%. For MAR, TS-RCAN exceeded UNet's performance by up\nto 0.014 in SSIM and 1.48 dB in PSNR. Additionally, TS-RCAN provided\nuncertainty information, which can be used to estimate the quality of the\nreconstructed images. TS-RCAN has potential use for SRR and MAR in the clinical\nsetting.\n","authors":["Hao Li","Jianan Liu","Marianne Schell","Tao Huang","Arne Lauer","Katharina Schregel","Jessica Jesser","Dominik F Vollherbst","Martin Bendszus","Sabine Heiland","Tim Hilgenfeld"],"pdf_url":"https://arxiv.org/pdf/2111.14259v5.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.14050v1","updated":"2024-10-17T21:46:00Z","published":"2024-10-17T21:46:00Z","title":"Learning Multimodal Cues of Children's Uncertainty","summary":"  Understanding uncertainty plays a critical role in achieving common ground\n(Clark et al.,1983). This is especially important for multimodal AI systems\nthat collaborate with users to solve a problem or guide the user through a\nchallenging concept. In this work, for the first time, we present a dataset\nannotated in collaboration with developmental and cognitive psychologists for\nthe purpose of studying nonverbal cues of uncertainty. We then present an\nanalysis of the data, studying different roles of uncertainty and its\nrelationship with task difficulty and performance. Lastly, we present a\nmultimodal machine learning model that can predict uncertainty given a\nreal-time video clip of a participant, which we find improves upon a baseline\nmultimodal transformer model. This work informs research on cognitive\ncoordination between human-human and human-AI and has broad implications for\ngesture understanding and generation. The anonymized version of our data and\ncode will be publicly available upon the completion of the required consent\nforms and data sheets.\n","authors":["Qi Cheng","Mert İnan","Rahma Mbarki","Grace Grmek","Theresa Choi","Yiming Sun","Kimele Persaud","Jenny Wang","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2410.14050v1.pdf","comment":"SIGDIAL 2023"},{"id":"http://arxiv.org/abs/2410.14045v1","updated":"2024-10-17T21:37:40Z","published":"2024-10-17T21:37:40Z","title":"Human Action Anticipation: A Survey","summary":"  Predicting future human behavior is an increasingly popular topic in computer\nvision, driven by the interest in applications such as autonomous vehicles,\ndigital assistants and human-robot interactions. The literature on behavior\nprediction spans various tasks, including action anticipation, activity\nforecasting, intent prediction, goal prediction, and so on. Our survey aims to\ntie together this fragmented literature, covering recent technical innovations\nas well as the development of new large-scale datasets for model training and\nevaluation. We also summarize the widely-used metrics for different tasks and\nprovide a comprehensive performance comparison of existing approaches on eleven\naction anticipation datasets. This survey serves as not only a reference for\ncontemporary methodologies in action anticipation, but also a guideline for\nfuture research direction of this evolving landscape.\n","authors":["Bolin Lai","Sam Toyer","Tushar Nagarajan","Rohit Girdhar","Shengxin Zha","James M. Rehg","Kris Kitani","Kristen Grauman","Ruta Desai","Miao Liu"],"pdf_url":"https://arxiv.org/pdf/2410.14045v1.pdf","comment":"30 pages, 9 figures, 12 tables"},{"id":"http://arxiv.org/abs/2406.11579v2","updated":"2024-10-17T21:05:16Z","published":"2024-06-17T14:16:12Z","title":"Duoduo CLIP: Efficient 3D Understanding with Multi-View Images","summary":"  We introduce Duoduo CLIP, a model for 3D representation learning that learns\nshape encodings from multi-view images instead of point-clouds. The choice of\nmulti-view images allows us to leverage 2D priors from off-the-shelf CLIP\nmodels to facilitate fine-tuning with 3D data. Our approach not only shows\nbetter generalization compared to existing point cloud methods, but also\nreduces GPU requirements and training time. In addition, the model is modified\nwith cross-view attention to leverage information across multiple frames of the\nobject which further boosts performance. Notably, our model is permutation\ninvariant to the order of multi-view images while being pose-free. Compared to\nthe current SOTA point cloud method that requires 480 A100 hours to train 1\nbillion model parameters we only require 57 A5000 hours and 87 million\nparameters. Multi-view images also provide more flexibility including being\nable to encode objects with a variable number of images, and performance scales\nwhen more views are used. In contrast, point cloud based methods require an\nentire scan or model of the object. We showcase this flexibility with\nbenchmarks from images of real-world objects. Our model also achieves better\nperformance in more fine-grained text to shape retrieval, demonstrating better\ntext-and-shape alignment than point cloud based models.\n","authors":["Han-Hung Lee","Yiming Zhang","Angel X. Chang"],"pdf_url":"https://arxiv.org/pdf/2406.11579v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03665v2","updated":"2024-10-17T20:51:19Z","published":"2024-10-04T17:59:57Z","title":"Estimating Body and Hand Motion in an Ego-sensed World","summary":"  We present EgoAllo, a system for human motion estimation from a head-mounted\ndevice. Using only egocentric SLAM poses and images, EgoAllo guides sampling\nfrom a conditional diffusion model to estimate 3D body pose, height, and hand\nparameters that capture the wearer's actions in the allocentric coordinate\nframe of the scene. To achieve this, our key insight is in representation: we\npropose spatial and temporal invariance criteria for improving model\nperformance, from which we derive a head motion conditioning parameterization\nthat improves estimation by up to 18%. We also show how the bodies estimated by\nour system can improve the hands: the resulting kinematic and temporal\nconstraints result in over 40% lower hand estimation errors compared to noisy\nmonocular estimates. Project page: https://egoallo.github.io/\n","authors":["Brent Yi","Vickie Ye","Maya Zheng","Lea Müller","Georgios Pavlakos","Yi Ma","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2410.03665v2.pdf","comment":"v2: fixed figures for Safari, typos"},{"id":"http://arxiv.org/abs/2410.14020v1","updated":"2024-10-17T20:46:13Z","published":"2024-10-17T20:46:13Z","title":"Segmentation of Pediatric Brain Tumors using a Radiologically informed,\n  Deep Learning Cascade","summary":"  Monitoring of Diffuse Intrinsic Pontine Glioma (DIPG) and Diffuse Midline\nGlioma (DMG) brain tumors in pediatric patients is key for assessment of\ntreatment response. Response Assessment in Pediatric Neuro-Oncology (RAPNO)\nguidelines recommend the volumetric measurement of these tumors using MRI.\nSegmentation challenges, such as the Brain Tumor Segmentation (BraTS)\nChallenge, promote development of automated approaches which are replicable,\ngeneralizable and accurate, to aid in these tasks. The current study presents a\nnovel adaptation of existing nnU-Net approaches for pediatric brain tumor\nsegmentation, submitted to the BraTS-PEDs 2024 challenge. We apply an adapted\nnnU-Net with hierarchical cascades to the segmentation task of the BraTS-PEDs\n2024 challenge. The residual encoder variant of nnU-Net, used as our baseline\nmodel, already provides high quality segmentations. We incorporate multiple\nchanges to the implementation of nnU-Net and devise a novel two-stage cascaded\nnnU-Net to segment the substructures of brain tumors from coarse to fine. Using\noutputs from the nnU-Net Residual Encoder (trained to segment CC, ED, ET and\nNET tumor labels from T1w, T1w-CE, T2w and T2-FLAIR MRI), these are passed to\ntwo additional models one classifying ET versus NET and a second classifying CC\nvs ED using cascade learning. We use radiological guidelines to steer which\nmulti parametric MRI (mpMRI) to use in these cascading models. Compared to a\ndefault nnU-Net and an ensembled nnU-net as baseline approaches, our novel\nmethod provides robust segmentations for the BraTS-PEDs 2024 challenge,\nachieving mean Dice scores of 0.657, 0.904, 0.703, and 0.967, and HD95 of 76.2,\n10.1, 111.0, and 12.3 for the ET, NET, CC and ED, respectively.\n","authors":["Timothy Mulvany","Daniel Griffiths-King","Jan Novak","Heather Rose"],"pdf_url":"https://arxiv.org/pdf/2410.14020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14017v1","updated":"2024-10-17T20:32:43Z","published":"2024-10-17T20:32:43Z","title":"Probabilistic U-Net with Kendall Shape Spaces for Geometry-Aware\n  Segmentations of Images","summary":"  One of the fundamental problems in computer vision is image segmentation, the\ntask of detecting distinct regions or objects in given images. Deep Neural\nNetworks (DNN) have been shown to be very effective in segmenting challenging\nimages, producing convincing segmentations. There is further need for\nprobabilistic DNNs that can reflect the uncertainties from the input images and\nthe models into the computed segmentations, in other words, new DNNs that can\ngenerate multiple plausible segmentations and their distributions depending on\nthe input or the model uncertainties. While there are existing probabilistic\nsegmentation models, many of them do not take into account the geometry or\nshape underlying the segmented regions. In this paper, we propose a\nprobabilistic image segmentation model that can incorporate the geometry of a\nsegmentation. Our proposed model builds on the Probabilistic U-Net of\n\\cite{kohl2018probabilistic} to generate probabilistic segmentations, i.e.\\!\nmultiple likely segmentations for an input image. Our model also adopts the\nKendall Shape Variational Auto-Encoder of \\cite{vadgama2023kendall} to encode a\nKendall shape space in the latent variable layers of the prior and posterior\nnetworks of the Probabilistic U-Net. Incorporating the shape space in this\nmanner leads to a more robust segmentation with spatially coherent regions,\nrespecting the underlying geometry in the input images.\n","authors":["Jiyoung Park","Günay Doğan"],"pdf_url":"https://arxiv.org/pdf/2410.14017v1.pdf","comment":"22 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.13989v1","updated":"2024-10-17T19:41:34Z","published":"2024-10-17T19:41:34Z","title":"Reproducibility study of \"LICO: Explainable Models with Language-Image\n  Consistency\"","summary":"  The growing reproducibility crisis in machine learning has brought forward a\nneed for careful examination of research findings. This paper investigates the\nclaims made by Lei et al. (2023) regarding their proposed method, LICO, for\nenhancing post-hoc interpretability techniques and improving image\nclassification performance. LICO leverages natural language supervision from a\nvision-language model to enrich feature representations and guide the learning\nprocess. We conduct a comprehensive reproducibility study, employing (Wide)\nResNets and established interpretability methods like Grad-CAM and RISE. We\nwere mostly unable to reproduce the authors' results. In particular, we did not\nfind that LICO consistently led to improved classification performance or\nimprovements in quantitative and qualitative measures of interpretability.\nThus, our findings highlight the importance of rigorous evaluation and\ntransparent reporting in interpretability research.\n","authors":["Luan Fletcher","Robert van der Klis","Martin Sedláček","Stefan Vasilev","Christos Athanasiadis"],"pdf_url":"https://arxiv.org/pdf/2410.13989v1.pdf","comment":"15 pages, 2 figures, Machine Learning Reproducibility Challenge 2024"},{"id":"http://arxiv.org/abs/2309.17329v3","updated":"2024-10-17T19:23:45Z","published":"2023-09-29T15:40:58Z","title":"Efficient Anatomical Labeling of Pulmonary Tree Structures via Deep\n  Point-Graph Representation-based Implicit Fields","summary":"  Pulmonary diseases rank prominently among the principal causes of death\nworldwide. Curing them will require, among other things, a better understanding\nof the complex 3D tree-shaped structures within the pulmonary system, such as\nairways, arteries, and veins. Traditional approaches using high-resolution\nimage stacks and standard CNNs on dense voxel grids face challenges in\ncomputational efficiency, limited resolution, local context, and inadequate\npreservation of shape topology. Our method addresses these issues by shifting\nfrom dense voxel to sparse point representation, offering better memory\nefficiency and global context utilization. However, the inherent sparsity in\npoint representation can lead to a loss of crucial connectivity in tree-shaped\nstructures. To mitigate this, we introduce graph learning on skeletonized\nstructures, incorporating differentiable feature fusion for improved topology\nand long-distance context capture. Furthermore, we employ an implicit function\nfor efficient conversion of sparse representations into dense reconstructions\nend-to-end. The proposed method not only delivers state-of-the-art performance\nin labeling accuracy, both overall and at key locations, but also enables\nefficient inference and the generation of closed surface shapes. Addressing\ndata scarcity in this field, we have also curated a comprehensive dataset to\nvalidate our approach. Data and code are available at\n\\url{https://github.com/M3DV/pulmonary-tree-labeling}.\n","authors":["Kangxian Xie","Jiancheng Yang","Donglai Wei","Ziqiao Weng","Pascal Fua"],"pdf_url":"https://arxiv.org/pdf/2309.17329v3.pdf","comment":"Accepted by Medical Image Analysis"},{"id":"http://arxiv.org/abs/2406.01029v4","updated":"2024-10-17T19:05:50Z","published":"2024-06-03T06:24:55Z","title":"CYCLO: Cyclic Graph Transformer Approach to Multi-Object Relationship\n  Modeling in Aerial Videos","summary":"  Video scene graph generation (VidSGG) has emerged as a transformative\napproach to capturing and interpreting the intricate relationships among\nobjects and their temporal dynamics in video sequences. In this paper, we\nintroduce the new AeroEye dataset that focuses on multi-object relationship\nmodeling in aerial videos. Our AeroEye dataset features various drone scenes\nand includes a visually comprehensive and precise collection of predicates that\ncapture the intricate relationships and spatial arrangements among objects. To\nthis end, we propose the novel Cyclic Graph Transformer (CYCLO) approach that\nallows the model to capture both direct and long-range temporal dependencies by\ncontinuously updating the history of interactions in a circular manner. The\nproposed approach also allows one to handle sequences with inherent cyclical\npatterns and process object relationships in the correct sequential order.\nTherefore, it can effectively capture periodic and overlapping relationships\nwhile minimizing information loss. The extensive experiments on the AeroEye\ndataset demonstrate the effectiveness of the proposed CYCLO model,\ndemonstrating its potential to perform scene understanding on drone videos.\nFinally, the CYCLO method consistently achieves State-of-the-Art (SOTA) results\non two in-the-wild scene graph generation benchmarks, i.e., PVSG and ASPIRe.\n","authors":["Trong-Thuan Nguyen","Pha Nguyen","Xin Li","Jackson Cothren","Alper Yilmaz","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2406.01029v4.pdf","comment":"Accepted to NeurIPS 2024"},{"id":"http://arxiv.org/abs/2410.13976v1","updated":"2024-10-17T19:02:31Z","published":"2024-10-17T19:02:31Z","title":"Debiasing Large Vision-Language Models by Ablating Protected Attribute\n  Representations","summary":"  Large Vision Language Models (LVLMs) such as LLaVA have demonstrated\nimpressive capabilities as general-purpose chatbots that can engage in\nconversations about a provided input image. However, their responses are\ninfluenced by societal biases present in their training datasets, leading to\nundesirable differences in how the model responds when presented with images\ndepicting people of different demographics. In this work, we propose a novel\ndebiasing framework for LVLMs by directly ablating biased attributes during\ntext generation to avoid generating text related to protected attributes, or\neven representing them internally. Our method requires no training and a\nrelatively small amount of representative biased outputs (~1000 samples). Our\nexperiments show that not only can we can minimize the propensity of LVLMs to\ngenerate text related to protected attributes, but we can even use synthetic\ndata to inform the ablation while retaining captioning performance on real data\nsuch as COCO. Furthermore, we find the resulting generations from a debiased\nLVLM exhibit similar accuracy as a baseline biased model, showing that\ndebiasing effects can be achieved without sacrificing model performance.\n","authors":["Neale Ratzlaff","Matthew Lyle Olson","Musashi Hinck","Shao-Yen Tseng","Vasudev Lal","Phillip Howard"],"pdf_url":"https://arxiv.org/pdf/2410.13976v1.pdf","comment":"NeurIPS workshop on SafeGenAI, 10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.13952v1","updated":"2024-10-17T18:22:50Z","published":"2024-10-17T18:22:50Z","title":"Satellite Streaming Video QoE Prediction: A Real-World Subjective\n  Database and Network-Level Prediction Models","summary":"  Demand for streaming services, including satellite, continues to exhibit\nunprecedented growth. Internet Service Providers find themselves at the\ncrossroads of technological advancements and rising customer expectations. To\nstay relevant and competitive, these ISPs must ensure their networks deliver\noptimal video streaming quality, a key determinant of user satisfaction.\nTowards this end, it is important to have accurate Quality of Experience\nprediction models in place. However, achieving robust performance by these\nmodels requires extensive data sets labeled by subjective opinion scores on\nvideos impaired by diverse playback disruptions. To bridge this data gap, we\nintroduce the LIVE-Viasat Real-World Satellite QoE Database. This database\nconsists of 179 videos recorded from real-world streaming services affected by\nvarious authentic distortion patterns. We also conducted a comprehensive\nsubjective study involving 54 participants, who contributed both\ncontinuous-time opinion scores and endpoint (retrospective) QoE scores. Our\nanalysis sheds light on various determinants influencing subjective QoE, such\nas stall events, spatial resolutions, bitrate, and certain network parameters.\nWe demonstrate the usefulness of this unique new resource by evaluating the\nefficacy of prevalent QoE-prediction models on it. We also created a new model\nthat maps the network parameters to predicted human perception scores, which\ncan be used by ISPs to optimize the video streaming quality of their networks.\nOur proposed model, which we call SatQA, is able to accurately predict QoE\nusing only network parameters, without any access to pixel data or\nvideo-specific metadata, estimated by Spearman's Rank Order Correlation\nCoefficient (SROCC), Pearson Linear Correlation Coefficient (PLCC), and Root\nMean Squared Error (RMSE), indicating high accuracy and reliability.\n","authors":["Bowen Chen","Zaixi Shang","Jae Won Chung","David Lerner","Werner Robitza","Rakesh Rao Ramachandra Rao","Alexander Raake","Alan C. Bovik"],"pdf_url":"https://arxiv.org/pdf/2410.13952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13863v1","updated":"2024-10-17T17:59:59Z","published":"2024-10-17T17:59:59Z","title":"Fluid: Scaling Autoregressive Text-to-image Generative Models with\n  Continuous Tokens","summary":"  Scaling up autoregressive models in vision has not proven as beneficial as in\nlarge language models. In this work, we investigate this scaling problem in the\ncontext of text-to-image generation, focusing on two critical factors: whether\nmodels use discrete or continuous tokens, and whether tokens are generated in a\nrandom or fixed raster order using BERT- or GPT-like transformer architectures.\nOur empirical results show that, while all models scale effectively in terms of\nvalidation loss, their evaluation performance -- measured by FID, GenEval\nscore, and visual quality -- follows different trends. Models based on\ncontinuous tokens achieve significantly better visual quality than those using\ndiscrete tokens. Furthermore, the generation order and attention mechanisms\nsignificantly affect the GenEval score: random-order models achieve notably\nbetter GenEval scores compared to raster-order models. Inspired by these\nfindings, we train Fluid, a random-order autoregressive model on continuous\ntokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16\non MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our\nfindings and results will encourage future efforts to further bridge the\nscaling gap between vision and language models.\n","authors":["Lijie Fan","Tianhong Li","Siyang Qin","Yuanzhen Li","Chen Sun","Michael Rubinstein","Deqing Sun","Kaiming He","Yonglong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.13863v1.pdf","comment":"Tech report"},{"id":"http://arxiv.org/abs/2410.13864v1","updated":"2024-10-17T17:59:59Z","published":"2024-10-17T17:59:59Z","title":"UniDrive: Towards Universal Driving Perception Across Camera\n  Configurations","summary":"  Vision-centric autonomous driving has demonstrated excellent performance with\neconomical sensors. As the fundamental step, 3D perception aims to infer 3D\ninformation from 2D images based on 3D-2D projection. This makes driving\nperception models susceptible to sensor configuration (e.g., camera intrinsics\nand extrinsics) variations. However, generalizing across camera configurations\nis important for deploying autonomous driving models on different car models.\nIn this paper, we present UniDrive, a novel framework for vision-centric\nautonomous driving to achieve universal perception across camera\nconfigurations. We deploy a set of unified virtual cameras and propose a\nground-aware projection method to effectively transform the original images\ninto these unified virtual views. We further propose a virtual configuration\noptimization method by minimizing the expected projection error between\noriginal cameras and virtual cameras. The proposed virtual camera projection\ncan be applied to existing 3D perception methods as a plug-and-play module to\nmitigate the challenges posed by camera parameter variability, resulting in\nmore adaptable and reliable driving perception models. To evaluate the\neffectiveness of our framework, we collect a dataset on Carla by driving the\nsame routes while only modifying the camera configurations. Experimental\nresults demonstrate that our method trained on one specific camera\nconfiguration can generalize to varying configurations with minor performance\ndegradation.\n","authors":["Ye Li","Wenzhao Zheng","Xiaonan Huang","Kurt Keutzer"],"pdf_url":"https://arxiv.org/pdf/2410.13864v1.pdf","comment":"Preprint; 14 pages, 5 figures, 2 tables; Code at\n  https://github.com/ywyeli/UniDrive"},{"id":"http://arxiv.org/abs/2410.13862v1","updated":"2024-10-17T17:59:58Z","published":"2024-10-17T17:59:58Z","title":"DepthSplat: Connecting Gaussian Splatting and Depth","summary":"  Gaussian splatting and single/multi-view depth estimation are typically\nstudied in isolation. In this paper, we present DepthSplat to connect Gaussian\nsplatting and depth estimation and study their interactions. More specifically,\nwe first contribute a robust multi-view depth model by leveraging pre-trained\nmonocular depth features, leading to high-quality feed-forward 3D Gaussian\nsplatting reconstructions. We also show that Gaussian splatting can serve as an\nunsupervised pre-training objective for learning powerful depth models from\nlarge-scale unlabelled datasets. We validate the synergy between Gaussian\nsplatting and depth estimation through extensive ablation and cross-task\ntransfer experiments. Our DepthSplat achieves state-of-the-art performance on\nScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and\nnovel view synthesis, demonstrating the mutual benefits of connecting both\ntasks. Our code, models, and video results are available at\nhttps://haofeixu.github.io/depthsplat/.\n","authors":["Haofei Xu","Songyou Peng","Fangjinhua Wang","Hermann Blum","Daniel Barath","Andreas Geiger","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2410.13862v1.pdf","comment":"Project page: https://haofeixu.github.io/depthsplat/"},{"id":"http://arxiv.org/abs/2410.13861v1","updated":"2024-10-17T17:59:57Z","published":"2024-10-17T17:59:57Z","title":"PUMA: Empowering Unified MLLM with Multi-granular Visual Generation","summary":"  Recent advancements in multimodal foundation models have yielded significant\nprogress in vision-language understanding. Initial attempts have also explored\nthe potential of multimodal large language models (MLLMs) for visual content\ngeneration. However, existing works have insufficiently addressed the varying\ngranularity demands of different image generation tasks within a unified MLLM\nparadigm - from the diversity required in text-to-image generation to the\nprecise controllability needed in image manipulation. In this work, we propose\nPUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA\nunifies multi-granular visual features as both inputs and outputs of MLLMs,\nelegantly addressing the different granularity requirements of various image\ngeneration tasks within a unified MLLM framework. Following multimodal\npretraining and task-specific instruction tuning, PUMA demonstrates proficiency\nin a wide range of multimodal tasks. This work represents a significant step\ntowards a truly unified MLLM capable of adapting to the granularity demands of\nvarious visual tasks. The code and model will be released in\nhttps://github.com/rongyaofang/PUMA.\n","authors":["Rongyao Fang","Chengqi Duan","Kun Wang","Hao Li","Hao Tian","Xingyu Zeng","Rui Zhao","Jifeng Dai","Hongsheng Li","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2410.13861v1.pdf","comment":"Project page: https://rongyaofang.github.io/puma/"},{"id":"http://arxiv.org/abs/2410.13860v1","updated":"2024-10-17T17:59:55Z","published":"2024-10-17T17:59:55Z","title":"VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding","summary":"  3D visual grounding is crucial for robots, requiring integration of natural\nlanguage and 3D scene understanding. Traditional methods depending on\nsupervised learning with 3D point clouds are limited by scarce datasets.\nRecently zero-shot methods leveraging LLMs have been proposed to address the\ndata issue. While effective, these methods only use object-centric information,\nlimiting their ability to handle complex queries. In this work, we present\nVLM-Grounder, a novel framework using vision-language models (VLMs) for\nzero-shot 3D visual grounding based solely on 2D images. VLM-Grounder\ndynamically stitches image sequences, employs a grounding and feedback scheme\nto find the target object, and uses a multi-view ensemble projection to\naccurately estimate 3D bounding boxes. Experiments on ScanRefer and Nr3D\ndatasets show VLM-Grounder outperforms previous zero-shot methods, achieving\n51.6% Acc@0.25 on ScanRefer and 48.0% Acc on Nr3D, without relying on 3D\ngeometry or object priors. Codes are available at\nhttps://github.com/OpenRobotLab/VLM-Grounder .\n","authors":["Runsen Xu","Zhiwei Huang","Tai Wang","Yilun Chen","Jiangmiao Pang","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2410.13860v1.pdf","comment":"CoRL 2024 Camera Ready. 25 pages. A novel zero-shot 3D visual\n  grounding framework based solely on 2D images"},{"id":"http://arxiv.org/abs/2410.13859v1","updated":"2024-10-17T17:59:53Z","published":"2024-10-17T17:59:53Z","title":"$γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large\n  Language Models","summary":"  Despite the significant progress in multimodal large language models (MLLMs),\ntheir high computational cost remains a barrier to real-world deployment.\nInspired by the mixture of depths (MoDs) in natural language processing, we aim\nto address this limitation from the perspective of ``activated tokens''. Our\nkey insight is that if most tokens are redundant for the layer computation,\nthen can be skipped directly via the MoD layer. However, directly converting\nthe dense layers of MLLMs to MoD layers leads to substantial performance\ndegradation. To address this issue, we propose an innovative MoD adaptation\nstrategy for existing MLLMs called $\\gamma$-MoD. In $\\gamma$-MoD, a novel\nmetric is proposed to guide the deployment of MoDs in the MLLM, namely rank of\nattention maps (ARank). Through ARank, we can effectively identify which layer\nis redundant and should be replaced with the MoD layer. Based on ARank, we\nfurther propose two novel designs to maximize the computational sparsity of\nMLLM while maintaining its performance, namely shared vision-language router\nand masked routing learning. With these designs, more than 90% dense layers of\nthe MLLM can be effectively converted to the MoD ones. To validate our method,\nwe apply it to three popular MLLMs, and conduct extensive experiments on 9\nbenchmark datasets. Experimental results not only validate the significant\nefficiency benefit of $\\gamma$-MoD to existing MLLMs but also confirm its\ngeneralization ability on various MLLMs. For example, with a minor performance\ndrop, i.e., -1.5%, $\\gamma$-MoD can reduce the training and inference time of\nLLaVA-HR by 31.0% and 53.2%, respectively.\n","authors":["Yaxin Luo","Gen Luo","Jiayi Ji","Yiyi Zhou","Xiaoshuai Sun","Zhiqiang Shen","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2410.13859v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13854v1","updated":"2024-10-17T17:59:24Z","published":"2024-10-17T17:59:24Z","title":"Can MLLMs Understand the Deep Implication Behind Chinese Images?","summary":"  As the capabilities of Multimodal Large Language Models (MLLMs) continue to\nimprove, the need for higher-order capability evaluation of MLLMs is\nincreasing. However, there is a lack of work evaluating MLLM for higher-order\nperception and understanding of Chinese visual content. To fill the gap, we\nintroduce the **C**hinese **I**mage **I**mplication understanding\n**Bench**mark, **CII-Bench**, which aims to assess the higher-order perception\nand understanding capabilities of MLLMs for Chinese images. CII-Bench stands\nout in several ways compared to existing benchmarks. Firstly, to ensure the\nauthenticity of the Chinese context, images in CII-Bench are sourced from the\nChinese Internet and manually reviewed, with corresponding answers also\nmanually crafted. Additionally, CII-Bench incorporates images that represent\nChinese traditional culture, such as famous Chinese traditional paintings,\nwhich can deeply reflect the model's understanding of Chinese traditional\nculture. Through extensive experiments on CII-Bench across multiple MLLMs, we\nhave made significant findings. Initially, a substantial gap is observed\nbetween the performance of MLLMs and humans on CII-Bench. The highest accuracy\nof MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an\nimpressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional\nculture images, suggesting limitations in their ability to understand\nhigh-level semantics and lack a deep knowledge base of Chinese traditional\nculture. Finally, it is observed that most models exhibit enhanced accuracy\nwhen image emotion hints are incorporated into the prompts. We believe that\nCII-Bench will enable MLLMs to gain a better understanding of Chinese semantics\nand Chinese-specific images, advancing the journey towards expert artificial\ngeneral intelligence (AGI). Our project is publicly available at\nhttps://cii-bench.github.io/.\n","authors":["Chenhao Zhang","Xi Feng","Yuelin Bai","Xinrun Du","Jinchang Hou","Kaixin Deng","Guangzeng Han","Qinrui Li","Bingli Wang","Jiaheng Liu","Xingwei Qu","Yifei Zhang","Qixuan Zhao","Yiming Liang","Ziqiang Liu","Feiteng Fang","Min Yang","Wenhao Huang","Chenghua Lin","Ge Zhang","Shiwen Ni"],"pdf_url":"https://arxiv.org/pdf/2410.13854v1.pdf","comment":"32 pages,18 figures. Project Page: https://cii-bench.github.io/ Code:\n  https://github.com/MING_X/CII-Bench Dataset:\n  https://huggingface.co/datasets/m-a-p/CII-Bench"},{"id":"http://arxiv.org/abs/2410.13852v1","updated":"2024-10-17T17:59:03Z","published":"2024-10-17T17:59:03Z","title":"Retrospective Learning from Interactions","summary":"  Multi-turn interactions between large language models (LLMs) and users\nnaturally include implicit feedback signals. If an LLM responds in an\nunexpected way to an instruction, the user is likely to signal it by rephrasing\nthe request, expressing frustration, or pivoting to an alternative task. Such\nsignals are task-independent and occupy a relatively constrained subspace of\nlanguage, allowing the LLM to identify them even if it fails on the actual\ntask. This creates an avenue for continually learning from interactions without\nadditional annotations. We introduce ReSpect, a method to learn from such\nsignals in past interactions via retrospection. We deploy ReSpect in a new\nmultimodal interaction scenario, where humans instruct an LLM to solve an\nabstract reasoning task with a combinatorial solution space. Through thousands\nof interactions with humans, we show how ReSpect gradually improves task\ncompletion rate from 31% to 82%, all without any external annotation.\n","authors":["Zizhao Chen","Mustafa Omer Gul","Yiwei Chen","Gloria Geng","Anne Wu","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2410.13852v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13851v1","updated":"2024-10-17T17:59:02Z","published":"2024-10-17T17:59:02Z","title":"Differentiable Robot Rendering","summary":"  Vision foundation models trained on massive amounts of visual data have shown\nunprecedented reasoning and planning skills in open-world settings. A key\nchallenge in applying them to robotic tasks is the modality gap between visual\ndata and action data. We introduce differentiable robot rendering, a method\nallowing the visual appearance of a robot body to be directly differentiable\nwith respect to its control parameters. Our model integrates a kinematics-aware\ndeformable model and Gaussians Splatting and is compatible with any robot form\nfactors and degrees of freedom. We demonstrate its capability and usage in\napplications including reconstruction of robot poses from images and\ncontrolling robots through vision language models. Quantitative and qualitative\nresults show that our differentiable rendering model provides effective\ngradients for robotic control directly from pixels, setting the foundation for\nthe future applications of vision foundation models in robotics.\n","authors":["Ruoshi Liu","Alper Canberk","Shuran Song","Carl Vondrick"],"pdf_url":"https://arxiv.org/pdf/2410.13851v1.pdf","comment":"Project Page: https://drrobot.cs.columbia.edu/"},{"id":"http://arxiv.org/abs/2410.13848v1","updated":"2024-10-17T17:58:37Z","published":"2024-10-17T17:58:37Z","title":"Janus: Decoupling Visual Encoding for Unified Multimodal Understanding\n  and Generation","summary":"  In this paper, we introduce Janus, an autoregressive framework that unifies\nmultimodal understanding and generation. Prior research often relies on a\nsingle visual encoder for both tasks, such as Chameleon. However, due to the\ndiffering levels of information granularity required by multimodal\nunderstanding and generation, this approach can lead to suboptimal performance,\nparticularly in multimodal understanding. To address this issue, we decouple\nvisual encoding into separate pathways, while still leveraging a single,\nunified transformer architecture for processing. The decoupling not only\nalleviates the conflict between the visual encoder's roles in understanding and\ngeneration, but also enhances the framework's flexibility. For instance, both\nthe multimodal understanding and generation components can independently select\ntheir most suitable encoding methods. Experiments show that Janus surpasses\nprevious unified model and matches or exceeds the performance of task-specific\nmodels. The simplicity, high flexibility, and effectiveness of Janus make it a\nstrong candidate for next-generation unified multimodal models.\n","authors":["Chengyue Wu","Xiaokang Chen","Zhiyu Wu","Yiyang Ma","Xingchao Liu","Zizheng Pan","Wen Liu","Zhenda Xie","Xingkai Yu","Chong Ruan","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2410.13848v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2410.13842v1","updated":"2024-10-17T17:57:01Z","published":"2024-10-17T17:57:01Z","title":"D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution\n  Refinement","summary":"  We introduce D-FINE, a powerful real-time object detector that achieves\noutstanding localization precision by redefining the bounding box regression\ntask in DETR models. D-FINE comprises two key components: Fine-grained\nDistribution Refinement (FDR) and Global Optimal Localization Self-Distillation\n(GO-LSD). FDR transforms the regression process from predicting fixed\ncoordinates to iteratively refining probability distributions, providing a\nfine-grained intermediate representation that significantly enhances\nlocalization accuracy. GO-LSD is a bidirectional optimization strategy that\ntransfers localization knowledge from refined distributions to shallower layers\nthrough self-distillation, while also simplifying the residual prediction tasks\nfor deeper layers. Additionally, D-FINE incorporates lightweight optimizations\nin computationally intensive modules and operations, achieving a better balance\nbetween speed and accuracy. Specifically, D-FINE-L / X achieves 54.0% / 55.8%\nAP on the COCO dataset at 124 / 78 FPS on an NVIDIA T4 GPU. When pretrained on\nObjects365, D-FINE-L / X attains 57.1% / 59.3% AP, surpassing all existing\nreal-time detectors. Furthermore, our method significantly enhances the\nperformance of a wide range of DETR models by up to 5.3% AP with negligible\nextra parameters and training costs. Our code and pretrained models:\nhttps://github.com/Peterande/D-FINE.\n","authors":["Yansong Peng","Hebei Li","Peixi Wu","Yueyi Zhang","Xiaoyan Sun","Feng Wu"],"pdf_url":"https://arxiv.org/pdf/2410.13842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13832v1","updated":"2024-10-17T17:53:24Z","published":"2024-10-17T17:53:24Z","title":"VidPanos: Generative Panoramic Videos from Casual Panning Videos","summary":"  Panoramic image stitching provides a unified, wide-angle view of a scene that\nextends beyond the camera's field of view. Stitching frames of a panning video\ninto a panoramic photograph is a well-understood problem for stationary scenes,\nbut when objects are moving, a still panorama cannot capture the scene. We\npresent a method for synthesizing a panoramic video from a casually-captured\npanning video, as if the original video were captured with a wide-angle camera.\nWe pose panorama synthesis as a space-time outpainting problem, where we aim to\ncreate a full panoramic video of the same length as the input video. Consistent\ncompletion of the space-time volume requires a powerful, realistic prior over\nvideo content and motion, for which we adapt generative video models. Existing\ngenerative models do not, however, immediately extend to panorama completion,\nas we show. We instead apply video generation as a component of our panorama\nsynthesis system, and demonstrate how to exploit the strengths of the models\nwhile minimizing their limitations. Our system can create video panoramas for a\nrange of in-the-wild scenes including people, vehicles, and flowing water, as\nwell as stationary background features.\n","authors":["Jingwei Ma","Erika Lu","Roni Paiss","Shiran Zada","Aleksander Holynski","Tali Dekel","Brian Curless","Michael Rubinstein","Forrester Cole"],"pdf_url":"https://arxiv.org/pdf/2410.13832v1.pdf","comment":"Project page at https://vidpanos.github.io/. To appear at SIGGRAPH\n  Asia 2024 (conference track)"},{"id":"http://arxiv.org/abs/2410.13830v1","updated":"2024-10-17T17:52:57Z","published":"2024-10-17T17:52:57Z","title":"DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise\n  Motion Control","summary":"  Recent advances in customized video generation have enabled users to create\nvideos tailored to both specific subjects and motion trajectories. However,\nexisting methods often require complicated test-time fine-tuning and struggle\nwith balancing subject learning and motion control, limiting their real-world\napplications. In this paper, we present DreamVideo-2, a zero-shot video\ncustomization framework capable of generating videos with a specific subject\nand motion trajectory, guided by a single image and a bounding box sequence,\nrespectively, and without the need for test-time fine-tuning. Specifically, we\nintroduce reference attention, which leverages the model's inherent\ncapabilities for subject learning, and devise a mask-guided motion module to\nachieve precise motion control by fully utilizing the robust motion signal of\nbox masks derived from bounding boxes. While these two components achieve their\nintended functions, we empirically observe that motion control tends to\ndominate over subject learning. To address this, we propose two key designs: 1)\nthe masked reference attention, which integrates a blended latent mask modeling\nscheme into reference attention to enhance subject representations at the\ndesired positions, and 2) a reweighted diffusion loss, which differentiates the\ncontributions of regions inside and outside the bounding boxes to ensure a\nbalance between subject and motion control. Extensive experimental results on a\nnewly curated dataset demonstrate that DreamVideo-2 outperforms\nstate-of-the-art methods in both subject customization and motion control. The\ndataset, code, and models will be made publicly available.\n","authors":["Yujie Wei","Shiwei Zhang","Hangjie Yuan","Xiang Wang","Haonan Qiu","Rui Zhao","Yutong Feng","Feng Liu","Zhizhong Huang","Jiaxin Ye","Yingya Zhang","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2410.13830v1.pdf","comment":"Project page: https://dreamvideo2.github.io/"},{"id":"http://arxiv.org/abs/2410.13826v1","updated":"2024-10-17T17:51:40Z","published":"2024-10-17T17:51:40Z","title":"Unearthing Skill-Level Insights for Understanding Trade-Offs of\n  Foundation Models","summary":"  With models getting stronger, evaluations have grown more complex, testing\nmultiple skills in one benchmark and even in the same instance at once.\nHowever, skill-wise performance is obscured when inspecting aggregate accuracy,\nunder-utilizing the rich signal modern benchmarks contain. We propose an\nautomatic approach to recover the underlying skills relevant for any evaluation\ninstance, by way of inspecting model-generated rationales. After validating the\nrelevance of rationale-parsed skills and inferring skills for $46$k instances\nover $12$ benchmarks, we observe many skills to be common across benchmarks,\nresulting in the curation of hundreds of skill-slices (i.e. sets of instances\ntesting a common skill). Inspecting accuracy over these slices yields novel\ninsights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet,\non average, Gemini 1.5 Pro is $18\\%$ more accurate in \"computing molar mass\",\nbut $19\\%$ less accurate in \"applying constitutional law\", despite the overall\naccuracies of the three models differing by a mere $0.4\\%$. Furthermore, we\ndemonstrate the practical utility of our approach by showing that insights\nderived from skill slice analysis can generalize to held-out instances: when\nrouting each instance to the model strongest on the relevant skills, we see a\n$3\\%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices and\nframework open a new avenue in model evaluation, leveraging skill-specific\nanalyses to unlock a more granular and actionable understanding of model\ncapabilities.\n","authors":["Mazda Moayeri","Vidhisha Balachandran","Varun Chandrasekaran","Safoora Yousefi","Thomas Fel","Soheil Feizi","Besmira Nushi","Neel Joshi","Vibhav Vineet"],"pdf_url":"https://arxiv.org/pdf/2410.13826v1.pdf","comment":"Code at: github.com/microsoft/skill-slice-insights"},{"id":"http://arxiv.org/abs/2410.13823v1","updated":"2024-10-17T17:48:36Z","published":"2024-10-17T17:48:36Z","title":"Deep Generative Models Unveil Patterns in Medical Images Through\n  Vision-Language Conditioning","summary":"  Deep generative models have significantly advanced medical imaging analysis\nby enhancing dataset size and quality. Beyond mere data augmentation, our\nresearch in this paper highlights an additional, significant capacity of deep\ngenerative models: their ability to reveal and demonstrate patterns in medical\nimages. We employ a generative structure with hybrid conditions, combining\nclinical data and segmentation masks to guide the image synthesis process.\nFurthermore, we innovatively transformed the tabular clinical data into textual\ndescriptions. This approach simplifies the handling of missing values and also\nenables us to leverage large pre-trained vision-language models that\ninvestigate the relations between independent clinical entries and comprehend\ngeneral terms, such as gender and smoking status. Our approach differs from and\npresents a more challenging task than traditional medical report-guided\nsynthesis due to the less visual correlation of our clinical information with\nthe images. To overcome this, we introduce a text-visual embedding mechanism\nthat strengthens the conditions, ensuring the network effectively utilizes the\nprovided information. Our pipeline is generalizable to both GAN-based and\ndiffusion models. Experiments on chest CT, particularly focusing on the smoking\nstatus, demonstrated a consistent intensity shift in the lungs which is in\nagreement with clinical observations, indicating the effectiveness of our\nmethod in capturing and visualizing the impact of specific attributes on\nmedical image patterns. Our methods offer a new avenue for the early detection\nand precise visualization of complex clinical conditions with deep generative\nmodels. All codes are https://github.com/junzhin/DGM-VLC.\n","authors":["Xiaodan Xing","Junzhi Ning","Yang Nan","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.13823v1.pdf","comment":"Accepted by AIM-FM Workshop of NeurIPS2024"},{"id":"http://arxiv.org/abs/2410.13822v1","updated":"2024-10-17T17:48:17Z","published":"2024-10-17T17:48:17Z","title":"Multi-style conversion for semantic segmentation of lesions in fundus\n  images by adversarial attacks","summary":"  The diagnosis of diabetic retinopathy, which relies on fundus images, faces\nchallenges in achieving transparency and interpretability when using a global\nclassification approach. However, segmentation-based databases are\nsignificantly more expensive to acquire and combining them is often\nproblematic. This paper introduces a novel method, termed adversarial style\nconversion, to address the lack of standardization in annotation styles across\ndiverse databases. By training a single architecture on combined databases, the\nmodel spontaneously modifies its segmentation style depending on the input,\ndemonstrating the ability to convert among different labeling styles. The\nproposed methodology adds a linear probe to detect dataset origin based on\nencoder features and employs adversarial attacks to condition the model's\nsegmentation style. Results indicate significant qualitative and quantitative\nthrough dataset combination, offering avenues for improved model\ngeneralization, uncertainty estimation and continuous interpolation between\nannotation styles. Our approach enables training a segmentation model with\ndiverse databases while controlling and leveraging annotation styles for\nimproved retinopathy diagnosis.\n","authors":["Clément Playout","Renaud Duval","Marie Carole Boucher","Farida Cheriet"],"pdf_url":"https://arxiv.org/pdf/2410.13822v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2410.13807v1","updated":"2024-10-17T17:41:52Z","published":"2024-10-17T17:41:52Z","title":"ConsisSR: Delving Deep into Consistency in Diffusion-based Image\n  Super-Resolution","summary":"  Real-world image super-resolution (Real-ISR) aims at restoring high-quality\n(HQ) images from low-quality (LQ) inputs corrupted by unknown and complex\ndegradations. In particular, pretrained text-to-image (T2I) diffusion models\nprovide strong generative priors to reconstruct credible and intricate details.\nHowever, T2I generation focuses on semantic consistency while Real-ISR\nemphasizes pixel-level reconstruction, which hinders existing methods from\nfully exploiting diffusion priors. To address this challenge, we introduce\nConsisSR to handle both semantic and pixel-level consistency. Specifically,\ncompared to coarse-grained text prompts, we exploit the more powerful CLIP\nimage embedding and effectively leverage both modalities through our Hybrid\nPrompt Adapter (HPA) for semantic guidance. Secondly, we introduce Time-aware\nLatent Augmentation (TALA) to mitigate the inherent gap between T2I generation\nand Real-ISR consistency requirements. By randomly mixing LQ and HQ latent\ninputs, our model not only handle timestep-specific diffusion noise but also\nrefine the accumulated latent representations. Last but not least, our\nGAN-Embedding strategy employs the pretrained Real-ESRGAN model to refine the\ndiffusion start point. This accelerates the inference process to 10 steps while\npreserving sampling quality, in a training-free manner. Our method demonstrates\nstate-of-the-art performance among both full-scale and accelerated models. The\ncode will be made publicly available.\n","authors":["Junhao Gu","Peng-Tao Jiang","Hao Zhang","Mi Zhou","Jinwei Chen","Wenming Yang","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2410.13807v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13790v1","updated":"2024-10-17T17:31:24Z","published":"2024-10-17T17:31:24Z","title":"MotionBank: A Large-scale Video Motion Benchmark with Disentangled\n  Rule-based Annotations","summary":"  In this paper, we tackle the problem of how to build and benchmark a large\nmotion model (LMM). The ultimate goal of LMM is to serve as a foundation model\nfor versatile motion-related tasks, e.g., human motion generation, with\ninterpretability and generalizability. Though advanced, recent LMM-related\nworks are still limited by small-scale motion data and costly text\ndescriptions. Besides, previous motion benchmarks primarily focus on pure body\nmovements, neglecting the ubiquitous motions in context, i.e., humans\ninteracting with humans, objects, and scenes. To address these limitations, we\nconsolidate large-scale video action datasets as knowledge banks to build\nMotionBank, which comprises 13 video action datasets, 1.24M motion sequences,\nand 132.9M frames of natural and diverse human motions. Different from\nlaboratory-captured motions, in-the-wild human-centric videos contain abundant\nmotions in context. To facilitate better motion text alignment, we also\nmeticulously devise a motion caption generation algorithm to automatically\nproduce rule-based, unbiased, and disentangled text descriptions via the\nkinematic characteristics for each motion. Extensive experiments show that our\nMotionBank is beneficial for general motion-related tasks of human motion\ngeneration, motion in-context generation, and motion understanding. Video\nmotions together with the rule-based text annotations could serve as an\nefficient alternative for larger LMMs. Our dataset, codes, and benchmark will\nbe publicly available at https://github.com/liangxuy/MotionBank.\n","authors":["Liang Xu","Shaoyang Hua","Zili Lin","Yifan Liu","Feipeng Ma","Yichao Yan","Xin Jin","Xiaokang Yang","Wenjun Zeng"],"pdf_url":"https://arxiv.org/pdf/2410.13790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13786v1","updated":"2024-10-17T17:22:59Z","published":"2024-10-17T17:22:59Z","title":"Emphasizing Semantic Consistency of Salient Posture for Speech-Driven\n  Gesture Generation","summary":"  Speech-driven gesture generation aims at synthesizing a gesture sequence\nsynchronized with the input speech signal. Previous methods leverage neural\nnetworks to directly map a compact audio representation to the gesture\nsequence, ignoring the semantic association of different modalities and failing\nto deal with salient gestures. In this paper, we propose a novel speech-driven\ngesture generation method by emphasizing the semantic consistency of salient\nposture. Specifically, we first learn a joint manifold space for the individual\nrepresentation of audio and body pose to exploit the inherent semantic\nassociation between two modalities, and propose to enforce semantic consistency\nvia a consistency loss. Furthermore, we emphasize the semantic consistency of\nsalient postures by introducing a weakly-supervised detector to identify\nsalient postures, and reweighting the consistency loss to focus more on\nlearning the correspondence between salient postures and the high-level\nsemantics of speech content. In addition, we propose to extract audio features\ndedicated to facial expression and body gesture separately, and design separate\nbranches for face and body gesture synthesis. Extensive experimental results\ndemonstrate the superiority of our method over the state-of-the-art approaches.\n","authors":["Fengqi Liu","Hexiang Wang","Jingyu Gong","Ran Yi","Qianyu Zhou","Xuequan Lu","Jiangbo Lu","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2410.13786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13569v1","updated":"2024-10-17T17:17:09Z","published":"2024-10-17T17:17:09Z","title":"Representing Model Weights with Language using Tree Experts","summary":"  The increasing availability of public models begs the question: can we train\nneural networks that use other networks as input? This paper learns to\nrepresent models within a joint space that embeds both model weights and\nlanguage. However, machine learning on model weights is challenging as model\nweights often exhibit significant variation unrelated to the models' semantic\nproperties (nuisance variation). We identify a key property of real-world\nmodels: most public models belong to a small set of Model Trees, where all\nmodels within a tree are fine-tuned from a common ancestor (e.g., a foundation\nmodel). Importantly, we find that within each tree there is less nuisance\nvariation between models. For example, while classifying models according to\ntheir training dataset generally requires complex architectures, in our case,\neven a linear classifier trained on a single layer is often effective. While\neffective, linear layers are computationally expensive as model weights are\nvery high dimensional. To address this, we introduce Probing Experts (ProbeX),\na theoretically motivated, lightweight probing method. Notably, ProbeX is the\nfirst probing method designed to learn from the weights of just a single model\nlayer. We also construct and release a dataset that simulates the structure of\npublic model repositories. Our results show that ProbeX can effectively map the\nweights of large models into a shared weight-language embedding space.\nFurthermore, we demonstrate the impressive generalization of our method,\nachieving zero-shot model classification and retrieval.\n","authors":["Eliahu Horwitz","Bar Cavia","Jonathan Kahana","Yedid Hoshen"],"pdf_url":"https://arxiv.org/pdf/2410.13569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13760v1","updated":"2024-10-17T16:55:14Z","published":"2024-10-17T16:55:14Z","title":"Eyelid Fold Consistency in Facial Modeling","summary":"  Eyelid shape is integral to identity and likeness in human facial modeling.\nHuman eyelids are diverse in appearance with varied skin fold and epicanthal\nfold morphology between individuals. Existing parametric face models express\neyelid shape variation to an extent, but do not preserve sufficient likeness\nacross a diverse range of individuals. We propose a new definition of eyelid\nfold consistency and implement geometric processing techniques to model diverse\neyelid shapes in a unified topology. Using this method we reprocess data used\nto train a parametric face model and demonstrate significant improvements in\nface-related machine learning tasks.\n","authors":["Lohit Petikam","Charlie Hewitt","Fatemeh Saleh","Tadas Baltrušaitis"],"pdf_url":"https://arxiv.org/pdf/2410.13760v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2210.16953v2","updated":"2024-10-17T22:47:50Z","published":"2022-10-30T21:26:07Z","title":"Improving Bilingual Lexicon Induction with Cross-Encoder Reranking","summary":"  Bilingual lexicon induction (BLI) with limited bilingual supervision is a\ncrucial yet challenging task in multilingual NLP. Current state-of-the-art BLI\nmethods rely on the induction of cross-lingual word embeddings (CLWEs) to\ncapture cross-lingual word similarities; such CLWEs are obtained 1) via\ntraditional static models (e.g., VecMap), or 2) by extracting type-level CLWEs\nfrom multilingual pretrained language models (mPLMs), or 3) through combining\nthe former two options. In this work, we propose a novel semi-supervised\npost-hoc reranking method termed BLICEr (BLI with Cross-Encoder Reranking),\napplicable to any precalculated CLWE space, which improves their BLI\ncapability. The key idea is to 'extract' cross-lingual lexical knowledge from\nmPLMs, and then combine it with the original CLWEs. This crucial step is done\nvia 1) creating a word similarity dataset, comprising positive word pairs\n(i.e., true translations) and hard negative pairs induced from the original\nCLWE space, and then 2) fine-tuning an mPLM (e.g., mBERT or XLM-R) in a\ncross-encoder manner to predict the similarity scores. At inference, we 3)\ncombine the similarity score from the original CLWE space with the score from\nthe BLI-tuned cross-encoder. BLICEr establishes new state-of-the-art results on\ntwo standard BLI benchmarks spanning a wide spectrum of diverse languages: it\nsubstantially outperforms a series of strong baselines across the board. We\nalso validate the robustness of BLICEr with different CLWEs.\n","authors":["Yaoyiran Li","Fangyu Liu","Ivan Vulić","Anna Korhonen"],"pdf_url":"https://arxiv.org/pdf/2210.16953v2.pdf","comment":"Findings of EMNLP 2022"},{"id":"http://arxiv.org/abs/2410.14066v1","updated":"2024-10-17T22:28:07Z","published":"2024-10-17T22:28:07Z","title":"Lightweight Correlation-Aware Table Compression","summary":"  The growing adoption of data lakes for managing relational data necessitates\nefficient, open storage formats that provide high scan performance and\ncompetitive compression ratios. While existing formats achieve fast scans\nthrough lightweight encoding techniques, they have reached a plateau in terms\nof minimizing storage footprint. Recently, correlation-aware compression\nschemes have been shown to reduce file sizes further. Yet, current approaches\neither incur significant scan overheads or require manual specification of\ncorrelations, limiting their practicability. We present $\\texttt{Virtual}$, a\nframework that integrates seamlessly with existing open formats to\nautomatically leverage data correlations, achieving substantial compression\ngains while having minimal scan performance overhead. Experiments on\n$\\texttt{data.gov}$ datasets show that $\\texttt{Virtual}$ reduces file sizes by\nup to 40% compared to Apache Parquet.\n","authors":["Mihail Stoian","Alexander van Renen","Jan Kobiolka","Ping-Lin Kuo","Josif Grabocka","Andreas Kipf"],"pdf_url":"https://arxiv.org/pdf/2410.14066v1.pdf","comment":"Third Table Representation Learning Workshop (TRL 2024)"},{"id":"http://arxiv.org/abs/2410.12123v2","updated":"2024-10-17T22:06:32Z","published":"2024-10-15T23:51:04Z","title":"The Moral Case for Using Language Model Agents for Recommendation","summary":"  Our information and communication environment has fallen short of the ideals\nthat networked global communication might have served. Identifying all the\ncauses of its pathologies is difficult, but existing recommender systems very\nlikely play a contributing role. In this paper, which draws on the normative\ntools of philosophy of computing, informed by empirical and technical insights\nfrom natural language processing and recommender systems, we make the moral\ncase for an alternative approach. We argue that existing recommenders\nincentivise mass surveillance, concentrate power, fall prey to narrow\nbehaviourism, and compromise user agency. Rather than just trying to avoid\nalgorithms entirely, or to make incremental improvements to the current\nparadigm, researchers and engineers should explore an alternative paradigm: the\nuse of language model (LM) agents to source and curate content that matches\nusers' preferences and values, expressed in natural language. The use of LM\nagents for recommendation poses its own challenges, including those related to\ncandidate generation, computational efficiency, preference modelling, and\nprompt injection. Nonetheless, if implemented successfully LM agents could:\nguide us through the digital public sphere without relying on mass\nsurveillance; shift power away from platforms towards users; optimise for what\nmatters instead of just for behavioural proxies; and scaffold our agency\ninstead of undermining it.\n","authors":["Seth Lazar","Luke Thorburn","Tian Jin","Luca Belli"],"pdf_url":"https://arxiv.org/pdf/2410.12123v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.08307v5","updated":"2024-10-17T21:50:37Z","published":"2022-03-15T22:51:22Z","title":"Improving Word Translation via Two-Stage Contrastive Learning","summary":"  Word translation or bilingual lexicon induction (BLI) is a key cross-lingual\ntask, aiming to bridge the lexical gap between different languages. In this\nwork, we propose a robust and effective two-stage contrastive learning\nframework for the BLI task. At Stage C1, we propose to refine standard\ncross-lingual linear maps between static word embeddings (WEs) via a\ncontrastive learning objective; we also show how to integrate it into the\nself-learning procedure for even more refined cross-lingual maps. In Stage C2,\nwe conduct BLI-oriented contrastive fine-tuning of mBERT, unlocking its word\ntranslation capability. We also show that static WEs induced from the\n`C2-tuned' mBERT complement static WEs from Stage C1. Comprehensive experiments\non standard BLI datasets for diverse languages and different experimental\nsetups demonstrate substantial gains achieved by our framework. While the BLI\nmethod from Stage C1 already yields substantial gains over all state-of-the-art\nBLI methods in our comparison, even stronger improvements are met with the full\ntwo-stage framework: e.g., we report gains for 112/112 BLI setups, spanning 28\nlanguage pairs.\n","authors":["Yaoyiran Li","Fangyu Liu","Nigel Collier","Anna Korhonen","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2203.08307v5.pdf","comment":"ACL 2022 Main"},{"id":"http://arxiv.org/abs/2410.14044v1","updated":"2024-10-17T21:37:08Z","published":"2024-10-17T21:37:08Z","title":"Best in Tau@LLMJudge: Criteria-Based Relevance Evaluation with Llama3","summary":"  Traditional evaluation of information retrieval (IR) systems relies on\nhuman-annotated relevance labels, which can be both biased and costly at scale.\nIn this context, large language models (LLMs) offer an alternative by allowing\nus to directly prompt them to assign relevance labels for passages associated\nwith each query. In this study, we explore alternative methods to directly\nprompt LLMs for assigned relevance labels, by exploring two hypotheses:\n  Hypothesis 1 assumes that it is helpful to break down \"relevance\" into\nspecific criteria - exactness, coverage, topicality, and contextual fit. We\nexplore different approaches that prompt large language models (LLMs) to obtain\ncriteria-level grades for all passages, and we consider various ways to\naggregate criteria-level grades into a relevance label. Hypothesis 2 assumes\nthat differences in linguistic style between queries and passages may\nnegatively impact the automatic relevance label prediction. We explore whether\nimprovements can be achieved by first synthesizing a summary of the passage in\nthe linguistic style of a query, and then using this summary in place of the\npassage to assess its relevance.\n  We include an empirical evaluation of our approaches based on data from the\nLLMJudge challenge run in Summer 2024, where our \"Four Prompts\" approach\nobtained the highest scores in Kendall's tau.\n","authors":["Naghmeh Farzi","Laura Dietz"],"pdf_url":"https://arxiv.org/pdf/2410.14044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14043v1","updated":"2024-10-17T21:35:55Z","published":"2024-10-17T21:35:55Z","title":"Efficient Retrieval of Temporal Event Sequences from Textual\n  Descriptions","summary":"  Retrieving temporal event sequences from textual descriptions is essential\nfor applications such as analyzing e-commerce behavior, monitoring social media\nactivities, and tracking criminal incidents. In this paper, we introduce\nTPP-LLM-Embedding, a unified model for efficiently embedding and retrieving\nevent sequences based on natural language descriptions. Built on the TPP-LLM\nframework, which integrates large language models with temporal point\nprocesses, our model encodes both event types and times, generating a\nsequence-level representation through pooling. Textual descriptions are\nembedded using the same architecture, ensuring a shared embedding space for\nboth sequences and descriptions. We optimize a contrastive loss based on\nsimilarity between these embeddings, bringing matching pairs closer and\nseparating non-matching ones. TPP-LLM-Embedding enables efficient retrieval and\ndemonstrates superior performance compared to baseline models across diverse\ndatasets.\n","authors":["Zefang Liu","Yinzhu Quan"],"pdf_url":"https://arxiv.org/pdf/2410.14043v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15232v2","updated":"2024-10-17T20:43:22Z","published":"2024-08-27T17:50:03Z","title":"Into the Unknown Unknowns: Engaged Human Learning through Participation\n  in Language Model Agent Conversations","summary":"  While language model (LM)-powered chatbots and generative search engines\nexcel at answering concrete queries, discovering information in the terrain of\nunknown unknowns remains challenging for users. To emulate the common\neducational scenario where children/students learn by listening to and\nparticipating in conversations of their parents/teachers, we create\nCollaborative STORM (Co-STORM). Unlike QA systems that require users to ask all\nthe questions, Co-STORM lets users observe and occasionally steer the discourse\namong several LM agents. The agents ask questions on the user's behalf,\nallowing the user to discover unknown unknowns serendipitously. To facilitate\nuser interaction, Co-STORM assists users in tracking the discourse by\norganizing the uncovered information into a dynamic mind map, ultimately\ngenerating a comprehensive report as takeaways. For automatic evaluation, we\nconstruct the WildSeek dataset by collecting real information-seeking records\nwith user goals. Co-STORM outperforms baseline methods on both discourse trace\nand report quality. In a further human evaluation, 70% of participants prefer\nCo-STORM over a search engine, and 78% favor it over a RAG chatbot.\n","authors":["Yucheng Jiang","Yijia Shao","Dekun Ma","Sina J. Semnani","Monica S. Lam"],"pdf_url":"https://arxiv.org/pdf/2408.15232v2.pdf","comment":"EMNLP 2024 Main"},{"id":"http://arxiv.org/abs/2307.08803v3","updated":"2024-10-17T19:09:13Z","published":"2023-07-17T19:38:40Z","title":"Mixed-initiative Query Rewriting in Conversational Passage Retrieval","summary":"  In this paper, we report our methods and experiments for the TREC\nConversational Assistance Track (CAsT) 2022. In this work, we aim to reproduce\nmulti-stage retrieval pipelines and explore one of the potential benefits of\ninvolving mixed-initiative interaction in conversational passage retrieval\nscenarios: reformulating raw queries. Before the first ranking stage of a\nmulti-stage retrieval pipeline, we propose a mixed-initiative query rewriting\nmodule, which achieves query rewriting based on the mixed-initiative\ninteraction between the users and the system, as the replacement for the neural\nrewriting method. Specifically, we design an algorithm to generate appropriate\nquestions related to the ambiguities in raw queries, and another algorithm to\nreformulate raw queries by parsing users' feedback and incorporating it into\nthe raw query. For the first ranking stage of our multi-stage pipelines, we\nadopt a sparse ranking function: BM25, and a dense retrieval method:\nTCT-ColBERT. For the second-ranking step, we adopt a pointwise reranker:\nMonoT5, and a pairwise reranker: DuoT5. Experiments on both TREC CAsT 2021 and\nTREC CAsT 2022 datasets show the effectiveness of our mixed-initiative-based\nquery rewriting (or query reformulation) method on improving retrieval\nperformance compared with two popular reformulators: a neural reformulator:\nCANARD-T5 and a rule-based reformulator: historical query reformulator(HQE).\n","authors":["Dayu Yang","Yue Zhang","Hui Fang"],"pdf_url":"https://arxiv.org/pdf/2307.08803v3.pdf","comment":"https://trec.nist.gov/pubs/trec31/papers/udel_fang.C.pdf"},{"id":"http://arxiv.org/abs/2404.11773v2","updated":"2024-10-17T18:59:18Z","published":"2024-04-17T21:56:27Z","title":"Behavior Alignment: A New Perspective of Evaluating LLM-based\n  Conversational Recommender Systems","summary":"  Large Language Models (LLMs) have demonstrated great potential in\nConversational Recommender Systems (CRS). However, the application of LLMs to\nCRS has exposed a notable discrepancy in behavior between LLM-based CRS and\nhuman recommenders: LLMs often appear inflexible and passive, frequently\nrushing to complete the recommendation task without sufficient inquiry.This\nbehavior discrepancy can lead to decreased accuracy in recommendations and\nlower user satisfaction. Despite its importance, existing studies in CRS lack a\nstudy about how to measure such behavior discrepancy. To fill this gap, we\npropose Behavior Alignment, a new evaluation metric to measure how well the\nrecommendation strategies made by a LLM-based CRS are consistent with human\nrecommenders'. Our experiment results show that the new metric is better\naligned with human preferences and can better differentiate how systems perform\nthan existing evaluation metrics. As Behavior Alignment requires explicit and\ncostly human annotations on the recommendation strategies, we also propose a\nclassification-based method to implicitly measure the Behavior Alignment based\non the responses. The evaluation results confirm the robustness of the method.\n","authors":["Dayu Yang","Fumian Chen","Hui Fang"],"pdf_url":"https://arxiv.org/pdf/2404.11773v2.pdf","comment":"Accepted by the 47th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval"},{"id":"http://arxiv.org/abs/2307.09384v3","updated":"2024-10-17T18:53:54Z","published":"2023-07-18T16:05:25Z","title":"ZeQR: Zero-shot Query Reformulation for Conversational Search","summary":"  As the popularity of voice assistants continues to surge, conversational\nsearch has gained increased attention in Information Retrieval. However, data\nsparsity issues in conversational search significantly hinder the progress of\nsupervised conversational search methods. Consequently, researchers are\nfocusing more on zero-shot conversational search approaches. Nevertheless,\nexisting zero-shot methods face three primary limitations: they are not\nuniversally applicable to all retrievers, their effectiveness lacks sufficient\nexplainability, and they struggle to resolve common conversational ambiguities\ncaused by omission. To address these limitations, we introduce a novel\nZero-shot Query Reformulation (or Query Rewriting) (ZeQR) framework that\nreformulates queries based on previous dialogue contexts without requiring\nsupervision from conversational search data. Specifically, our framework\nutilizes language models designed for machine reading comprehension tasks to\nexplicitly resolve two common ambiguities: coreference and omission, in raw\nqueries. In comparison to existing zero-shot methods, our approach is\nuniversally applicable to any retriever without additional adaptation or\nindexing. It also provides greater explainability and effectively enhances\nquery intent understanding because ambiguities are explicitly and proactively\nresolved. Through extensive experiments on four TREC conversational datasets,\nwe demonstrate the effectiveness of our method, which consistently outperforms\nstate-of-the-art baselines.\n","authors":["Dayu Yang","Yue Zhang","Hui Fang"],"pdf_url":"https://arxiv.org/pdf/2307.09384v3.pdf","comment":"Accepted by the 9th ACM SIGIR International Conference on the Theory\n  of Information Retrieval"},{"id":"http://arxiv.org/abs/2310.01038v2","updated":"2024-10-17T18:35:41Z","published":"2023-10-02T09:30:11Z","title":"Dataset Condensation for Recommendation","summary":"  Training recommendation models on large datasets requires significant time\nand resources. It is desired to construct concise yet informative datasets for\nefficient training. Recent advances in dataset condensation show promise in\naddressing this problem by synthesizing small datasets. However, applying\nexisting methods of dataset condensation to recommendation has limitations: (1)\nthey fail to generate discrete user-item interactions, and (2) they could not\npreserve users' potential preferences. To address the limitations, we propose a\nlightweight condensation framework tailored for recommendation (DConRec),\nfocusing on condensing user-item historical interaction sets. Specifically, we\nmodel the discrete user-item interactions via a probabilistic approach and\ndesign a pre-augmentation module to incorporate the potential preferences of\nusers into the condensed datasets. While the substantial size of datasets leads\nto costly optimization, we propose a lightweight policy gradient estimation to\naccelerate the data synthesis. Experimental results on multiple real-world\ndatasets have demonstrated the effectiveness and efficiency of our framework.\nBesides, we provide a theoretical analysis of the provable convergence of\nDConRec. Our implementation is available at:\nhttps://github.com/JiahaoWuGit/DConRec.\n","authors":["Jiahao Wu","Wenqi Fan","Jingfan Chen","Shengcai Liu","Qijiong Liu","Rui He","Qing Li","Ke Tang"],"pdf_url":"https://arxiv.org/pdf/2310.01038v2.pdf","comment":"Accepted by IEEE TKDE. Also titled as \"Condensing Pre-augmented\n  Recommendation Data via Lightweight Policy Gradient Estimation\""},{"id":"http://arxiv.org/abs/2410.13959v1","updated":"2024-10-17T18:34:43Z","published":"2024-10-17T18:34:43Z","title":"FinQAPT: Empowering Financial Decisions with End-to-End LLM-driven\n  Question Answering Pipeline","summary":"  Financial decision-making hinges on the analysis of relevant information\nembedded in the enormous volume of documents in the financial domain. To\naddress this challenge, we developed FinQAPT, an end-to-end pipeline that\nstreamlines the identification of relevant financial reports based on a query,\nextracts pertinent context, and leverages Large Language Models (LLMs) to\nperform downstream tasks. To evaluate the pipeline, we experimented with\nvarious techniques to optimize the performance of each module using the FinQA\ndataset. We introduced a novel clustering-based negative sampling technique to\nenhance context extraction and a novel prompting method called Dynamic N-shot\nPrompting to boost the numerical question-answering capabilities of LLMs. At\nthe module level, we achieved state-of-the-art accuracy on FinQA, attaining an\naccuracy of 80.6\\%. However, at the pipeline level, we observed decreased\nperformance due to challenges in extracting relevant context from financial\nreports. We conducted a detailed error analysis of each module and the\nend-to-end pipeline, pinpointing specific challenges that must be addressed to\ndevelop a robust solution for handling complex financial tasks.\n","authors":["Kuldeep Singh","Simerjot Kaur","Charese Smiley"],"pdf_url":"https://arxiv.org/pdf/2410.13959v1.pdf","comment":"Accepted in ICAIF 2024, 8 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2410.13951v1","updated":"2024-10-17T18:22:42Z","published":"2024-10-17T18:22:42Z","title":"Identifying High Consideration E-Commerce Search Queries","summary":"  In e-commerce, high consideration search missions typically require careful\nand elaborate decision making, and involve a substantial research investment\nfrom customers. We consider the task of identifying High Consideration (HC)\nqueries. Identifying such queries enables e-commerce sites to better serve user\nneeds using targeted experiences such as curated QA widgets that help users\nreach purchase decisions. We explore the task by proposing an Engagement-based\nQuery Ranking (EQR) approach, focusing on query ranking to indicate potential\nengagement levels with query-related shopping knowledge content during product\nsearch. Unlike previous studies on predicting trends, EQR prioritizes\nquery-level features related to customer behavior, finance, and catalog\ninformation rather than popularity signals. We introduce an accurate and\nscalable method for EQR and present experimental results demonstrating its\neffectiveness. Offline experiments show strong ranking performance. Human\nevaluation shows a precision of 96% for HC queries identified by our model. The\nmodel was commercially deployed, and shown to outperform human-selected queries\nin terms of downstream customer impact, as measured through engagement.\n","authors":["Zhiyu Chen","Jason Choi","Besnik Fetahu","Shervin Malmasi"],"pdf_url":"https://arxiv.org/pdf/2410.13951v1.pdf","comment":"Accepted by EMNLP 2024 (Industry Track)"},{"id":"http://arxiv.org/abs/2410.13765v1","updated":"2024-10-17T17:03:23Z","published":"2024-10-17T17:03:23Z","title":"Knowledge-Aware Query Expansion with Large Language Models for Textual\n  and Relational Retrieval","summary":"  Large language models (LLMs) have been used to generate query expansions\naugmenting original queries for improving information search. Recent studies\nalso explore providing LLMs with initial retrieval results to generate query\nexpansions more grounded to document corpus. However, these methods mostly\nfocus on enhancing textual similarities between search queries and target\ndocuments, overlooking document relations. For queries like \"Find me a highly\nrated camera for wildlife photography compatible with my Nikon F-Mount lenses\",\nexisting methods may generate expansions that are semantically similar but\nstructurally unrelated to user intents. To handle such semi-structured queries\nwith both textual and relational requirements, in this paper we propose a\nknowledge-aware query expansion framework, augmenting LLMs with structured\ndocument relations from knowledge graph (KG). To further address the limitation\nof entity-based scoring in existing KG-based methods, we leverage document\ntexts as rich KG node representations and use document-based relation filtering\nfor our Knowledge-Aware Retrieval (KAR). Extensive experiments on three\ndatasets of diverse domains show the advantages of our method compared against\nstate-of-the-art baselines on textual and relational semi-structured retrieval.\n","authors":["Yu Xia","Junda Wu","Sungchul Kim","Tong Yu","Ryan A. Rossi","Haoliang Wang","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.13765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13707v1","updated":"2024-10-17T16:07:51Z","published":"2024-10-17T16:07:51Z","title":"Disjointness Violations in Wikidata","summary":"  Disjointness checks are among the most important constraint checks in a\nknowledge base and can be used to help detect and correct incorrect statements\nand internal contradictions. Wikidata is a very large, community-managed\nknowledge base. Because of both its size and construction, Wikidata contains\nmany incorrect statements and internal contradictions. We analyze the current\nmodeling of disjointness on Wikidata, identify patterns that cause these\ndisjointness violations and categorize them. We use SPARQL queries to identify\neach ``culprit'' causing a disjointness violation and lay out formulas to\nidentify and fix conflicting information. We finally discuss how disjointness\ninformation could be better modeled and expanded in Wikidata in the future.\n","authors":["Ege Atacan Doğan","Peter F. Patel-Schneider"],"pdf_url":"https://arxiv.org/pdf/2410.13707v1.pdf","comment":"Sixth International Knowledge Graph and Semantic Web Conference"},{"id":"http://arxiv.org/abs/2410.13680v1","updated":"2024-10-17T15:40:09Z","published":"2024-10-17T15:40:09Z","title":"Pessimistic Evaluation","summary":"  Traditional evaluation of information access systems has focused primarily on\naverage utility across a set of information needs (information retrieval) or\nusers (recommender systems). In this work, we argue that evaluating only with\naverage metric measurements assumes utilitarian values not aligned with\ntraditions of information access based on equal access. We advocate for\npessimistic evaluation of information access systems focusing on worst case\nutility. These methods are (a) grounded in ethical and pragmatic concepts, (b)\ntheoretically complementary to existing robustness and fairness methods, and\n(c) empirically validated across a set of retrieval and recommendation tasks.\nThese results suggest that pessimistic evaluation should be included in\nexisting experimentation processes to better understand the behavior of\nsystems, especially when concerned with principles of social good.\n","authors":["Fernando Diaz"],"pdf_url":"https://arxiv.org/pdf/2410.13680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13604v1","updated":"2024-10-17T14:39:24Z","published":"2024-10-17T14:39:24Z","title":"Large Language Models as Narrative-Driven Recommenders","summary":"  Narrative-driven recommenders aim to provide personalized suggestions for\nuser requests expressed in free-form text such as \"I want to watch a thriller\nwith a mind-bending story, like Shutter Island.\" Although large language models\n(LLMs) have been shown to excel in processing general natural language queries,\ntheir effectiveness for handling such recommendation requests remains\nrelatively unexplored. To close this gap, we compare the performance of 38\nopen- and closed-source LLMs of various sizes, such as LLama 3.2 and GPT-4o, in\na movie recommendation setting. For this, we utilize a gold-standard,\ncrowdworker-annotated dataset of posts from reddit's movie suggestion community\nand employ various prompting strategies, including zero-shot, identity, and\nfew-shot prompting. Our findings demonstrate the ability of LLMs to generate\ncontextually relevant movie recommendations, significantly outperforming other\nstate-of-the-art approaches, such as doc2vec. While we find that closed-source\nand large-parameterized models generally perform best, medium-sized open-source\nmodels remain competitive, being only slightly outperformed by their more\ncomputationally expensive counterparts. Furthermore, we observe no significant\ndifferences across prompting strategies for most models, underscoring the\neffectiveness of simple approaches such as zero-shot prompting for\nnarrative-driven recommendations. Overall, this work offers valuable insights\nfor recommender system researchers as well as practitioners aiming to integrate\nLLMs into real-world recommendation tools.\n","authors":["Lukas Eberhard","Thorsten Ruprechter","Denis Helic"],"pdf_url":"https://arxiv.org/pdf/2410.13604v1.pdf","comment":"Under review; 19 pages"},{"id":"http://arxiv.org/abs/2410.13588v1","updated":"2024-10-17T14:22:57Z","published":"2024-10-17T14:22:57Z","title":"Cross-Domain Sequential Recommendation via Neural Process","summary":"  Cross-Domain Sequential Recommendation (CDSR) is a hot topic in\nsequence-based user interest modeling, which aims at utilizing a single model\nto predict the next items for different domains. To tackle the CDSR, many\nmethods are focused on domain overlapped users' behaviors fitting, which\nheavily relies on the same user's different-domain item sequences collaborating\nsignals to capture the synergy of cross-domain item-item correlation. Indeed,\nthese overlapped users occupy a small fraction of the entire user set only,\nwhich introduces a strong assumption that the small group of domain overlapped\nusers is enough to represent all domain user behavior characteristics. However,\nintuitively, such a suggestion is biased, and the insufficient learning\nparadigm in non-overlapped users will inevitably limit model performance.\nFurther, it is not trivial to model non-overlapped user behaviors in CDSR\nbecause there are no other domain behaviors to collaborate with, which causes\nthe observed single-domain users' behavior sequences to be hard to contribute\nto cross-domain knowledge mining. Considering such a phenomenon, we raise a\nchallenging and unexplored question: How to unleash the potential of\nnon-overlapped users' behaviors to empower CDSR?\n","authors":["Haipeng Li","Jiangxia Cao","Yiwen Gao","Yunhuai Liu","Shuchao Pang"],"pdf_url":"https://arxiv.org/pdf/2410.13588v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2409.20305v2","updated":"2024-10-17T13:19:08Z","published":"2024-09-30T14:04:27Z","title":"Mixed-Precision Embeddings for Large-Scale Recommendation Models","summary":"  Embedding techniques have become essential components of large databases in\nthe deep learning era. By encoding discrete entities, such as words, items, or\ngraph nodes, into continuous vector spaces, embeddings facilitate more\nefficient storage, retrieval, and processing in large databases. Especially in\nthe domain of recommender systems, millions of categorical features are encoded\nas unique embedding vectors, which facilitates the modeling of similarities and\ninteractions among features. However, numerous embedding vectors can result in\nsignificant storage overhead. In this paper, we aim to compress the embedding\ntable through quantization techniques. Given that features vary in importance\nlevels, we seek to identify an appropriate precision for each feature to\nbalance model accuracy and memory usage. To this end, we propose a novel\nembedding compression method, termed Mixed-Precision Embeddings (MPE).\nSpecifically, to reduce the size of the search space, we first group features\nby frequency and then search precision for each feature group. MPE further\nlearns the probability distribution over precision levels for each feature\ngroup, which can be used to identify the most suitable precision with a\nspecially designed sampling strategy. Extensive experiments on three public\ndatasets demonstrate that MPE significantly outperforms existing embedding\ncompression methods. Remarkably, MPE achieves about 200x compression on the\nCriteo dataset without comprising the prediction accuracy.\n","authors":["Shiwei Li","Zhuoqi Hu","Xing Tang","Haozhao Wang","Shijie Xu","Weihong Luo","Yuhua Li","Xiuqiang He","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2409.20305v2.pdf","comment":"under submision"},{"id":"http://arxiv.org/abs/2406.16350v2","updated":"2024-10-17T10:59:08Z","published":"2024-06-24T06:46:32Z","title":"A Survey on Intent-aware Recommender Systems","summary":"  Many modern online services feature personalized recommendations. A central\nchallenge when providing such recommendations is that the reason why an\nindividual user accesses the service may change from visit to visit or even\nduring an ongoing usage session. To be effective, a recommender system should\ntherefore aim to take the users' probable intent of using the service at a\ncertain point in time into account. In recent years, researchers have thus\nstarted to address this challenge by incorporating intent-awareness into\nrecommender systems. Correspondingly, a number of technical approaches were put\nforward, including diversification techniques, intent prediction models or\nlatent intent modeling approaches. In this paper, we survey and categorize\nexisting approaches to building the next generation of Intent-Aware Recommender\nSystems (IARS). Based on an analysis of current evaluation practices, we\noutline open gaps and possible future directions in this area, which in\nparticular include the consideration of additional interaction signals and\ncontextual information to further improve the effectiveness of such systems.\n","authors":["Dietmar Jannach","Markus Zanker"],"pdf_url":"https://arxiv.org/pdf/2406.16350v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13374v1","updated":"2024-10-17T09:24:40Z","published":"2024-10-17T09:24:40Z","title":"Context-aware adaptive personalised recommendation: a meta-hybrid","summary":"  Recommenders take place on a wide scale of e-commerce systems, reducing the\nproblem of information overload. The most common approach is to choose a\nrecommender used by the system to make predictions. However, users vary from\neach other; thus, a one-fits-all approach seems to be sub-optimal. In this\npaper, we propose a meta-hybrid recommender that uses machine learning to\npredict an optimal algorithm. In this way, the best-performing recommender is\nused for each specific session and user. This selection depends on contextual\nand preferential information collected about the user. We use standard\nMovieLens and The Movie DB datasets for offline evaluation. We show that based\non the proposed model, it is possible to predict which recommender will provide\nthe most precise recommendations to a user. The theoretical performance of our\nmeta-hybrid outperforms separate approaches by 20-50% in normalized Discounted\nGain and Root Mean Square Error metrics. However, it is hard to obtain the\noptimal performance based on widely-used standard information stored about\nusers.\n","authors":["Peter Tibensky","Michal Kompan"],"pdf_url":"https://arxiv.org/pdf/2410.13374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00800v2","updated":"2024-10-17T09:13:18Z","published":"2024-07-22T11:58:36Z","title":"Chatbot-Based Ontology Interaction Using Large Language Models and\n  Domain-Specific Standards","summary":"  The following contribution introduces a concept that employs Large Language\nModels (LLMs) and a chatbot interface to enhance SPARQL query generation for\nontologies, thereby facilitating intuitive access to formalized knowledge.\nUtilizing natural language inputs, the system converts user inquiries into\naccurate SPARQL queries that strictly query the factual content of the\nontology, effectively preventing misinformation or fabrication by the LLM. To\nenhance the quality and precision of outcomes, additional textual information\nfrom established domain-specific standards is integrated into the ontology for\nprecise descriptions of its concepts and relationships. An experimental study\nassesses the accuracy of generated SPARQL queries, revealing significant\nbenefits of using LLMs for querying ontologies and highlighting areas for\nfuture research.\n","authors":["Jonathan Reif","Tom Jeleniewski","Milapji Singh Gill","Felix Gehlhoff","Alexander Fay"],"pdf_url":"https://arxiv.org/pdf/2408.00800v2.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2410.13326v1","updated":"2024-10-17T08:37:25Z","published":"2024-10-17T08:37:25Z","title":"Comparing the Utility, Preference, and Performance of Course Material\n  Search Functionality and Retrieval-Augmented Generation Large Language Model\n  (RAG-LLM) AI Chatbots in Information-Seeking Tasks","summary":"  Providing sufficient support for students requires substantial resources,\nespecially considering the growing enrollment numbers. Students need help in a\nvariety of tasks, ranging from information-seeking to requiring support with\ncourse assignments. To explore the utility of recent large language models\n(LLMs) as a support mechanism, we developed an LLM-powered AI chatbot that\naugments the answers that are produced with information from the course\nmaterials. To study the effect of the LLM-powered AI chatbot, we conducted a\nlab-based user study (N=14), in which the participants worked on tasks from a\nweb software development course. The participants were divided into two groups,\nwhere one of the groups first had access to the chatbot and then to a more\ntraditional search functionality, while another group started with the search\nfunctionality and was then given the chatbot. We assessed the participants'\nperformance and perceptions towards the chatbot and the search functionality\nand explored their preferences towards the support functionalities. Our\nfindings highlight that both support mechanisms are seen as useful and that\nsupport mechanisms work well for specific tasks, while less so for other tasks.\nWe also observe that students tended to prefer the second support mechanism\nmore, where students who were first given the chatbot tended to prefer the\nsearch functionality and vice versa.\n","authors":["Leonardo Pasquarelli","Charles Koutcheme","Arto Hellas"],"pdf_url":"https://arxiv.org/pdf/2410.13326v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.10025v2","updated":"2024-10-17T08:16:52Z","published":"2024-09-16T06:33:26Z","title":"DiffATR: Diffusion-based Generative Modeling for Audio-Text Retrieval","summary":"  Existing audio-text retrieval (ATR) methods are essentially discriminative\nmodels that aim to maximize the conditional likelihood, represented as\np(candidates|query). Nevertheless, this methodology fails to consider the\nintrinsic data distribution p(query), leading to difficulties in discerning\nout-of-distribution data. In this work, we attempt to tackle this constraint\nthrough a generative perspective and model the relationship between audio and\ntext as their joint probability p(candidates,query). To this end, we present a\ndiffusion-based ATR framework (DiffATR), which models ATR as an iterative\nprocedure that progressively generates joint distribution from noise.\nThroughout its training phase, DiffATR is optimized from both generative and\ndiscriminative viewpoints: the generator is refined through a generation loss,\nwhile the feature extractor benefits from a contrastive loss, thus combining\nthe merits of both methodologies. Experiments on the AudioCaps and Clotho\ndatasets with superior performances, verify the effectiveness of our approach.\nNotably, without any alterations, our DiffATR consistently exhibits strong\nperformance in out-of-domain retrieval settings.\n","authors":["Yifei Xin","Xuxin Cheng","Zhihong Zhu","Xusheng Yang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2409.10025v2.pdf","comment":"Accepted by Interspeech2024"},{"id":"http://arxiv.org/abs/2410.13293v1","updated":"2024-10-17T07:46:49Z","published":"2024-10-17T07:46:49Z","title":"SBI-RAG: Enhancing Math Word Problem Solving for Students through\n  Schema-Based Instruction and Retrieval-Augmented Generation","summary":"  Many students struggle with math word problems (MWPs), often finding it\ndifficult to identify key information and select the appropriate mathematical\noperations.Schema-based instruction (SBI) is an evidence-based strategy that\nhelps students categorize problems based on their structure, improving\nproblem-solving accuracy. Building on this, we propose a Schema-Based\nInstruction Retrieval-Augmented Generation (SBI-RAG) framework that\nincorporates a large language model (LLM).Our approach emphasizes step-by-step\nreasoning by leveraging schemas to guide solution generation. We evaluate its\nperformance on the GSM8K dataset, comparing it with GPT-4 and GPT-3.5 Turbo,\nand introduce a \"reasoning score\" metric to assess solution quality. Our\nfindings suggest that SBI-RAG enhances reasoning clarity and problem-solving\naccuracy, potentially providing educational benefits for students\n","authors":["Prakhar Dixit","Tim Oates"],"pdf_url":"https://arxiv.org/pdf/2410.13293v1.pdf","comment":"Accepted to the 4th MATH-AI Workshop at NeurIPS'24"},{"id":"http://arxiv.org/abs/2410.13248v1","updated":"2024-10-17T06:15:00Z","published":"2024-10-17T06:15:00Z","title":"Disentangling Likes and Dislikes in Personalized Generative Explainable\n  Recommendation","summary":"  Recent research on explainable recommendation generally frames the task as a\nstandard text generation problem, and evaluates models simply based on the\ntextual similarity between the predicted and ground-truth explanations.\nHowever, this approach fails to consider one crucial aspect of the systems:\nwhether their outputs accurately reflect the users' (post-purchase) sentiments,\ni.e., whether and why they would like and/or dislike the recommended items. To\nshed light on this issue, we introduce new datasets and evaluation methods that\nfocus on the users' sentiments. Specifically, we construct the datasets by\nexplicitly extracting users' positive and negative opinions from their\npost-purchase reviews using an LLM, and propose to evaluate systems based on\nwhether the generated explanations 1) align well with the users' sentiments,\nand 2) accurately identify both positive and negative opinions of users on the\ntarget items. We benchmark several recent models on our datasets and\ndemonstrate that achieving strong performance on existing metrics does not\nensure that the generated explanations align well with the users' sentiments.\nLastly, we find that existing models can provide more sentiment-aware\nexplanations when the users' (predicted) ratings for the target items are\ndirectly fed into the models as input. We will release our code and datasets\nupon acceptance.\n","authors":["Ryotaro Shimizu","Takashi Wada","Yu Wang","Johannes Kruse","Sean O'Brien","Sai HtaungKham","Linxin Song","Yuya Yoshikawa","Yuki Saito","Fugee Tsung","Masayuki Goto","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2410.13248v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13226v1","updated":"2024-10-17T05:17:01Z","published":"2024-10-17T05:17:01Z","title":"Research on Travel Route Planing Problems Based on Greedy Algorithm","summary":"  The greedy algorithm based route planning problem is a method of finding the\noptimal or near optimal route between a given starting and ending point. This\narticle first uses PCA method to reduce the dimensionality of urban evaluation\nindicators, extracts key principal components, and KMO and TOPSIS algorithms to\nreduce the dimensionality of the data. Secondly, for datasets that have not\npassed the KMO test, a comprehensive evaluation will be conducted using the\nentropy weight method and TOPSIS method. Finally, based on the greedy\nalgorithm, a route planning algorithm was proposed and optimized to provide\npersonalized route customization according to the different needs of tourists.\nWe also took into account the local travel efficiency, the time required to\nvisit tourist attractions, and necessary daily rest time to reduce costs and\navoid falling into the local optimal solution.\n","authors":["Yiquan Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13217v1","updated":"2024-10-17T04:48:06Z","published":"2024-10-17T04:48:06Z","title":"MixEHR-Nest: Identifying Subphenotypes within Electronic Health Records\n  through Hierarchical Guided-Topic Modeling","summary":"  Automatic subphenotyping from electronic health records (EHRs)provides\nnumerous opportunities to understand diseases with unique subgroups and enhance\npersonalized medicine for patients. However, existing machine learning\nalgorithms either focus on specific diseases for better interpretability or\nproduce coarse-grained phenotype topics without considering nuanced disease\npatterns. In this study, we propose a guided topic model, MixEHR-Nest, to infer\nsub-phenotype topics from thousands of disease using multi-modal EHR data.\nSpecifically, MixEHR-Nest detects multiple subtopics from each phenotype topic,\nwhose prior is guided by the expert-curated phenotype concepts such as\nPhenotype Codes (PheCodes) or Clinical Classification Software (CCS) codes. We\nevaluated MixEHR-Nest on two EHR datasets: (1) the MIMIC-III dataset consisting\nof over 38 thousand patients from intensive care unit (ICU) from Beth Israel\nDeaconess Medical Center (BIDMC) in Boston, USA; (2) the healthcare\nadministrative database PopHR, comprising 1.3 million patients from Montreal,\nCanada. Experimental results demonstrate that MixEHR-Nest can identify\nsubphenotypes with distinct patterns within each phenotype, which are\npredictive for disease progression and severity. Consequently, MixEHR-Nest\ndistinguishes between type 1 and type 2 diabetes by inferring subphenotypes\nusing CCS codes, which do not differentiate these two subtype concepts.\nAdditionally, MixEHR-Nest not only improved the prediction accuracy of\nshort-term mortality of ICU patients and initial insulin treatment in diabetic\npatients but also revealed the contributions of subphenotypes. For longitudinal\nanalysis, MixEHR-Nest identified subphenotypes of distinct age prevalence under\nthe same phenotypes, such as asthma, leukemia, epilepsy, and depression. The\nMixEHR-Nest software is available at GitHub:\nhttps://github.com/li-lab-mcgill/MixEHR-Nest.\n","authors":["Ruohan Wang","Zilong Wang","Ziyang Song","David Buckeridge","Yue Li"],"pdf_url":"https://arxiv.org/pdf/2410.13217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05666v7","updated":"2024-10-17T03:23:49Z","published":"2024-06-09T06:49:22Z","title":"Probability Distribution Learning: A theoretical framework for Deep\n  Learning","summary":"  This paper introduces probability distribution learning (PD learning), a\nnovel theoretical learning framework. Departing from the traditional\nstatistical learning framework, PD learning focuses on learning the underlying\nprobability distribution, which is modeled as a random variable within the\nprobability simplex. Within this framework, the learning error is decomposed\ninto uncertainty, estimation error, and the model's fitting error.\nSubsequently, we present the methodology for calculating uncertainty, along\nwith optimization strategies for both estimation error and fitting error. Given\nthat minimizing the fitting error typically constitutes a non-convex\noptimization problem, we introduce a standard loss function and the gradient\nstructural control (GSC) algorithm, and demonstrate that by employing this\nfunction, the optima of fitting error minimization can be approached by\nreducing the gradient norm and structural error. Furthermore, we apply the PD\nlearning framework to deep learning, elucidating the mechanisms by which\ntechniques such as random parameter initialization, over-parameterization,\nbias-variance trade-off, and dropout influence deep model training. Finally,\nexperimental results on various models validate the effectiveness of the\nproposed framework.\n","authors":["Binchuan Qi","Wei Gong","Li Li"],"pdf_url":"https://arxiv.org/pdf/2406.05666v7.pdf","comment":"arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors"},{"id":"http://arxiv.org/abs/2402.16872v2","updated":"2024-10-17T02:53:23Z","published":"2024-01-29T03:30:15Z","title":"NFT1000: A Cross-Modal Dataset for Non-Fungible Token Retrieval","summary":"  With the rise of \"Metaverse\" and \"Web 3.0\", Non-Fungible Token (NFT) has\nemerged as a kind of pivotal digital asset, garnering significant attention. By\nthe end of March 2024, more than 1.7 billion NFTs have been minted across\nvarious blockchain platforms. To effectively locate a desired NFT, conducting\nsearches within a vast array of NFTs is essential. The challenge in NFT\nretrieval is heightened due to the high degree of similarity among different\nNFTs, regarding regional and semantic aspects. In this paper, we will introduce\na benchmark dataset named \"NFT Top1000 Visual-Text Dataset\" (NFT1000),\ncontaining 7.56 million image-text pairs, and being collected from 1000 most\nfamous PFP1 NFT collections2 by sales volume on the Ethereum blockchain. Based\non this dataset and leveraging the CLIP series of pre-trained models as our\nfoundation, we propose the dynamic masking fine-tuning scheme. This innovative\napproach results in a 7.4\\% improvement in the top1 accuracy rate, while\nutilizing merely 13\\% of the total training data (0.79 million vs. 6.1\nmillion). We also propose a robust metric Comprehensive Variance Index (CVI) to\nassess the similarity and retrieval difficulty of visual-text pairs data. The\ndataset will be released as an open-source resource. For more details, please\nrefer to: https://github.com/ShuxunoO/NFT-Net.git.\n","authors":["Shuxun Wang","Yunfei Lei","Ziqi Zhang","Wei Liu","Haowei Liu","Li Yang","Wenjuan Li","Bing Li","Weiming Hu"],"pdf_url":"https://arxiv.org/pdf/2402.16872v2.pdf","comment":"11 pages,12figures to be published in ACM Multimedia 2024, see\n  https://openreview.net/forum?id=xUtNrKH8iB&noteId=xUtNrKH8iB"},{"id":"http://arxiv.org/abs/2408.01262v4","updated":"2024-10-17T02:20:47Z","published":"2024-08-02T13:35:11Z","title":"RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework","summary":"  Retrieval-Augmented Generation (RAG) is a powerful approach that enables\nlarge language models (LLMs) to incorporate external knowledge. However,\nevaluating the effectiveness of RAG systems in specialized scenarios remains\nchallenging due to the high costs of data construction and the lack of suitable\nevaluation metrics. This paper introduces RAGEval, a framework designed to\nassess RAG systems across diverse scenarios by generating high-quality\ndocuments, questions, answers, and references through a schema-based pipeline.\nWith a focus on factual accuracy, we propose three novel metrics Completeness,\nHallucination, and Irrelevance to rigorously evaluate LLM-generated responses.\nExperimental results show that RAGEval outperforms zero-shot and one-shot\nmethods in terms of clarity, safety, conformity, and richness of generated\nsamples. Furthermore, the use of LLMs for scoring the proposed metrics\ndemonstrates a high level of consistency with human evaluations. RAGEval\nestablishes a new paradigm for evaluating RAG systems in real-world\napplications.\n","authors":["Kunlun Zhu","Yifan Luo","Dingling Xu","Ruobing Wang","Shi Yu","Shuo Wang","Yukun Yan","Zhenghao Liu","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2408.01262v4.pdf","comment":"https://github.com/OpenBMB/RAGEval"},{"id":"http://arxiv.org/abs/2406.00944v2","updated":"2024-10-17T02:15:11Z","published":"2024-06-03T02:56:14Z","title":"A Theory for Token-Level Harmonization in Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\nlarge language models (LLMs). Studies show that while RAG provides valuable\nexternal information (benefit), it may also mislead LLMs (detriment) with noisy\nor incorrect retrieved texts. Although many existing methods attempt to\npreserve benefit and avoid detriment, they lack a theoretical explanation for\nRAG. The benefit and detriment in the next token prediction of RAG remain a\nblack box that cannot be quantified or compared in an explainable manner, so\nexisting methods are data-driven, need additional utility evaluators or\npost-hoc. This paper takes the first step towards providing a theory to explain\nand trade off the benefit and detriment in RAG. First, we model RAG as the\nfusion between distribution of LLMs knowledge and distribution of retrieved\ntexts. Then, we formalize the trade-off between the value of external knowledge\n(benefit) and its potential risk of misleading LLMs (detriment) in next token\nprediction of RAG by distribution difference in this fusion. Finally, we prove\nthat the actual effect of RAG on the token, which is the comparison between\nbenefit and detriment, can be predicted without any training or accessing the\nutility of retrieval. Based on our theory, we propose a practical novel method,\nTok-RAG, which achieves collaborative generation between the pure LLM and RAG\nat token level to preserve benefit and avoid detriment. Experiments in\nreal-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\neffectiveness of our method and support our theoretical findings.\n","authors":["Shicheng Xu","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.00944v2.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2410.13125v1","updated":"2024-10-17T01:27:57Z","published":"2024-10-17T01:27:57Z","title":"Transformers4NewsRec: A Transformer-based News Recommendation Framework","summary":"  Pre-trained transformer models have shown great promise in various natural\nlanguage processing tasks, including personalized news recommendations. To\nharness the power of these models, we introduce Transformers4NewsRec, a new\nPython framework built on the \\textbf{Transformers} library. This framework is\ndesigned to unify and compare the performance of various news recommendation\nmodels, including deep neural networks and graph-based models.\nTransformers4NewsRec offers flexibility in terms of model selection, data\npreprocessing, and evaluation, allowing both quantitative and qualitative\nanalysis.\n","authors":["Dairui Liu","Honghui Du","Boming Yang","Neil Hurley","Aonghus Lawlor","Irene Li","Derek Greene","Ruihai Dong"],"pdf_url":"https://arxiv.org/pdf/2410.13125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13118v1","updated":"2024-10-17T01:12:48Z","published":"2024-10-17T01:12:48Z","title":"Retrieval-Enhanced Named Entity Recognition","summary":"  When combined with In-Context Learning, a technique that enables models to\nadapt to new tasks by incorporating task-specific examples or demonstrations\ndirectly within the input prompt, autoregressive language models have achieved\ngood performance in a wide range of tasks and applications. However, this\ncombination has not been properly explored in the context of named entity\nrecognition, where the structure of this task poses unique challenges. We\npropose RENER (Retrieval-Enhanced Named Entity Recognition), a technique for\nnamed entity recognition using autoregressive language models based on\nIn-Context Learning and information retrieval techniques. When presented with\nan input text, RENER fetches similar examples from a dataset of training\nexamples that are used to enhance a language model to recognize named entities\nfrom this input text. RENER is modular and independent of the underlying\nlanguage model and information retrieval algorithms. Experimental results show\nthat in the CrossNER collection we achieve state-of-the-art performance with\nthe proposed technique and that information retrieval can increase the F-score\nby up to 11 percentage points.\n","authors":["Enzo Shiraishi","Raphael Y. de Camargo","Henrique L. P. Silva","Ronaldo C. Prati"],"pdf_url":"https://arxiv.org/pdf/2410.13118v1.pdf","comment":"13 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2410.13117v1","updated":"2024-10-17T01:02:04Z","published":"2024-10-17T01:02:04Z","title":"Preference Diffusion for Recommendation","summary":"  Recommender systems predict personalized item rankings based on user\npreference distributions derived from historical behavior data. Recently,\ndiffusion models (DMs) have gained attention in recommendation for their\nability to model complex distributions, yet current DM-based recommenders often\nrely on traditional objectives like mean squared error (MSE) or recommendation\nobjectives, which are not optimized for personalized ranking tasks or fail to\nfully leverage DM's generative potential. To address this, we propose\nPreferDiff, a tailored optimization objective for DM-based recommenders.\nPreferDiff transforms BPR into a log-likelihood ranking objective and\nintegrates multiple negative samples to better capture user preferences.\nSpecifically, we employ variational inference to handle the intractability\nthrough minimizing the variational upper bound and replaces MSE with cosine\nerror to improve alignment with recommendation tasks. Finally, we balance\nlearning generation and preference to enhance the training stability of DMs.\nPreferDiff offers three key benefits: it is the first personalized ranking loss\ndesigned specifically for DM-based recommenders and it improves ranking and\nfaster convergence by addressing hard negatives. We also prove that it is\ntheoretically connected to Direct Preference Optimization which indicates that\nit has the potential to align user preferences in DM-based recommenders via\ngenerative modeling. Extensive experiments across three benchmarks validate its\nsuperior recommendation performance and commendable general sequential\nrecommendation capabilities. Our codes are available at\n\\url{https://github.com/lswhim/PreferDiff}.\n","authors":["Shuo Liu","An Zhang","Guoqing Hu","Hong Qian","Tat-seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.13117v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.13733v1","updated":"2024-10-17T16:36:38Z","published":"2024-10-17T16:36:38Z","title":"Improving Multi-modal Large Language Model through Boosting Vision\n  Capabilities","summary":"  We focus on improving the visual understanding capability for boosting the\nvision-language models. We propose \\textbf{Arcana}, a multiModal language\nmodel, which introduces two crucial techniques. First, we present Multimodal\nLoRA (MM-LoRA), a module designed to enhance the decoder. Unlike traditional\nlanguage-driven decoders, MM-LoRA consists of two parallel LoRAs -- one for\nvision and one for language -- each with its own parameters. This disentangled\nparameters design allows for more specialized learning in each modality and\nbetter integration of multimodal information. Second, we introduce the Query\nLadder adapter (QLadder) to improve the visual encoder. QLadder employs a\nlearnable ``\\textit{ladder}'' structure to deeply aggregates the intermediate\nrepresentations from the frozen pretrained visual encoder (e.g., CLIP image\nencoder). This enables the model to learn new and informative visual features,\nas well as remaining the powerful capabilities of the pretrained visual\nencoder. These techniques collectively enhance Arcana's visual perception\npower, enabling it to leverage improved visual information for more accurate\nand contextually relevant outputs across various multimodal scenarios.\nExtensive experiments and ablation studies demonstrate the effectiveness and\ngeneralization capability of our Arcana. The code and re-annotated data are\navailable at \\url{https://arcana-project-page.github.io}.\n","authors":["Yanpeng Sun","Huaxin Zhang","Qiang Chen","Xinyu Zhang","Nong Sang","Gang Zhang","Jingdong Wang","Zechao Li"],"pdf_url":"https://arxiv.org/pdf/2410.13733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12407v2","updated":"2024-10-17T15:59:34Z","published":"2024-10-16T09:42:29Z","title":"Beyond Coarse-Grained Matching in Video-Text Retrieval","summary":"  Video-text retrieval has seen significant advancements, yet the ability of\nmodels to discern subtle differences in captions still requires verification.\nIn this paper, we introduce a new approach for fine-grained evaluation. Our\napproach can be applied to existing datasets by automatically generating hard\nnegative test captions with subtle single-word variations across nouns, verbs,\nadjectives, adverbs, and prepositions. We perform comprehensive experiments\nusing four state-of-the-art models across two standard benchmarks (MSR-VTT and\nVATEX) and two specially curated datasets enriched with detailed descriptions\n(VLN-UVO and VLN-OOPS), resulting in a number of novel insights: 1) our\nanalyses show that the current evaluation benchmarks fall short in detecting a\nmodel's ability to perceive subtle single-word differences, 2) our fine-grained\nevaluation highlights the difficulty models face in distinguishing such subtle\nvariations. To enhance fine-grained understanding, we propose a new baseline\nthat can be easily combined with current methods. Experiments on our\nfine-grained evaluations demonstrate that this approach enhances a model's\nability to understand fine-grained differences.\n","authors":["Aozhu Chen","Hazel Doughty","Xirong Li","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12407v2.pdf","comment":"Accepted to ACCV 2024"},{"id":"http://arxiv.org/abs/2410.13647v1","updated":"2024-10-17T15:13:26Z","published":"2024-10-17T15:13:26Z","title":"Multimodal growth and development assessment model","summary":"  With the development of social economy and the improvement of people's\nattention to health, the growth and development of children and adolescents has\nbecome an important indicator to measure the level of national health.\nTherefore, accurate and timely assessment of children's growth and development\nhas become increasingly important. At the same time, global health\ninequalities, especially child malnutrition and stunting in developing\ncountries, urgently require effective assessment tools to monitor and\nintervene. In recent years, the rapid development of technologies such as big\ndata, artificial intelligence, and cloud computing, and the cross-integration\nof multiple disciplines such as biomedicine, statistics, and computer science\nhave promoted the rapid development of large-scale models for growth and\ndevelopment assessment. However, there are still problems such as too single\nevaluation factors, inaccurate diagnostic results, and inability to give\naccurate and reasonable recommendations. The multi-modal growth and development\nassessment model uses the public data set of RSNA ( North American College of\nRadiology ) as the training set, and the data set of the Department of\nPediatrics of Huaibei People's Hospital as the open source test set. The\nembedded ICL module enables the model to quickly adapt and identify the tasks\nthat need to be done to ensure that under the premise of considering multiple\nevaluation factors, accurate diagnosis results and reasonable medical\nrecommendations are given, so as to provide solutions to the above problems and\npromote the development of the medical field.\n","authors":["Ying Li","Zichen Song","Zijie Gong","Sitan Huang","Jiewei Ge"],"pdf_url":"https://arxiv.org/pdf/2410.13647v1.pdf","comment":"7 Pages 7 Figures"},{"id":"http://arxiv.org/abs/2410.13419v1","updated":"2024-10-17T10:41:52Z","published":"2024-10-17T10:41:52Z","title":"MeloTrans: A Text to Symbolic Music Generation Model Following Human\n  Composition Habit","summary":"  At present, neural network models show powerful sequence prediction ability\nand are used in many automatic composition models. In comparison, the way\nhumans compose music is very different from it. Composers usually start by\ncreating musical motifs and then develop them into music through a series of\nrules. This process ensures that the music has a specific structure and\nchanging pattern. However, it is difficult for neural network models to learn\nthese composition rules from training data, which results in a lack of\nmusicality and diversity in the generated music. This paper posits that\nintegrating the learning capabilities of neural networks with human-derived\nknowledge may lead to better results. To archive this, we develop the\nPOP909$\\_$M dataset, the first to include labels for musical motifs and their\nvariants, providing a basis for mimicking human compositional habits. Building\non this, we propose MeloTrans, a text-to-music composition model that employs\nprinciples of motif development rules. Our experiments demonstrate that\nMeloTrans excels beyond existing music generation models and even surpasses\nLarge Language Models (LLMs) like ChatGPT-4. This highlights the importance of\nmerging human insights with neural network capabilities to achieve superior\nsymbolic music generation.\n","authors":["Yutian Wang","Wanyin Yang","Zhenrong Dai","Yilong Zhang","Kun Zhao","Hui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.13419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07728v4","updated":"2024-10-17T09:54:06Z","published":"2024-07-10T15:00:08Z","title":"SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature\n  Disentanglement and Enhancement","summary":"  Singing voice conversion (SVC) aims to convert a singer's voice to another\nsinger's from a reference audio while keeping the original semantics. However,\nexisting SVC methods can hardly perform zero-shot due to incomplete feature\ndisentanglement or dependence on the speaker look-up table. We propose the\nfirst open-source high-quality zero-shot SVC model SaMoye that can convert\nsinging to human and non-human timbre. SaMoye disentangles the singing voice's\nfeatures into content, timbre, and pitch features, where we combine multiple\nASR models and compress the content features to reduce timbre leaks. Besides,\nwe enhance the timbre features by unfreezing the speaker encoder and mixing the\nspeaker embedding with top-3 similar speakers. We also establish an\nunparalleled large-scale dataset to guarantee zero-shot performance, which\ncomprises more than 1,815 hours of pure singing voice and 6,367 speakers. We\nconduct objective and subjective experiments to find that SaMoye outperforms\nother models in zero-shot SVC tasks even under extreme conditions like\nconverting singing to animals' timbre. The code and weight of SaMoye are\navailable on https://github.com/CarlWangChina/SaMoye-SVC. The weights, code,\ndataset, and documents of SaMoye are publicly available on\n\\url{https://github.com/CarlWangChina/SaMoye-SVC}.\n","authors":["Zihao Wang","Le Ma","Yongsheng Feng","Xin Pan","Yuhang Jin","Kejun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.07728v4.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.13360v1","updated":"2024-10-17T09:10:26Z","published":"2024-10-17T09:10:26Z","title":"Remember, Retrieve and Generate: Understanding Infinite Visual Concepts\n  as Your Personalized Assistant","summary":"  The development of large language models (LLMs) has significantly enhanced\nthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,\nlack of user-specific knowledge still restricts their application in human's\ndaily life. In this paper, we introduce the Retrieval Augmented Personalization\n(RAP) framework for MLLMs' personalization. Starting from a general MLLM, we\nturn it into a personalized assistant in three steps. (a) Remember: We design a\nkey-value database to store user-related information, e.g., user's name, avatar\nand other attributes. (b) Retrieve: When the user initiates a conversation, RAP\nwill retrieve relevant information from the database using a multimodal\nretriever. (c) Generate: The input query and retrieved concepts' information\nare fed into MLLMs to generate personalized, knowledge-augmented responses.\nUnlike previous methods, RAP allows real-time concept editing via updating the\nexternal database. To further improve generation quality and alignment with\nuser-specific information, we design a pipeline for data collection and create\na specialized dataset for personalized training of MLLMs. Based on the dataset,\nwe train a series of MLLMs as personalized multimodal assistants. By\npretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual\nconcepts without additional finetuning. Our models demonstrate outstanding\nflexibility and generation quality across a variety of tasks, such as\npersonalized image captioning, question answering and visual recognition. The\ncode, data and models are available at https://github.com/Hoar012/RAP-MLLM.\n","authors":["Haoran Hao","Jiaming Han","Changsheng Li","Yu-Feng Li","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2410.13360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11522v2","updated":"2024-10-17T08:18:14Z","published":"2024-10-15T11:48:31Z","title":"Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero\n  Shot Music Emotion Prediction","summary":"  In this work, we present a novel method for music emotion recognition that\nleverages Large Language Model (LLM) embeddings for label alignment across\nmultiple datasets and zero-shot prediction on novel categories. First, we\ncompute LLM embeddings for emotion labels and apply non-parametric clustering\nto group similar labels, across multiple datasets containing disjoint labels.\nWe use these cluster centers to map music features (MERT) to the LLM embedding\nspace. To further enhance the model, we introduce an alignment regularization\nthat enables dissociation of MERT embeddings from different clusters. This\nfurther enhances the model's ability to better adaptation to unseen datasets.\nWe demonstrate the effectiveness of our approach by performing zero-shot\ninference on a new dataset, showcasing its ability to generalize to unseen\nlabels without additional training.\n","authors":["Renhang Liu","Abhinaba Roy","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2410.11522v2.pdf","comment":null}]},"2024-10-16T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.13070v1","updated":"2024-10-16T21:53:48Z","published":"2024-10-16T21:53:48Z","title":"Is Semantic Chunking Worth the Computational Cost?","summary":"  Recent advances in Retrieval-Augmented Generation (RAG) systems have\npopularized semantic chunking, which aims to improve retrieval performance by\ndividing documents into semantically coherent segments. Despite its growing\nadoption, the actual benefits over simpler fixed-size chunking, where documents\nare split into consecutive, fixed-size segments, remain unclear. This study\nsystematically evaluates the effectiveness of semantic chunking using three\ncommon retrieval-related tasks: document retrieval, evidence retrieval, and\nretrieval-based answer generation. The results show that the computational\ncosts associated with semantic chunking are not justified by consistent\nperformance gains. These findings challenge the previous assumptions about\nsemantic chunking and highlight the need for more efficient chunking strategies\nin RAG systems.\n","authors":["Renyi Qu","Ruixuan Tu","Forrest Bao"],"pdf_url":"https://arxiv.org/pdf/2410.13070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13051v1","updated":"2024-10-16T21:24:13Z","published":"2024-10-16T21:24:13Z","title":"Supply Chain Network Extraction and Entity Classification Leveraging\n  Large Language Models","summary":"  Supply chain networks are critical to the operational efficiency of\nindustries, yet their increasing complexity presents significant challenges in\nmapping relationships and identifying the roles of various entities.\nTraditional methods for constructing supply chain networks rely heavily on\nstructured datasets and manual data collection, limiting their scope and\nefficiency. In contrast, recent advancements in Natural Language Processing\n(NLP) and large language models (LLMs) offer new opportunities for discovering\nand analyzing supply chain networks using unstructured text data. This paper\nproposes a novel approach that leverages LLMs to extract and process raw\ntextual information from publicly available sources to construct a\ncomprehensive supply chain graph. We focus on the civil engineering sector as a\ncase study, demonstrating how LLMs can uncover hidden relationships among\ncompanies, projects, and other entities. Additionally, we fine-tune an LLM to\nclassify entities within the supply chain graph, providing detailed insights\ninto their roles and relationships. The results show that domain-specific\nfine-tuning improves classification accuracy, highlighting the potential of\nLLMs for industry-specific supply chain analysis. Our contributions include the\ndevelopment of a supply chain graph for the civil engineering sector, as well\nas a fine-tuned LLM model that enhances entity classification and understanding\nof supply chain networks.\n","authors":["Tong Liu","Hadi Meidani"],"pdf_url":"https://arxiv.org/pdf/2410.13051v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2410.13047v1","updated":"2024-10-16T21:17:18Z","published":"2024-10-16T21:17:18Z","title":"LLM Confidence Evaluation Measures in Zero-Shot CSS Classification","summary":"  Assessing classification confidence is critical for leveraging large language\nmodels (LLMs) in automated labeling tasks, especially in the sensitive domains\npresented by Computational Social Science (CSS) tasks. In this paper, we make\nthree key contributions: (1) we propose an uncertainty quantification (UQ)\nperformance measure tailored for data annotation tasks, (2) we compare, for the\nfirst time, five different UQ strategies across three distinct LLMs and CSS\ndata annotation tasks, (3) we introduce a novel UQ aggregation strategy that\neffectively identifies low-confidence LLM annotations and disproportionately\nuncovers data incorrectly labeled by the LLMs. Our results demonstrate that our\nproposed UQ aggregation strategy improves upon existing methods andcan be used\nto significantly improve human-in-the-loop data annotation processes.\n","authors":["David Farr","Iain Cruickshank","Nico Manzonelli","Nicholas Clark","Kate Starbird","Jevin West"],"pdf_url":"https://arxiv.org/pdf/2410.13047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13037v1","updated":"2024-10-16T20:52:39Z","published":"2024-10-16T20:52:39Z","title":"LFOSum: Summarizing Long-form Opinions with Large Language Models","summary":"  Online reviews play a pivotal role in influencing consumer decisions across\nvarious domains, from purchasing products to selecting hotels or restaurants.\nHowever, the sheer volume of reviews -- often containing repetitive or\nirrelevant content -- leads to information overload, making it challenging for\nusers to extract meaningful insights. Traditional opinion summarization models\nface challenges in handling long inputs and large volumes of reviews, while\nnewer Large Language Model (LLM) approaches often fail to generate accurate and\nfaithful summaries. To address those challenges, this paper introduces (1) a\nnew dataset of long-form user reviews, each entity comprising over a thousand\nreviews, (2) two training-free LLM-based summarization approaches that scale to\nlong inputs, and (3) automatic evaluation metrics. Our dataset of user reviews\nis paired with in-depth and unbiased critical summaries by domain experts,\nserving as a reference for evaluation. Additionally, our novel reference-free\nevaluation metrics provide a more granular, context-sensitive assessment of\nsummary faithfulness. We benchmark several open-source and closed-source LLMs\nusing our methods. Our evaluation reveals that LLMs still face challenges in\nbalancing sentiment and format adherence in long-form summaries, though\nopen-source models can narrow the gap when relevant information is retrieved in\na focused manner.\n","authors":["Mir Tafseer Nayeem","Davood Rafiei"],"pdf_url":"https://arxiv.org/pdf/2410.13037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12956v1","updated":"2024-10-16T18:44:28Z","published":"2024-10-16T18:44:28Z","title":"Towards Computational Analysis of Pansori Singing","summary":"  Pansori is one of the most representative vocal genres of Korean traditional\nmusic, which has an elaborated vocal melody line with strong vibrato. Although\nthe music is transmitted orally without any music notation, transcribing\npansori music in Western staff notation has been introduced for several\npurposes, such as documentation of music, education, or research. In this\npaper, we introduce computational analysis of pansori based on both audio and\ncorresponding transcription, how modern Music Information Retrieval tasks can\nbe used in analyzing traditional music and how it revealed different audio\ncharacteristics of what pansori contains.\n","authors":["Sangheon Park","Danbinaerin Han","Dasaem Jeong"],"pdf_url":"https://arxiv.org/pdf/2410.12956v1.pdf","comment":"Late-Breaking Demo Session of the 25th International Society for\n  Music Information Retrieval (ISMIR) Conference, 2024"},{"id":"http://arxiv.org/abs/2407.18940v2","updated":"2024-10-16T18:37:15Z","published":"2024-07-10T18:00:03Z","title":"LitSearch: A Retrieval Benchmark for Scientific Literature Search","summary":"  Literature search questions, such as \"Where can I find research on the\nevaluation of consistency in generated summaries?\" pose significant challenges\nfor modern search engines and retrieval systems. These questions often require\na deep understanding of research concepts and the ability to reason across\nentire articles. In this work, we introduce LitSearch, a retrieval benchmark\ncomprising 597 realistic literature search queries about recent ML and NLP\npapers. LitSearch is constructed using a combination of (1) questions generated\nby GPT-4 based on paragraphs containing inline citations from research papers\nand (2) questions manually written by authors about their recently published\npapers. All LitSearch questions were manually examined or edited by experts to\nensure high quality. We extensively benchmark state-of-the-art retrieval models\nand also evaluate two LLM-based reranking pipelines. We find a significant\nperformance gap between BM25 and state-of-the-art dense retrievers, with a\n24.8% absolute difference in recall@5. The LLM-based reranking strategies\nfurther improve the best-performing dense retriever by 4.4%. Additionally,\ncommercial search engines and research tools like Google Search perform poorly\non LitSearch, lagging behind the best dense retriever by up to 32 recall\npoints. Taken together, these results show that LitSearch is an informative new\ntestbed for retrieval systems while catering to a real-world use case.\n","authors":["Anirudh Ajith","Mengzhou Xia","Alexis Chevalier","Tanya Goyal","Danqi Chen","Tianyu Gao"],"pdf_url":"https://arxiv.org/pdf/2407.18940v2.pdf","comment":"Accepted by EMNLP 2024. Dataset and code are available at\n  https://github.com/princeton-nlp/LitSearch"},{"id":"http://arxiv.org/abs/2406.12593v2","updated":"2024-10-16T13:45:54Z","published":"2024-06-18T13:25:18Z","title":"PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental\n  Learning for Document Retrieval","summary":"  Differentiable Search Index (DSI) utilizes Pre-trained Language Models (PLMs)\nfor efficient document retrieval without relying on external indexes. However,\nDSI needs full re-training to handle updates in dynamic corpora, causing\nsignificant computational inefficiencies. We introduce PromptDSI, a\nprompt-based rehearsal-free approach for instance-wise incremental learning\ndocument retrieval. PromptDSI attaches prompts to the frozen PLM's encoder of\nDSI, leveraging its powerful representation to efficiently index new corpora\nwhile maintaining a balance between stability and plasticity. We eliminate the\ninitial forward pass of prompt-based continual learning methods that doubles\ntraining and inference time. Moreover, we propose a topic-aware prompt pool\nthat employs neural topic embeddings as fixed keys. This strategy ensures\ndiverse and effective prompt usage, addressing the challenge of parameter\nunderutilization caused by the collapse of the query-key matching mechanism.\nOur empirical evaluations demonstrate that BERT-based PromptDSI matches IncDSI\nin managing forgetting while improving new corpora performance by more than 4%\nHits@10 on NQ320k and upto 3% MRR@10 on MS MARCO 300k.\n","authors":["Tuan-Luc Huynh","Thuy-Trang Vu","Weiqing Wang","Yinwei Wei","Trung Le","Dragan Gasevic","Yuan-Fang Li","Thanh-Toan Do"],"pdf_url":"https://arxiv.org/pdf/2406.12593v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2406.14162v3","updated":"2024-10-16T13:16:25Z","published":"2024-06-20T10:04:09Z","title":"DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation","summary":"  Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}.\n","authors":["Jingwei Ni","Tobias Schimanski","Meihong Lin","Mrinmaya Sachan","Elliott Ash","Markus Leippold"],"pdf_url":"https://arxiv.org/pdf/2406.14162v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.19014v3","updated":"2024-10-16T12:55:55Z","published":"2024-09-24T01:40:50Z","title":"FLEX: Expert-level False-Less EXecution Metric for Reliable Text-to-SQL\n  Benchmark","summary":"  Text-to-SQL systems have become crucial for translating natural language into\nSQL queries in various industries, enabling non-technical users to perform\ncomplex data operations. The need for accurate evaluation methods has increased\nas these systems have grown more sophisticated. However, the Execution Accuracy\n(EX), the most prevalent evaluation metric, still shows many false positives\nand negatives. Thus, this paper introduces FLEX (False-Less EXecution), a novel\napproach to evaluating text-to-SQL systems using large language models (LLMs)\nto emulate human expert-level evaluation of SQL queries. Our metric improves\nagreement with human experts (from 62 to 87.04 in Cohen's kappa) with\ncomprehensive context and sophisticated criteria. Our extensive experiments\nyield several key insights: (1) Models' performance increases by over 2.6\npoints on average, substantially affecting rankings on Spider and BIRD\nbenchmarks; (2) The underestimation of models in EX primarily stems from\nannotation quality issues; and (3) Model performance on particularly\nchallenging questions tends to be overestimated. This work contributes to a\nmore accurate and nuanced evaluation of text-to-SQL systems, potentially\nreshaping our understanding of state-of-the-art performance in this field.\n","authors":["Heegyu Kim","Taeyang Jeon","Seunghwan Choi","Seungtaek Choi","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2409.19014v3.pdf","comment":"preprint, under review"},{"id":"http://arxiv.org/abs/2410.12519v1","updated":"2024-10-16T12:54:34Z","published":"2024-10-16T12:54:34Z","title":"RosePO: Aligning LLM-based Recommenders with Human Values","summary":"  Recently, there has been a growing interest in leveraging Large Language\nModels (LLMs) for recommendation systems, which usually adapt a pre-trained LLM\nto the recommendation scenario through supervised fine-tuning (SFT). However,\nboth the pre-training and SFT stages fail to explicitly model the comparative\nrelationships of a user's preferences on different items. To construct a\n\"helpful and harmless\" LLM-based recommender, we propose a general framework --\nRecommendation with smoothing personalized Preference Optimization (RosePO),\nwhich better aligns with customized human values during the post-training\nstage. Specifically, in addition to the input and chosen response that\nnaturally align with SFT data, we design a rejected sampling strategy tailored\nfor enhancing helpfulness, along with two strategies aimed at mitigating biases\nto promote harmlessness. To ensure robustness against uncertain labels present\nin automatically constructed preference data, we introduce a personalized\nsmoothing factor predicted by a preference oracle into the optimization\nobjective. Evaluation on three real-world datasets demonstrates the\neffectiveness of our method, showcasing not only improved recommendation\nperformance but also mitigation of semantic hallucination and popularity bias.\n","authors":["Jiayi Liao","Xiangnan He","Ruobing Xie","Jiancan Wu","Yancheng Yuan","Xingwu Sun","Zhanhui Kang","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.12519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13905v1","updated":"2024-10-16T12:29:22Z","published":"2024-10-16T12:29:22Z","title":"P4GCN: Vertical Federated Social Recommendation with Privacy-Preserving\n  Two-Party Graph Convolution Networks","summary":"  In recent years, graph neural networks (GNNs) have been commonly utilized for\nsocial recommendation systems. However, real-world scenarios often present\nchallenges related to user privacy and business constraints, inhibiting direct\naccess to valuable social information from other platforms. While many existing\nmethods have tackled matrix factorization-based social recommendations without\ndirect social data access, developing GNN-based federated social recommendation\nmodels under similar conditions remains largely unexplored. To address this\nissue, we propose a novel vertical federated social recommendation method\nleveraging privacy-preserving two-party graph convolution networks (P4GCN) to\nenhance recommendation accuracy without requiring direct access to sensitive\nsocial information. First, we introduce a Sandwich-Encryption module to ensure\ncomprehensive data privacy during the collaborative computing process. Second,\nwe provide a thorough theoretical analysis of the privacy guarantees,\nconsidering the participation of both curious and honest parties. Extensive\nexperiments on four real-world datasets demonstrate that P4GCN outperforms\nstate-of-the-art methods in terms of recommendation accuracy. The code is\navailable at https://github.com/WwZzz/P4GCN.\n","authors":["Zheng Wang","Wanwan Wang","Yimin Huang","Zhaopeng Peng","Ziqi Yang","Cheng Wang","Xiaoliang Fan"],"pdf_url":"https://arxiv.org/pdf/2410.13905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12473v1","updated":"2024-10-16T11:41:24Z","published":"2024-10-16T11:41:24Z","title":"Unifying Economic and Language Models for Enhanced Sentiment Analysis of\n  the Oil Market","summary":"  Crude oil, a critical component of the global economy, has its prices\ninfluenced by various factors such as economic trends, political events, and\nnatural disasters. Traditional prediction methods based on historical data have\ntheir limits in forecasting, but recent advancements in natural language\nprocessing bring new possibilities for event-based analysis. In particular,\nLanguage Models (LM) and their advancement, the Generative Pre-trained\nTransformer (GPT), have shown potential in classifying vast amounts of natural\nlanguage. However, these LMs often have difficulty with domain-specific\nterminology, limiting their effectiveness in the crude oil sector. Addressing\nthis gap, we introduce CrudeBERT, a fine-tuned LM specifically for the crude\noil market. The results indicate that CrudeBERT's sentiment scores align more\nclosely with the WTI Futures curve and significantly enhance price predictions,\nunderscoring the crucial role of integrating economic principles into LMs.\n","authors":["Himmet Kaplan","Ralf-Peter Mundani","Heiko Rölke","Albert Weichselbraun","Martin Tschudy"],"pdf_url":"https://arxiv.org/pdf/2410.12473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12451v1","updated":"2024-10-16T10:58:53Z","published":"2024-10-16T10:58:53Z","title":"Mitigating Dual Latent Confounding Biases in Recommender Systems","summary":"  Recommender systems are extensively utilised across various areas to predict\nuser preferences for personalised experiences and enhanced user engagement and\nsatisfaction. Traditional recommender systems, however, are complicated by\nconfounding bias, particularly in the presence of latent confounders that\naffect both item exposure and user feedback. Existing debiasing methods often\nfail to capture the complex interactions caused by latent confounders in\ninteraction data, especially when dual latent confounders affect both the user\nand item sides. To address this, we propose a novel debiasing method that\njointly integrates the Instrumental Variables (IV) approach and identifiable\nVariational Auto-Encoder (iVAE) for Debiased representation learning in\nRecommendation systems, referred to as IViDR. Specifically, IViDR leverages the\nembeddings of user features as IVs to address confounding bias caused by latent\nconfounders between items and user feedback, and reconstructs the embedding of\nitems to obtain debiased interaction data. Moreover, IViDR employs an\nIdentifiable Variational Auto-Encoder (iVAE) to infer identifiable\nrepresentations of latent confounders between item exposure and user feedback\nfrom both the original and debiased interaction data. Additionally, we provide\ntheoretical analyses of the soundness of using IV and the identifiability of\nthe latent representations. Extensive experiments on both synthetic and\nreal-world datasets demonstrate that IViDR outperforms state-of-the-art models\nin reducing bias and providing reliable recommendations.\n","authors":["Jianfeng Deng","Qingfeng Chen","Debo Cheng","Jiuyong Li","Lin Liu","Xiaojing Du"],"pdf_url":"https://arxiv.org/pdf/2410.12451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12400v1","updated":"2024-10-16T09:28:58Z","published":"2024-10-16T09:28:58Z","title":"QUIDS: Query Intent Generation via Dual Space Modeling","summary":"  Query understanding is a crucial component of Information Retrieval (IR),\naimed at identifying the underlying search intent of textual queries. However,\nmost existing approaches oversimplify this task into query classification or\nclustering, which fails to fully capture the nuanced intent behind the query.\nIn this paper, we address the task of query intent generation: to automatically\ngenerate detailed and precise intent descriptions for search queries using\nrelevant and irrelevant documents given a query. These intent descriptions can\nhelp users understand why the search engine considered the top-ranked documents\nrelevant, and provide more transparency to the retrieval process. We propose a\ndual-space model that uses semantic relevance and irrelevance information in\nthe returned documents to explain the understanding of the query intent.\nSpecifically, in the encoding process, we project, separate, and distinguish\nrelevant and irrelevant documents in the representation space. Then, we\nintroduce a semantic decoupling model in the novel disentangling space, where\nthe semantics of irrelevant information are removed from the relevant space,\nensuring that only the essential and relevant intent is captured. This process\nrefines the understanding of the query and provides more accurate explanations\nfor the search results. Experiments on benchmark data demonstrate that our\nmethods produce high-quality query intent descriptions, outperforming existing\nmethods for this task, as well as state-of-the-art query-based summarization\nmethods. A token-level visualization of attention scores reveals that our model\neffectively reduces the focus on irrelevant intent topics. Our findings open up\npromising research and application directions for query intent generation,\nparticularly in exploratory search.\n","authors":["Yumeng Wang","Xiuying Chen","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2410.12400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12890v1","updated":"2024-10-16T08:43:39Z","published":"2024-10-16T08:43:39Z","title":"REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via\n  Model Fusion of Embedding Models","summary":"  Retrieval augmented generation (RAG) pipelines are commonly used in tasks\nsuch as question-answering (QA), relying on retrieving relevant documents from\na vector store computed using a pretrained embedding model. However, if the\nretrieved context is inaccurate, the answers generated using the large language\nmodel (LLM) may contain errors or hallucinations. Although pretrained embedding\nmodels have advanced, adapting them to new domains remains challenging.\nFine-tuning is a potential solution, but industry settings often lack the\nnecessary fine-tuning data. To address these challenges, we propose REFINE, a\nnovel technique that generates synthetic data from available documents and then\nuses a model fusion approach to fine-tune embeddings for improved retrieval\nperformance in new domains, while preserving out-of-domain capability. We\nconducted experiments on the two public datasets: SQUAD and RAG-12000 and a\nproprietary TOURISM dataset. Results demonstrate that even the standard\nfine-tuning with the proposed data augmentation technique outperforms the\nvanilla pretrained model. Furthermore, when combined with model fusion, the\nproposed approach achieves superior performance, with a 5.76% improvement in\nrecall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and\nRAG-12000 respectively.\n","authors":["Ambuje Gupta","Mrinal Rawat","Andreas Stolcke","Roberto Pieraccini"],"pdf_url":"https://arxiv.org/pdf/2410.12890v1.pdf","comment":"Accepted in AJCAI'24"},{"id":"http://arxiv.org/abs/2410.12366v1","updated":"2024-10-16T08:37:39Z","published":"2024-10-16T08:37:39Z","title":"Multi-Cause Deconfounding for Recommender Systems with Latent\n  Confounders","summary":"  In recommender systems, various latent confounding factors (e.g., user social\nenvironment and item public attractiveness) can affect user behavior, item\nexposure, and feedback in distinct ways. These factors may directly or\nindirectly impact user feedback and are often shared across items or users,\nmaking them multi-cause latent confounders. However, existing methods typically\nfail to account for latent confounders between users and their feedback, as\nwell as those between items and user feedback simultaneously. To address the\nproblem of multi-cause latent confounders, we propose a multi-cause\ndeconfounding method for recommender systems with latent confounders (MCDCF).\nMCDCF leverages multi-cause causal effect estimation to learn substitutes for\nlatent confounders associated with both users and items, using user behaviour\ndata. Specifically, MCDCF treats the multiple items that users interact with\nand the multiple users that interact with items as treatment variables,\nenabling it to learn substitutes for the latent confounders that influence the\nestimation of causality between users and their feedback, as well as between\nitems and user feedback. Additionally, we theoretically demonstrate the\nsoundness of our MCDCF method. Extensive experiments on three real-world\ndatasets demonstrate that our MCDCF method effectively recovers latent\nconfounders related to users and items, reducing bias and thereby improving\nrecommendation accuracy.\n","authors":["Zhirong Huang","Shichao Zhang","Debo Cheng","Jiuyong Li","Lin Liu","Guixian Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.12366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00333v2","updated":"2024-10-16T05:21:47Z","published":"2024-06-01T07:18:56Z","title":"A Practice-Friendly LLM-Enhanced Paradigm with Preference Parsing for\n  Sequential Recommendation","summary":"  The training paradigm integrating large language models (LLM) is gradually\nreshaping sequential recommender systems (SRS) and has shown promising results.\nHowever, most existing LLM-enhanced methods rely on rich textual information on\nthe item side and instance-level supervised fine-tuning (SFT) to inject\ncollaborative information into LLM, which is inefficient and limited in many\napplications. To alleviate these problems, this paper proposes a\npractice-friendly LLM-enhanced paradigm with preference parsing (P2Rec) for\nSRS. Specifically, in the information reconstruction stage, we design a new\nuser-level SFT task for collaborative information injection with the assistance\nof a pre-trained SRS model, which is more efficient and compatible with limited\ntext information. Our goal is to let LLM learn to reconstruct a corresponding\nprior preference distribution from each user's interaction sequence, where LLM\nneeds to effectively parse the latent category of each item and the\nrelationship between different items to accomplish this task. In the\ninformation augmentation stage, we feed each item into LLM to obtain a set of\nenhanced embeddings that combine collaborative information and LLM inference\ncapabilities. These embeddings can then be used to help train various future\nSRS models. Finally, we verify the effectiveness and efficiency of our TSLRec\non three SRS benchmark datasets.\n","authors":["Dugang Liu","Shenxian Xian","Xiaolin Lin","Xiaolian Zhang","Hong Zhu","Yuan Fang","Zhen Chen","Zhong Ming"],"pdf_url":"https://arxiv.org/pdf/2406.00333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12229v1","updated":"2024-10-16T04:44:34Z","published":"2024-10-16T04:44:34Z","title":"Comprehending Knowledge Graphs with Large Language Models for\n  Recommender Systems","summary":"  Recently, the introduction of knowledge graphs (KGs) has significantly\nadvanced recommender systems by facilitating the discovery of potential\nassociations between items. However, existing methods still face several\nlimitations. First, most KGs suffer from missing facts or limited scopes. This\ncan lead to biased knowledge representations, thereby constraining the model's\nperformance. Second, existing methods typically convert textual information\ninto IDs, resulting in the loss of natural semantic connections between\ndifferent items. Third, existing methods struggle to capture high-order\nrelationships in global KGs due to their inefficient layer-by-layer information\npropagation mechanisms, which are prone to introducing significant noise. To\naddress these limitations, we propose a novel method called CoLaKG, which\nleverages large language models (LLMs) for knowledge-aware recommendation. The\nextensive world knowledge and remarkable reasoning capabilities of LLMs enable\nthem to supplement KGs. Additionally, the strong text comprehension abilities\nof LLMs allow for a better understanding of semantic information. Based on\nthis, we first extract subgraphs centered on each item from the KG and convert\nthem into textual inputs for the LLM. The LLM then outputs its comprehension of\nthese item-centered subgraphs, which are subsequently transformed into semantic\nembeddings. Furthermore, to utilize the global information of the KG, we\nconstruct an item-item graph using these semantic embeddings, which can\ndirectly capture higher-order associations between items. Both the semantic\nembeddings and the structural information from the item-item graph are\neffectively integrated into the recommendation model through our designed\nrepresentation alignment and neighbor augmentation modules. Extensive\nexperiments on four real-world datasets demonstrate the superiority of our\nmethod.\n","authors":["Ziqiang Cui","Yunpeng Weng","Xing Tang","Fuyuan Lyu","Dugang Liu","Xiuqiang He","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2410.12229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12228v1","updated":"2024-10-16T04:44:15Z","published":"2024-10-16T04:44:15Z","title":"Triple Modality Fusion: Aligning Visual, Textual, and Graph Data with\n  Large Language Models for Multi-Behavior Recommendations","summary":"  Integrating diverse data modalities is crucial for enhancing the performance\nof personalized recommendation systems. Traditional models, which often rely on\nsingular data sources, lack the depth needed to accurately capture the\nmultifaceted nature of item features and user behaviors. This paper introduces\na novel framework for multi-behavior recommendations, leveraging the fusion of\ntriple-modality, which is visual, textual, and graph data through alignment\nwith large language models (LLMs). By incorporating visual information, we\ncapture contextual and aesthetic item characteristics; textual data provides\ninsights into user interests and item features in detail; and graph data\nelucidates relationships within the item-behavior heterogeneous graphs. Our\nproposed model called Triple Modality Fusion (TMF) utilizes the power of LLMs\nto align and integrate these three modalities, achieving a comprehensive\nrepresentation of user behaviors. The LLM models the user's interactions\nincluding behaviors and item features in natural languages. Initially, the LLM\nis warmed up using only natural language-based prompts. We then devise the\nmodality fusion module based on cross-attention and self-attention mechanisms\nto integrate different modalities from other models into the same embedding\nspace and incorporate them into an LLM. Extensive experiments demonstrate the\neffectiveness of our approach in improving recommendation accuracy. Further\nablation studies validate the effectiveness of our model design and benefits of\nthe TMF.\n","authors":["Luyi Ma","Xiaohan Li","Zezhong Fan","Jianpeng Xu","Jason Cho","Praveen Kanumala","Kaushiki Nag","Sushant Kumar","Kannan Achan"],"pdf_url":"https://arxiv.org/pdf/2410.12228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17969v2","updated":"2024-10-16T03:35:22Z","published":"2024-05-28T08:56:33Z","title":"Knowledge Circuits in Pretrained Transformers","summary":"  The remarkable capabilities of modern large language models are rooted in\ntheir vast repositories of knowledge encoded within their parameters, enabling\nthem to perceive the world and engage in reasoning. The inner workings of how\nthese models store knowledge have long been a subject of intense interest and\ninvestigation among researchers. To date, most studies have concentrated on\nisolated components within these models, such as the Multilayer Perceptrons and\nattention head. In this paper, we delve into the computation graph of the\nlanguage model to uncover the knowledge circuits that are instrumental in\narticulating specific knowledge. The experiments, conducted with GPT2 and\nTinyLLAMA, have allowed us to observe how certain information heads, relation\nheads, and Multilayer Perceptrons collaboratively encode knowledge within the\nmodel. Moreover, we evaluate the impact of current knowledge editing techniques\non these knowledge circuits, providing deeper insights into the functioning and\nconstraints of these editing methodologies. Finally, we utilize knowledge\ncircuits to analyze and interpret language model behaviors such as\nhallucinations and in-context learning. We believe the knowledge circuits hold\npotential for advancing our understanding of Transformers and guiding the\nimproved design of knowledge editing. Code and data are available in\nhttps://github.com/zjunlp/KnowledgeCircuits.\n","authors":["Yunzhi Yao","Ningyu Zhang","Zekun Xi","Mengru Wang","Ziwen Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2405.17969v2.pdf","comment":"NeurIPS 2024, 32 pages"},{"id":"http://arxiv.org/abs/2410.09119v2","updated":"2024-10-16T03:15:12Z","published":"2024-10-10T21:13:56Z","title":"$\\textit{lucie}$: An Improved Python Package for Loading Datasets from\n  the UCI Machine Learning Repository","summary":"  The University of California--Irvine (UCI) Machine Learning (ML) Repository\n(UCIMLR) is consistently cited as one of the most popular dataset repositories,\nhosting hundreds of high-impact datasets. However, a significant portion,\nincluding 28.4% of the top 250, cannot be imported via the $\\textit{ucimlrepo}$\npackage that is provided and recommended by the UCIMLR website. Instead, they\nare hosted as .zip files, containing nonstandard formats that are difficult to\nimport without additional ad hoc processing. To address this issue, here we\npresent $\\textit{lucie}$ -- $\\underline{l}oad$ $\\underline{U}niversity$\n$\\underline{C}alifornia$ $\\underline{I}rvine$ $\\underline{e}xamples$ -- a\nutility that automatically determines the data format and imports many of these\npreviously non-importable datasets, while preserving as much of a tabular data\nstructure as possible. $\\textit{lucie}$ was designed using the top 100 most\npopular datasets and benchmarked on the next 130, where it resulted in a\nsuccess rate of 95.4% vs. 73.1% for $\\textit{ucimlrepo}$. $\\textit{lucie}$ is\navailable as a Python package on PyPI with 98% code coverage.\n","authors":["Kenneth Ge","Phuc Nguyen","Ramy Arnaout"],"pdf_url":"https://arxiv.org/pdf/2410.09119v2.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.07427v2","updated":"2024-10-16T02:37:50Z","published":"2024-08-14T10:03:40Z","title":"Beyond Inter-Item Relations: Dynamic Adaption for Enhancing LLM-Based\n  Sequential Recommendation","summary":"  Sequential recommender systems (SRS) predict the next items that users may\nprefer based on user historical interaction sequences. Inspired by the rise of\nlarge language models (LLMs) in various AI applications, there is a surge of\nwork on LLM-based SRS. Despite their attractive performance, existing LLM-based\nSRS still exhibit some limitations, including neglecting intra-item relations,\nignoring long-term collaborative knowledge and using inflexible architecture\ndesigns for adaption. To alleviate these issues, we propose an LLM-based\nsequential recommendation model named DARec. Built on top of coarse-grained\nadaption for capturing inter-item relations, DARec is further enhanced with (1)\ncontext masking that models intra-item relations to help LLM better understand\ntoken and item semantics in the context of SRS, (2) collaborative knowledge\ninjection that helps LLM incorporate long-term collaborative knowledge, and (3)\na dynamic adaption mechanism that uses Bayesian optimization to flexibly choose\nlayer-wise adapter architectures in order to better incorporate different\nsequential information. Extensive experiments demonstrate that DARec can\neffectively handle sequential recommendation in a dynamic and adaptive manner.\n","authors":["CanYi Liu","Wei Li"," Youchen"," Zhang","Hui Li","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2408.07427v2.pdf","comment":"11 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.12886v1","updated":"2024-10-16T01:57:56Z","published":"2024-10-16T01:57:56Z","title":"AT-RAG: An Adaptive RAG Model Enhancing Query Efficiency with Topic\n  Filtering and Iterative Reasoning","summary":"  Recent advancements in QA with LLM, like GPT-4, have shown limitations in\nhandling complex multi-hop queries. We propose AT-RAG, a novel multistep RAG\nincorporating topic modeling for efficient document retrieval and reasoning.\nUsing BERTopic, our model dynamically assigns topics to queries, improving\nretrieval accuracy and efficiency. We evaluated AT-RAG on multihop benchmark\ndatasets QA and a medical case study QA. Results show significant improvements\nin correctness, completeness, and relevance compared to existing methods.\nAT-RAG reduces retrieval time while maintaining high precision, making it\nsuitable for general tasks QA and complex domain-specific challenges such as\nmedical QA. The integration of topic filtering and iterative reasoning enables\nour model to handle intricate queries efficiently, which makes it suitable for\napplications that require nuanced information retrieval and decision-making.\n","authors":["Mohammad Reza Rezaei","Maziar Hafezi","Amit Satpathy","Lovell Hodge","Ebrahim Pourjafari"],"pdf_url":"https://arxiv.org/pdf/2410.12886v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.13088v1","updated":"2024-10-16T23:05:59Z","published":"2024-10-16T23:05:59Z","title":"Self-Comparison for Dataset-Level Membership Inference in Large\n  (Vision-)Language Models","summary":"  Large Language Models (LLMs) and Vision-Language Models (VLMs) have made\nsignificant advancements in a wide range of natural language processing and\nvision-language tasks. Access to large web-scale datasets has been a key factor\nin their success. However, concerns have been raised about the unauthorized use\nof copyrighted materials and potential copyright infringement. Existing\nmethods, such as sample-level Membership Inference Attacks (MIA) and\ndistribution-based dataset inference, distinguish member data (data used for\ntraining) and non-member data by leveraging the common observation that models\ntend to memorize and show greater confidence in member data. Nevertheless,\nthese methods face challenges when applied to LLMs and VLMs, such as the\nrequirement for ground-truth member data or non-member data that shares the\nsame distribution as the test data. In this paper, we propose a novel\ndataset-level membership inference method based on Self-Comparison. We find\nthat a member prefix followed by a non-member suffix (paraphrased from a member\nsuffix) can further trigger the model's memorization on training data. Instead\nof directly comparing member and non-member data, we introduce paraphrasing to\nthe second half of the sequence and evaluate how the likelihood changes before\nand after paraphrasing. Unlike prior approaches, our method does not require\naccess to ground-truth member data or non-member data in identical\ndistribution, making it more practical. Extensive experiments demonstrate that\nour proposed method outperforms traditional MIA and dataset inference\ntechniques across various datasets and models, including including public\nmodels, fine-tuned models, and API-based commercial models.\n","authors":["Jie Ren","Kangrui Chen","Chen Chen","Vikash Sehwag","Yue Xing","Jiliang Tang","Lingjuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2410.13088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12957v1","updated":"2024-10-16T18:44:56Z","published":"2024-10-16T18:44:56Z","title":"MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic\n  Synchronization","summary":"  Generating music that aligns with the visual content of a video has been a\nchallenging task, as it requires a deep understanding of visual semantics and\ninvolves generating music whose melody, rhythm, and dynamics harmonize with the\nvisual narratives. This paper presents MuVi, a novel framework that effectively\naddresses these challenges to enhance the cohesion and immersive experience of\naudio-visual content. MuVi analyzes video content through a specially designed\nvisual adaptor to extract contextually and temporally relevant features. These\nfeatures are used to generate music that not only matches the video's mood and\ntheme but also its rhythm and pacing. We also introduce a contrastive\nmusic-visual pre-training scheme to ensure synchronization, based on the\nperiodicity nature of music phrases. In addition, we demonstrate that our\nflow-matching-based music generator has in-context learning ability, allowing\nus to control the style and genre of the generated music. Experimental results\nshow that MuVi demonstrates superior performance in both audio quality and\ntemporal synchronization. The generated music video samples are available at\nhttps://muvi-v2m.github.io.\n","authors":["Ruiqi Li","Siqi Zheng","Xize Cheng","Ziang Zhang","Shengpeng Ji","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.12957v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2410.12700v1","updated":"2024-10-16T16:03:42Z","published":"2024-10-16T16:03:42Z","title":"Embedding an Ethical Mind: Aligning Text-to-Image Synthesis via\n  Lightweight Value Optimization","summary":"  Recent advancements in diffusion models trained on large-scale data have\nenabled the generation of indistinguishable human-level images, yet they often\nproduce harmful content misaligned with human values, e.g., social bias, and\noffensive content. Despite extensive research on Large Language Models (LLMs),\nthe challenge of Text-to-Image (T2I) model alignment remains largely\nunexplored. Addressing this problem, we propose LiVO (Lightweight Value\nOptimization), a novel lightweight method for aligning T2I models with human\nvalues. LiVO only optimizes a plug-and-play value encoder to integrate a\nspecified value principle with the input prompt, allowing the control of\ngenerated images over both semantics and values. Specifically, we design a\ndiffusion model-tailored preference optimization loss, which theoretically\napproximates the Bradley-Terry model used in LLM alignment but provides a more\nflexible trade-off between image quality and value conformity. To optimize the\nvalue encoder, we also develop a framework to automatically construct a\ntext-image preference dataset of 86k (prompt, aligned image, violating image,\nvalue principle) samples. Without updating most model parameters and through\nadaptive value selection from the input prompt, LiVO significantly reduces\nharmful outputs and achieves faster convergence, surpassing several strong\nbaselines and taking an initial step towards ethically aligned T2I models.\n","authors":["Xingqi Wang","Xiaoyuan Yi","Xing Xie","Jia Jia"],"pdf_url":"https://arxiv.org/pdf/2410.12700v1.pdf","comment":"Accepted by ACM Multimedia 2024. The dataset and code can be found at\n  https://github.com/achernarwang/LiVO"},{"id":"http://arxiv.org/abs/2410.12220v1","updated":"2024-10-16T04:31:25Z","published":"2024-10-16T04:31:25Z","title":"Rethinking Bjøntegaard Delta for Compression Efficiency Evaluation:\n  Are We Calculating It Precisely and Reliably?","summary":"  For decades, the Bj{\\o}ntegaard Delta (BD) has been the metric for evaluating\ncodec Rate-Distortion (R-D) performance. Yet, in most studies, BD is determined\nusing just 4-5 R-D data points, could this be sufficient? As codecs and quality\nmetrics advance, does the conventional BD estimation still hold up? Crucially,\nare the performance improvements of new codecs and tools genuine, or merely\nartifacts of estimation flaws? This paper addresses these concerns by\nreevaluating BD estimation. We present a novel approach employing a\nparameterized deep neural network to model R-D curves with high precision\nacross various metrics, accompanied by a comprehensive R-D dataset. This\napproach both assesses the reliability of BD calculations and serves as a\nprecise BD estimator. Our findings advocate for the adoption of rigorous R-D\nsampling and reliability metrics in future compression research to ensure the\nvalidity and reliability of results.\n","authors":["Xinyu Hang","Shenpeng Song","Zhimeng Huang","Chuanmin Jia","Siwei Ma","Wen Gao"],"pdf_url":"https://arxiv.org/pdf/2410.12220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12219v1","updated":"2024-10-16T04:29:46Z","published":"2024-10-16T04:29:46Z","title":"OmnixR: Evaluating Omni-modality Language Models on Reasoning across\n  Modalities","summary":"  We introduce OmnixR, an evaluation suite designed to benchmark SoTA\nOmni-modality Language Models, such as GPT-4o and Gemini. Evaluating OLMs,\nwhich integrate multiple modalities such as text, vision, and audio, presents\nunique challenges. Particularly, the user message might often consist of\nmultiple modalities, such that OLMs have to establish holistic understanding\nand reasoning across modalities to accomplish the task. Existing benchmarks are\nlimited to single modality or dual-modality tasks, overlooking comprehensive\nmulti-modal assessments of model reasoning. To address this, OmnixR offers two\nevaluation variants: (1)synthetic subset: a synthetic dataset generated\nautomatically by translating text into multiple modalities--audio, images,\nvideo, and hybrids (Omnify). (2)realistic subset: a real-world dataset,\nmanually curated and annotated by experts, for evaluating cross-modal reasoning\nin natural settings. OmnixR presents a unique evaluation towards assessing OLMs\nover a diverse mix of modalities, such as a question that involves video,\naudio, and text, providing a rigorous cross-modal reasoning testbed unlike any\nexisting benchmarks. Our experiments find that all state-of-the-art OLMs\nstruggle with OmnixR questions that require integrating information from\nmultiple modalities to answer. Further analysis highlights differences in\nreasoning behavior, underscoring the challenges of omni-modal AI alignment.\n","authors":["Lichang Chen","Hexiang Hu","Mingda Zhang","Yiwen Chen","Zifeng Wang","Yandong Li","Pranav Shyam","Tianyi Zhou","Heng Huang","Ming-Hsuan Yang","Boqing Gong"],"pdf_url":"https://arxiv.org/pdf/2410.12219v1.pdf","comment":"19 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2407.07464v2","updated":"2024-10-16T03:44:41Z","published":"2024-07-10T08:40:39Z","title":"Video-to-Audio Generation with Hidden Alignment","summary":"  Generating semantically and temporally aligned audio content in accordance\nwith video input has become a focal point for researchers, particularly\nfollowing the remarkable breakthrough in text-to-video generation. In this\nwork, we aim to offer insights into the video-to-audio generation paradigm,\nfocusing on three crucial aspects: vision encoders, auxiliary embeddings, and\ndata augmentation techniques. Beginning with a foundational model built on a\nsimple yet surprisingly effective intuition, we explore various vision encoders\nand auxiliary embeddings through ablation studies. Employing a comprehensive\nevaluation pipeline that emphasizes generation quality and video-audio\nsynchronization alignment, we demonstrate that our model exhibits\nstate-of-the-art video-to-audio generation capabilities. Furthermore, we\nprovide critical insights into the impact of different data augmentation\nmethods on enhancing the generation framework's overall capacity. We showcase\npossibilities to advance the challenge of generating synchronized audio from\nsemantic and temporal perspectives. We hope these insights will serve as a\nstepping stone toward developing more realistic and accurate audio-visual\ngeneration models.\n","authors":["Manjie Xu","Chenxing Li","Xinyi Tu","Yong Ren","Rilin Chen","Yu Gu","Wei Liang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2407.07464v2.pdf","comment":"https://sites.google.com/view/vta-ldm"},{"id":"http://arxiv.org/abs/2410.12191v1","updated":"2024-10-16T03:25:16Z","published":"2024-10-16T03:25:16Z","title":"Test-time adaptation for image compression with distribution\n  regularization","summary":"  Current test- or compression-time adaptation image compression (TTA-IC)\napproaches, which leverage both latent and decoder refinements as a two-step\nadaptation scheme, have potentially enhanced the rate-distortion (R-D)\nperformance of learned image compression models on cross-domain compression\ntasks, \\textit{e.g.,} from natural to screen content images. However, compared\nwith the emergence of various decoder refinement variants, the latent\nrefinement, as an inseparable ingredient, is barely tailored to cross-domain\nscenarios. To this end, we aim to develop an advanced latent refinement method\nby extending the effective hybrid latent refinement (HLR) method, which is\ndesigned for \\textit{in-domain} inference improvement but shows noticeable\ndegradation of the rate cost in \\textit{cross-domain} tasks. Specifically, we\nfirst provide theoretical analyses, in a cue of marginalization approximation\nfrom in- to cross-domain scenarios, to uncover that the vanilla HLR suffers\nfrom an underlying mismatch between refined Gaussian conditional and hyperprior\ndistributions, leading to deteriorated joint probability approximation of\nmarginal distribution with increased rate consumption. To remedy this issue, we\nintroduce a simple Bayesian approximation-endowed \\textit{distribution\nregularization} to encourage learning a better joint probability approximation\nin a plug-and-play manner. Extensive experiments on six in- and cross-domain\ndatasets demonstrate that our proposed method not only improves the R-D\nperformance compared with other latent refinement counterparts, but also can be\nflexibly integrated into existing TTA-IC methods with incremental benefits.\n","authors":["Kecheng Chen","Pingping Zhang","Tiexin Qin","Shiqi Wang","Hong Yan","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2410.12191v1.pdf","comment":null}]},"2024-10-15T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.07722v2","updated":"2024-10-15T19:58:37Z","published":"2024-10-10T08:41:34Z","title":"DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities","summary":"  Learned Sparse Retrieval (LSR) models use vocabularies from pre-trained\ntransformers, which often split entities into nonsensical fragments. Splitting\nentities can reduce retrieval accuracy and limits the model's ability to\nincorporate up-to-date world knowledge not included in the training data. In\nthis work, we enhance the LSR vocabulary with Wikipedia concepts and entities,\nenabling the model to resolve ambiguities more effectively and stay current\nwith evolving knowledge. Central to our approach is a Dynamic Vocabulary (DyVo)\nhead, which leverages existing entity embeddings and an entity retrieval\ncomponent that identifies entities relevant to a query or document. We use the\nDyVo head to generate entity weights, which are then merged with word piece\nweights to create joint representations for efficient indexing and retrieval\nusing an inverted index. In experiments across three entity-rich document\nranking datasets, the resulting DyVo model substantially outperforms\nstate-of-the-art baselines.\n","authors":["Thong Nguyen","Shubham Chatterjee","Sean MacAvaney","Iain Mackie","Jeff Dalton","Andrew Yates"],"pdf_url":"https://arxiv.org/pdf/2410.07722v2.pdf","comment":"https://github.com/thongnt99/DyVo"},{"id":"http://arxiv.org/abs/2410.11841v1","updated":"2024-10-15T17:59:30Z","published":"2024-10-15T17:59:30Z","title":"GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable\n  Recommendation","summary":"  Large language model-based explainable recommendation (LLM-based ER) systems\nshow promise in generating human-like explanations for recommendations.\nHowever, they face challenges in modeling user-item collaborative preferences,\npersonalizing explanations, and handling sparse user-item interactions. To\naddress these issues, we propose GaVaMoE, a novel Gaussian-Variational Gated\nMixture of Experts framework for explainable recommendation. GaVaMoE introduces\ntwo key components: (1) a rating reconstruction module that employs Variational\nAutoencoder (VAE) with a Gaussian Mixture Model (GMM) to capture complex\nuser-item collaborative preferences, serving as a pre-trained multi-gating\nmechanism; and (2) a set of fine-grained expert models coupled with the\nmulti-gating mechanism for generating highly personalized explanations. The VAE\ncomponent models latent factors in user-item interactions, while the GMM\nclusters users with similar behaviors. Each cluster corresponds to a gate in\nthe multi-gating mechanism, routing user-item pairs to appropriate expert\nmodels. This architecture enables GaVaMoE to generate tailored explanations for\nspecific user types and preferences, mitigating data sparsity by leveraging\nuser similarities. Extensive experiments on three real-world datasets\ndemonstrate that GaVaMoE significantly outperforms existing methods in\nexplanation quality, personalization, and consistency. Notably, GaVaMoE\nexhibits robust performance in scenarios with sparse user-item interactions,\nmaintaining high-quality explanations even for users with limited historical\ndata.\n","authors":["Fei Tang","Yongliang Shen","Hang Zhang","Zeqi Tan","Wenqi Zhang","Guiyang Hou","Kaitao Song","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.11841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03764v2","updated":"2024-10-15T16:01:11Z","published":"2024-05-06T18:02:00Z","title":"GOVERN: Gradient Orientation Vote Ensemble for Multi-Teacher Reinforced\n  Distillation","summary":"  Pre-trained language models have become an integral component of\nquestion-answering systems, achieving remarkable performance. However, for\npractical deployment, it is crucial to perform knowledge distillation to\nmaintain high performance while operating under computational constraints. In\nthis paper, we address a key question: given the importance of unsupervised\ndistillation for student model performance, how can knowledge from multiple\nteacher models be effectively ensemble during this stage without the guidance\nof labels? We propose a novel algorithm, GOVERN, to tackle this issue. GOVERN\nhas demonstrated significant improvements in both offline and online\nexperiments, enabling the student model to achieve results comparable to that\nof teacher ensembles. Our experiments show that GOVERN remarkably requires a\nmere 1\\% of the ensemble method's inference budget to achieve 99.5\\% of\nperformance. The proposed algorithm has been successfully deployed in a\nreal-world commercial question-answering system, demonstrating its real-world\napplicability.\n","authors":["Wenjie Zhou","Zhenxin Ding","Xiaodong Zhang","Haibo Shi","Junfeng Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2405.03764v2.pdf","comment":"Accepted by EMNLP 2024 Industry Track"},{"id":"http://arxiv.org/abs/2410.11719v1","updated":"2024-10-15T15:50:53Z","published":"2024-10-15T15:50:53Z","title":"Adaptive Coordinators and Prompts on Heterogeneous Graphs for\n  Cross-Domain Recommendations","summary":"  In the online digital world, users frequently engage with diverse items\nacross multiple domains (e.g., e-commerce platforms, streaming services, and\nsocial media networks), forming complex heterogeneous interaction graphs.\nLeveraging this multi-domain information can undoubtedly enhance the\nperformance of recommendation systems by providing more comprehensive user\ninsights and alleviating data sparsity in individual domains. However,\nintegrating multi-domain knowledge for the cross-domain recommendation is very\nhard due to inherent disparities in user behavior and item characteristics and\nthe risk of negative transfer, where irrelevant or conflicting information from\nthe source domains adversely impacts the target domain's performance. To\naddress these challenges, we offer HAGO, a novel framework with\n$\\textbf{H}$eterogeneous $\\textbf{A}$daptive $\\textbf{G}$raph\nco$\\textbf{O}$rdinators, which dynamically integrate multi-domain graphs into a\ncohesive structure by adaptively adjusting the connections between coordinators\nand multi-domain graph nodes, thereby enhancing beneficial inter-domain\ninteractions while mitigating negative transfer effects. Additionally, we\ndevelop a universal multi-domain graph pre-training strategy alongside HAGO to\ncollaboratively learn high-quality node representations across domains. To\neffectively transfer the learned multi-domain knowledge to the target domain,\nwe design an effective graph prompting method, which incorporates pre-trained\nembeddings with learnable prompts for the recommendation task. Our framework is\ncompatible with various graph-based models and pre-training techniques,\ndemonstrating broad applicability and effectiveness. Further experimental\nresults show that our solutions outperform state-of-the-art methods in\nmulti-domain recommendation scenarios and highlight their potential for\nreal-world applications.\n","authors":["Hengyu Zhang","Chunxu Shen","Xiangguo Sun","Jie Tan","Yu Rong","Chengzhi Piao","Hong Cheng","Lingling Yi"],"pdf_url":"https://arxiv.org/pdf/2410.11719v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.07671v2","updated":"2024-10-15T15:29:51Z","published":"2024-10-10T07:29:31Z","title":"DISCO: A Hierarchical Disentangled Cognitive Diagnosis Framework for\n  Interpretable Job Recommendation","summary":"  The rapid development of online recruitment platforms has created\nunprecedented opportunities for job seekers while concurrently posing the\nsignificant challenge of quickly and accurately pinpointing positions that\nalign with their skills and preferences. Job recommendation systems have\nsignificantly alleviated the extensive search burden for job seekers by\noptimizing user engagement metrics, such as clicks and applications, thus\nachieving notable success. In recent years, a substantial amount of research\nhas been devoted to developing effective job recommendation models, primarily\nfocusing on text-matching based and behavior modeling based methods. While\nthese approaches have realized impressive outcomes, it is imperative to note\nthat research on the explainability of recruitment recommendations remains\nprofoundly unexplored. To this end, in this paper, we propose DISCO, a\nhierarchical Disentanglement based Cognitive diagnosis framework, aimed at\nflexibly accommodating the underlying representation learning model for\neffective and interpretable job recommendations. Specifically, we first design\na hierarchical representation disentangling module to explicitly mine the\nhierarchical skill-related factors implied in hidden representations of job\nseekers and jobs. Subsequently, we propose level-aware association modeling to\nenhance information communication and robust representation learning both\ninter- and intra-level, which consists of the interlevel knowledge influence\nmodule and the level-wise contrastive learning. Finally, we devise an\ninteraction diagnosis module incorporating a neural diagnosis function for\neffectively modeling the multi-level recruitment interaction process between\njob seekers and jobs, which introduces the cognitive measurement theory.\n","authors":["Xiaoshan Yu","Chuan Qin","Qi Zhang","Chen Zhu","Haiping Ma","Xingyi Zhang","Hengshu Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.07671v2.pdf","comment":"Accepted by ICDM 2024. 10 pages"},{"id":"http://arxiv.org/abs/2401.00797v2","updated":"2024-10-15T12:37:40Z","published":"2024-01-01T15:57:15Z","title":"Curriculum-scheduled Knowledge Distillation from Multiple Pre-trained\n  Teachers for Multi-domain Sequential Recommendation","summary":"  Pre-trained recommendation models (PRMs) have received increasing interest\nrecently. However, their intrinsically heterogeneous model structure, huge\nmodel size and computation cost hinder their adoptions in practical recommender\nsystems. Hence, it is highly essential to explore how to use different\npre-trained recommendation models efficiently in real-world systems. In this\npaper, we propose a novel curriculum-scheduled knowledge distillation from\nmultiple pre-trained teachers for multi-domain sequential recommendation,\ncalled CKD-MDSR, which takes full advantages of different PRMs as multiple\nteacher models to boost a small student recommendation model, integrating the\nknowledge across multiple domains from PRMs. Specifically, CKD-MDSR first\nadopts curriculum-scheduled user behavior sequence sampling and distills\ninformative knowledge jointly from the representative PRMs such as UniSRec and\nRecformer. Then, the knowledge from the above PRMs are selectively integrated\ninto the student model in consideration of their confidence and consistency.\nFinally, we verify the proposed method on multi-domain sequential\nrecommendation and further demonstrate its universality with multiple types of\nstudent models, including feature interaction and graph based recommendation\nmodels. Extensive experiments on five real-world datasets demonstrate the\neffectiveness and efficiency of CKD-MDSR, which can be viewed as an efficient\nshortcut using PRMs in real-world systems.\n","authors":["Wenqi Sun","Ruobing Xie","Junjie Zhang","Wayne Xin Zhao","Leyu Lin","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2401.00797v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10645v2","updated":"2024-10-15T11:01:33Z","published":"2024-08-20T08:36:59Z","title":"CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation","summary":"  Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, CoRA (an acronym for Collaborative LoRA),\nwith a collaborative weights generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings, converting them into collaborative\nweights with low-rank properties through the collaborative weights generator.\nWe then merge the collaborative weights into LLM's weights, enabling LLM to\nperceive the collaborative signals and generate personalized recommendations\nwithout fine-tuning or extra collaborative tokens in prompts. Extensive\nexperiments confirm that CoRA effectively integrates collaborative information\ninto LLM, enhancing recommendation performance.\n","authors":["Yuting Liu","Jinghao Zhang","Yizhou Dang","Yuliang Liang","Qiang Liu","Guibing Guo","Jianzhe Zhao","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.10645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11464v1","updated":"2024-10-15T10:11:18Z","published":"2024-10-15T10:11:18Z","title":"CoActionGraphRec: Sequential Multi-Interest Recommendations Using\n  Co-Action Graphs","summary":"  There are unique challenges to developing item recommender systems for\ne-commerce platforms like eBay due to sparse data and diverse user interests.\nWhile rich user-item interactions are important, eBay's data sparsity exceeds\nother e-commerce sites by an order of magnitude. To address this challenge, we\npropose CoActionGraphRec (CAGR), a text based two-tower deep learning model\n(Item Tower and User Tower) utilizing co-action graph layers. In order to\nenhance user and item representations, a graph-based solution tailored to\neBay's environment is utilized. For the Item Tower, we represent each item\nusing its co-action items to capture collaborative signals in a co-action graph\nthat is fully leveraged by the graph neural network component. For the User\nTower, we build a fully connected graph of each user's behavior sequence, with\nedges encoding pairwise relationships. Furthermore, an explicit interaction\nmodule learns representations capturing behavior interactions. Extensive\noffline and online A/B test experiments demonstrate the effectiveness of our\nproposed approach and results show improved performance over state-of-the-art\nmethods on key metrics.\n","authors":["Yi Sun","Yuri M. Brovman"],"pdf_url":"https://arxiv.org/pdf/2410.11464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11457v1","updated":"2024-10-15T10:02:55Z","published":"2024-10-15T10:02:55Z","title":"LR-SQL: A Supervised Fine-Tuning Method for Text2SQL Tasks under\n  Low-Resource Scenarios","summary":"  Large language models revolutionize Text2SQL through supervised fine-tuning,\nyet a crucial limitation is overlooked: the complexity of databases leads to an\nincreased context length, consequently resulting in higher GPU memory demands\nfor model fine-tuning. To address this issue, we propose LR-SQL. LR-SQL\ncomprises two supervised fine-tuning models: the schema\\_link model and the\nSQL\\_generation model, with the schema\\_link model serving as the focal point\nfor streamlining the overall process. During the fine-tuning of the\nschema\\_link model, LR-SQL breaks down the complete database into flexible\ncombinations of tables with adjustable quantities, enabling the model to learn\nthe relationships within the entire database from these dispersed slices.\nFurthermore, to enhance the model's ability to perceive the relationships among\nvarious discrete slices during inference, LR-SQL trains the model's\nChain-of-Thought capability for this task. Experimental results demonstrate\nthat LR-SQL can reduce the total GPU memory usage by 40\\% compared to existing\nfine-tuning methods, while only losing 2\\% of table prediction accuracy in\nschema\\_link task. For the overall Text2SQL task, the Execution Accuracy\ndecrease by 0.6\\%.Our project is now available on\nhttps://github.com/hongWin/LR-SQL\n","authors":["Wen Wuzhenghong","Zhang Yongpan","Pan Su","Sun Yuwei","Lu Pengwei","Ding Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.11457v1.pdf","comment":"12pages, 4 figures,submitting to a journal"},{"id":"http://arxiv.org/abs/2410.11370v1","updated":"2024-10-15T07:50:34Z","published":"2024-10-15T07:50:34Z","title":"Enhance Graph Alignment for Large Language Models","summary":"  Graph-structured data is prevalent in the real world. Recently, due to the\npowerful emergent capabilities, Large Language Models (LLMs) have shown\npromising performance in modeling graphs. The key to effectively applying LLMs\non graphs is converting graph data into a format LLMs can comprehend.\nGraph-to-token approaches are popular in enabling LLMs to process graph\ninformation. They transform graphs into sequences of tokens and align them with\ntext tokens through instruction tuning, where self-supervised instruction\ntuning helps LLMs acquire general knowledge about graphs, and supervised\nfine-tuning specializes LLMs for the downstream tasks on graphs. Despite their\ninitial success, we find that existing methods have a misalignment between\nself-supervised tasks and supervised downstream tasks, resulting in negative\ntransfer from self-supervised fine-tuning to downstream tasks. To address these\nissues, we propose Graph Alignment Large Language Models (GALLM) to benefit\nfrom aligned task templates. In the self-supervised tuning stage, we introduce\na novel text matching task using templates aligned with downstream tasks. In\nthe task-specific tuning stage, we propose two category prompt methods that\nlearn supervision information from additional explanation with further aligned\ntemplates. Experimental evaluations on four datasets demonstrate substantial\nimprovements in supervised learning, multi-dataset generalizability, and\nparticularly in zero-shot capability, highlighting the model's potential as a\ngraph foundation model.\n","authors":["Haitong Luo","Xuying Meng","Suhang Wang","Tianxiang Zhao","Fali Wang","Hanyun Cao","Yujun Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.11370v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2410.11355v1","updated":"2024-10-15T07:25:33Z","published":"2024-10-15T07:25:33Z","title":"Reducing Labeling Costs in Sentiment Analysis via Semi-Supervised\n  Learning","summary":"  Labeling datasets is a noteworthy challenge in machine learning, both in\nterms of cost and time. This research, however, leverages an efficient answer.\nBy exploring label propagation in semi-supervised learning, we can\nsignificantly reduce the number of labels required compared to traditional\nmethods. We employ a transductive label propagation method based on the\nmanifold assumption for text classification. Our approach utilizes a\ngraph-based method to generate pseudo-labels for unlabeled data for the text\nclassification task, which are then used to train deep neural networks. By\nextending labels based on cosine proximity within a nearest neighbor graph from\nnetwork embeddings, we combine unlabeled data into supervised learning, thereby\nreducing labeling costs. Based on previous successes in other domains, this\nstudy builds and evaluates this approach's effectiveness in sentiment analysis,\npresenting insights into semi-supervised learning.\n","authors":["Minoo Jafarlou","Mario M. Kubek"],"pdf_url":"https://arxiv.org/pdf/2410.11355v1.pdf","comment":"12 pages, 7 figures, accepted at the 2024 8th International\n  Conference on Natural Language Processing and Information Retrieval (NLPIR\n  2024), Okayama, Japan, 2024"},{"id":"http://arxiv.org/abs/2410.11327v1","updated":"2024-10-15T06:54:27Z","published":"2024-10-15T06:54:27Z","title":"Sequential LLM Framework for Fashion Recommendation","summary":"  The fashion industry is one of the leading domains in the global e-commerce\nsector, prompting major online retailers to employ recommendation systems for\nproduct suggestions and customer convenience. While recommendation systems have\nbeen widely studied, most are designed for general e-commerce problems and\nstruggle with the unique challenges of the fashion domain. To address these\nissues, we propose a sequential fashion recommendation framework that leverages\na pre-trained large language model (LLM) enhanced with recommendation-specific\nprompts. Our framework employs parameter-efficient fine-tuning with extensive\nfashion data and introduces a novel mix-up-based retrieval technique for\ntranslating text into relevant product suggestions. Extensive experiments show\nour proposed framework significantly enhances fashion recommendation\nperformance.\n","authors":["Han Liu","Xianfeng Tang","Tianlang Chen","Jiapeng Liu","Indu Indu","Henry Peng Zou","Peng Dai","Roberto Fernandez Galan","Michael D Porter","Dongmei Jia","Ning Zhang","Lian Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.11327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00080v3","updated":"2024-10-15T05:34:07Z","published":"2024-04-30T16:35:08Z","title":"Recommenadation aided Caching using Combinatorial Multi-armed Bandits","summary":"  We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.\n","authors":["Pavamana K J","Chandramani Kishore Singh"],"pdf_url":"https://arxiv.org/pdf/2405.00080v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02130v4","updated":"2024-10-15T04:06:44Z","published":"2024-01-04T08:31:47Z","title":"Spectral-Based Graph Neural Networks for Complementary Item\n  Recommendation","summary":"  Modeling complementary relationships greatly helps recommender systems to\naccurately and promptly recommend the subsequent items when one item is\npurchased. Unlike traditional similar relationships, items with complementary\nrelationships may be purchased successively (such as iPhone and Airpods Pro),\nand they not only share relevance but also exhibit dissimilarity. Since the two\nattributes are opposites, modeling complementary relationships is challenging.\nPrevious attempts to exploit these relationships have either ignored or\noversimplified the dissimilarity attribute, resulting in ineffective modeling\nand an inability to balance the two attributes. Since Graph Neural Networks\n(GNNs) can capture the relevance and dissimilarity between nodes in the\nspectral domain, we can leverage spectral-based GNNs to effectively understand\nand model complementary relationships. In this study, we present a novel\napproach called Spectral-based Complementary Graph Neural Networks (SComGNN)\nthat utilizes the spectral properties of complementary item graphs. We make the\nfirst observation that complementary relationships consist of low-frequency and\nmid-frequency components, corresponding to the relevance and dissimilarity\nattributes, respectively. Based on this spectral observation, we design\nspectral graph convolutional networks with low-pass and mid-pass filters to\ncapture the low-frequency and mid-frequency components. Additionally, we\npropose a two-stage attention mechanism to adaptively integrate and balance the\ntwo attributes. Experimental results on four e-commerce datasets demonstrate\nthe effectiveness of our model, with SComGNN significantly outperforming\nexisting baseline models.\n","authors":["Haitong Luo","Xuying Meng","Suhang Wang","Hanyun Cao","Weiyao Zhang","Yequan Wang","Yujun Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02130v4.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2410.11217v1","updated":"2024-10-15T03:04:26Z","published":"2024-10-15T03:04:26Z","title":"On the Capacity of Citation Generation by Large Language Models","summary":"  Retrieval-augmented generation (RAG) appears as a promising method to\nalleviate the \"hallucination\" problem in large language models (LLMs), since it\ncan incorporate external traceable resources for response generation. The\nessence of RAG in combating the hallucination issue lies in accurately\nattributing claims in responses to the corresponding retrieved documents.\nHowever, most of existing works focus on improving the quality of generated\nresponses from the LLM, while largely overlooked its ability to attribute\nsources accurately. In this study, we conduct a systematic analysis about the\ncapabilities of LLMs in generating citations within response generation, and\nfurther introduce a novel method to enhance their citation generation\nabilities. Specifically, we evaluate both the correctness and citation quality\nfor seven widely-used LLMs on two benchmark datasets. Meanwhile, we introduce\nnew citation evaluation metrics to eliminate the over-penalization of\nunnecessary and excessive citations in existing metrics. Furthermore, we\npropose a Generate-then-Refine method that completes relevant citations and\nremoves irrelevant ones without altering the response text. The results on\nWebGLM-QA, ASQA and ELI5 datasets show that our method substantially improves\nthe quality of citations in responses generated by LLMs.\n","authors":["Haosheng Qian","Yixing Fan","Ruqing Zhang","Jiafeng Guo"],"pdf_url":"https://arxiv.org/pdf/2410.11217v1.pdf","comment":"Accepted by CCIR 2024"},{"id":"http://arxiv.org/abs/2410.11150v1","updated":"2024-10-15T00:23:18Z","published":"2024-10-15T00:23:18Z","title":"Optimizing Encoder-Only Transformers for Session-Based Recommendation\n  Systems","summary":"  Session-based recommendation is the task of predicting the next item a user\nwill interact with, often without access to historical user data. In this work,\nwe introduce Sequential Masked Modeling, a novel approach for encoder-only\ntransformer architectures to tackle the challenges of single-session\nrecommendation. Our method combines data augmentation through window sliding\nwith a unique penultimate token masking strategy to capture sequential\ndependencies more effectively. By enhancing how transformers handle session\ndata, Sequential Masked Modeling significantly improves next-item prediction\nperformance.\n  We evaluate our approach on three widely-used datasets, Yoochoose 1/64,\nDiginetica, and Tmall, comparing it to state-of-the-art single-session,\ncross-session, and multi-relation approaches. The results demonstrate that our\nTransformer-SMM models consistently outperform all models that rely on the same\namount of information, while even rivaling methods that have access to more\nextensive user history. This study highlights the potential of encoder-only\ntransformers in session-based recommendation and opens the door for further\nimprovements.\n","authors":["Anis Redjdal","Luis Pinto","Michel Desmarais"],"pdf_url":"https://arxiv.org/pdf/2410.11150v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.12051v1","updated":"2024-10-15T20:41:10Z","published":"2024-10-15T20:41:10Z","title":"Enabling Data-Driven and Empathetic Interactions: A Context-Aware 3D\n  Virtual Agent in Mixed Reality for Enhanced Financial Customer Experience","summary":"  In this paper, we introduce a novel system designed to enhance customer\nservice in the financial and retail sectors through a context-aware 3D virtual\nagent, utilizing Mixed Reality (MR) and Vision Language Models (VLMs). Our\napproach focuses on enabling data-driven and empathetic interactions that\nensure customer satisfaction by introducing situational awareness of the\nphysical location, personalized interactions based on customer profiles, and\nrigorous privacy and security standards. We discuss our design considerations\ncritical for deployment in real-world customer service environments, addressing\nchallenges in user data management and sensitive information handling. We also\noutline the system architecture and key features unique to banking and retail\nenvironments. Our work demonstrates the potential of integrating MR and VLMs in\nservice industries, offering practical insights in customer service delivery\nwhile maintaining high standards of security and personalization.\n","authors":["Cindy Xu","Mengyu Chen","Pranav Deshpande","Elvir Azanli","Runqing Yang","Joseph Ligman"],"pdf_url":"https://arxiv.org/pdf/2410.12051v1.pdf","comment":"to appear at 1st Workshop on Intelligent XR: Harnessing AI for\n  Next-Generation XR User Experiences at International Symposium on Mixed and\n  Augmented Reality (ISMAR) 2024"},{"id":"http://arxiv.org/abs/2410.12018v1","updated":"2024-10-15T19:33:57Z","published":"2024-10-15T19:33:57Z","title":"LocoMotion: Learning Motion-Focused Video-Language Representations","summary":"  This paper strives for motion-focused video-language representations.\nExisting methods to learn video-language representations use spatial-focused\ndata, where identifying the objects and scene is often enough to distinguish\nthe relevant caption. We instead propose LocoMotion to learn from\nmotion-focused captions that describe the movement and temporal progression of\nlocal object motions. We achieve this by adding synthetic motions to videos and\nusing the parameters of these motions to generate corresponding captions.\nFurthermore, we propose verb-variation paraphrasing to increase the caption\nvariety and learn the link between primitive motions and high-level verbs. With\nthis, we are able to learn a motion-focused video-language representation.\nExperiments demonstrate our approach is effective for a variety of downstream\ntasks, particularly when limited data is available for fine-tuning. Code is\navailable: https://hazeldoughty.github.io/Papers/LocoMotion/\n","authors":["Hazel Doughty","Fida Mohammad Thoker","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2410.12018v1.pdf","comment":"ACCV 2024"},{"id":"http://arxiv.org/abs/2410.11817v1","updated":"2024-10-15T17:46:31Z","published":"2024-10-15T17:46:31Z","title":"Improving Long-Text Alignment for Text-to-Image Diffusion Models","summary":"  The rapid advancement of text-to-image (T2I) diffusion models has enabled\nthem to generate unprecedented results from given texts. However, as text\ninputs become longer, existing encoding methods like CLIP face limitations, and\naligning the generated images with long texts becomes challenging. To tackle\nthese issues, we propose LongAlign, which includes a segment-level encoding\nmethod for processing long texts and a decomposed preference optimization\nmethod for effective alignment training. For segment-level encoding, long texts\nare divided into multiple segments and processed separately. This method\novercomes the maximum input length limits of pretrained encoding models. For\npreference optimization, we provide decomposed CLIP-based preference models to\nfine-tune diffusion models. Specifically, to utilize CLIP-based preference\nmodels for T2I alignment, we delve into their scoring mechanisms and find that\nthe preference scores can be decomposed into two components: a text-relevant\npart that measures T2I alignment and a text-irrelevant part that assesses other\nvisual aspects of human preference. Additionally, we find that the\ntext-irrelevant part contributes to a common overfitting problem during\nfine-tuning. To address this, we propose a reweighting strategy that assigns\ndifferent weights to these two components, thereby reducing overfitting and\nenhancing alignment. After fine-tuning $512 \\times 512$ Stable Diffusion (SD)\nv1.5 for about 20 hours using our method, the fine-tuned SD outperforms\nstronger foundation models in T2I alignment, such as PixArt-$\\alpha$ and\nKandinsky v2.2. The code is available at\nhttps://github.com/luping-liu/LongAlign.\n","authors":["Luping Liu","Chao Du","Tianyu Pang","Zehan Wang","Chongxuan Li","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2410.11817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12831v2","updated":"2024-10-15T17:31:56Z","published":"2024-06-18T17:51:37Z","title":"VIA: Unified Spatiotemporal Video Adaptation Framework for Global and\n  Local Video Editing","summary":"  Video editing is a cornerstone of digital media, from entertainment and\neducation to professional communication. However, previous methods often\noverlook the necessity of comprehensively understanding both global and local\ncontexts, leading to inaccurate and inconsistent edits in the spatiotemporal\ndimension, especially for long videos. In this paper, we introduce VIA, a\nunified spatiotemporal Video Adaptation framework for global and local video\nediting, pushing the limits of consistently editing minute-long videos. First,\nto ensure local consistency within individual frames, we designed test-time\nediting adaptation to adapt a pre-trained image editing model for improving\nconsistency between potential editing directions and the text instruction, and\nadapt masked latent variables for precise local control. Furthermore, to\nmaintain global consistency over the video sequence, we introduce\nspatiotemporal adaptation that recursively gather consistent attention\nvariables in key frames and strategically applies them across the whole\nsequence to realize the editing effects. Extensive experiments demonstrate\nthat, compared to baseline methods, our VIA approach produces edits that are\nmore faithful to the source videos, more coherent in the spatiotemporal\ncontext, and more precise in local control. More importantly, we show that VIA\ncan achieve consistent long video editing in minutes, unlocking the potential\nfor advanced video editing tasks over long video sequences.\n","authors":["Jing Gu","Yuwei Fang","Ivan Skorokhodov","Peter Wonka","Xinya Du","Sergey Tulyakov","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2406.12831v2.pdf","comment":"19 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.11779v1","updated":"2024-10-15T16:57:44Z","published":"2024-10-15T16:57:44Z","title":"MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation","summary":"  Multimodal Large Language Models (MLLMs) frequently exhibit hallucination\nphenomena, but the underlying reasons remain poorly understood. In this paper,\nwe present an empirical analysis and find that, although MLLMs incorrectly\ngenerate the objects in the final output, they are actually able to recognize\nvisual objects in the preceding layers. We speculate that this may be due to\nthe strong knowledge priors of the language model suppressing the visual\ninformation, leading to hallucinations. Motivated by this, we propose a novel\ndynamic correction decoding method for MLLMs (DeCo), which adaptively selects\nthe appropriate preceding layers and proportionally integrates knowledge into\nthe final layer to adjust the output logits. Note that DeCo is model agnostic\nand can be seamlessly incorporated with various classic decoding strategies and\napplied to different MLLMs. We evaluate DeCo on widely-used benchmarks,\ndemonstrating that it can reduce hallucination rates by a large margin compared\nto baselines, highlighting its potential to mitigate hallucinations. Code is\navailable at https://github.com/zjunlp/DeCo.\n","authors":["Chenxi Wang","Xiang Chen","Ningyu Zhang","Bozhong Tian","Haoming Xu","Shumin Deng","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2410.11779v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2410.11701v1","updated":"2024-10-15T15:39:37Z","published":"2024-10-15T15:39:37Z","title":"Magnifier Prompt: Tackling Multimodal Hallucination via Extremely Simple\n  Instructions","summary":"  Hallucinations in multimodal large language models (MLLMs) hinder their\npractical applications. To address this, we propose a Magnifier Prompt\n(MagPrompt), a simple yet effective method to tackle hallucinations in MLLMs\nvia extremely simple instructions. MagPrompt is based on the following two key\nprinciples, which guide the design of various effective prompts, demonstrating\nrobustness: (1) MLLMs should focus more on the image. (2) When there are\nconflicts between the image and the model's inner knowledge, MLLMs should\nprioritize the image. MagPrompt is training-free and can be applied to\nopen-source and closed-source models, such as GPT-4o and Gemini-pro. It\nperforms well across many datasets and its effectiveness is comparable or even\nbetter than more complex methods like VCD. Furthermore, our prompt design\nprinciples and experimental analyses provide valuable insights into multimodal\nhallucination.\n","authors":["Yuhan Fu","Ruobing Xie","Jiazhen Liu","Bangxiang Lan","Xingwu Sun","Zhanhui Kang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2410.11701v1.pdf","comment":"9 pages, 13 tables, 4 figures"},{"id":"http://arxiv.org/abs/2410.11582v1","updated":"2024-10-15T13:15:50Z","published":"2024-10-15T13:15:50Z","title":"On-the-fly Modulation for Balanced Multimodal Learning","summary":"  Multimodal learning is expected to boost model performance by integrating\ninformation from different modalities. However, its potential is not fully\nexploited because the widely-used joint training strategy, which has a uniform\nobjective for all modalities, leads to imbalanced and under-optimized uni-modal\nrepresentations. Specifically, we point out that there often exists modality\nwith more discriminative information, e.g., vision of playing football and\nsound of blowing wind. They could dominate the joint training process,\nresulting in other modalities being significantly under-optimized. To alleviate\nthis problem, we first analyze the under-optimized phenomenon from both the\nfeed-forward and the back-propagation stages during optimization. Then,\nOn-the-fly Prediction Modulation (OPM) and On-the-fly Gradient Modulation (OGM)\nstrategies are proposed to modulate the optimization of each modality, by\nmonitoring the discriminative discrepancy between modalities during training.\nConcretely, OPM weakens the influence of the dominant modality by dropping its\nfeature with dynamical probability in the feed-forward stage, while OGM\nmitigates its gradient in the back-propagation stage. In experiments, our\nmethods demonstrate considerable improvement across a variety of multimodal\ntasks. These simple yet effective strategies not only enhance performance in\nvanilla and task-oriented multimodal models, but also in more complex\nmultimodal tasks, showcasing their effectiveness and flexibility. The source\ncode is available at \\url{https://github.com/GeWu-Lab/BML_TPAMI2024}.\n","authors":["Yake Wei","Di Hu","Henghui Du","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2410.11582v1.pdf","comment":"Accepted by T-PAMI 2024"},{"id":"http://arxiv.org/abs/2404.01336v3","updated":"2024-10-15T12:40:39Z","published":"2024-03-30T14:39:09Z","title":"FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain\n  Fake News Detection","summary":"  Existing benchmarks for fake news detection have significantly contributed to\nthe advancement of models in assessing the authenticity of news content.\nHowever, these benchmarks typically focus solely on news pertaining to a single\nsemantic topic or originating from a single platform, thereby failing to\ncapture the diversity of multi-domain news in real scenarios. In order to\nunderstand fake news across various domains, the external knowledge and\nfine-grained annotations are indispensable to provide precise evidence and\nuncover the diverse underlying strategies for fabrication, which are also\nignored by existing benchmarks. To address this gap, we introduce a novel\nmulti-domain knowledge-enhanced benchmark with fine-grained annotations, named\n\\textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six\nsemantic topics and eight platforms. Each news item is enriched with\nmulti-modal content, potential social context, semi-manually verified common\nknowledge, and fine-grained annotations that surpass conventional binary\nlabels. Furthermore, we formulate three challenging tasks based on FineFake and\npropose a knowledge-enhanced domain adaptation network. Extensive experiments\nare conducted on FineFake under various scenarios, providing accurate and\nreliable benchmarks for future endeavors. The entire FineFake project is\npublicly accessible as an open-source repository at\n\\url{https://github.com/Accuser907/FineFake}.\n","authors":["Ziyi Zhou","Xiaoming Zhang","Litian Zhang","Jiacheng Liu","Senzhang Wang","Zheng Liu","Xi Zhang","Chaozhuo Li","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2404.01336v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11417v1","updated":"2024-10-15T09:07:25Z","published":"2024-10-15T09:07:25Z","title":"VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models","summary":"  Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.\n","authors":["Xiaohan Lan","Yitian Yuan","Zequn Jie","Lin Ma"],"pdf_url":"https://arxiv.org/pdf/2410.11417v1.pdf","comment":"9 pages, 4 figures"}]},"2024-10-14T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.05536v2","updated":"2024-10-14T23:20:26Z","published":"2024-10-07T22:25:37Z","title":"On Feature Decorrelation in Cloth-Changing Person Re-identification","summary":"  Cloth-changing person re-identification (CC-ReID) poses a significant\nchallenge in computer vision. A prevailing approach is to prompt models to\nconcentrate on causal attributes, like facial features and hairstyles, rather\nthan confounding elements such as clothing appearance. Traditional methods to\nachieve this involve integrating multi-modality data or employing manually\nannotated clothing labels, which tend to complicate the model and require\nextensive human effort. In our study, we demonstrate that simply reducing\nfeature correlations during training can significantly enhance the baseline\nmodel's performance. We theoretically elucidate this effect and introduce a\nnovel regularization technique based on density ratio estimation. This\ntechnique aims to minimize feature correlation in the training process of\ncloth-changing ReID baselines. Our approach is model-independent, offering\nbroad enhancements without needing additional data or labels. We validate our\nmethod through comprehensive experiments on prevalent CC-ReID datasets, showing\nits effectiveness in improving baseline models' generalization capabilities.\n","authors":["Hongjun Wang","Jiyuan Chen","Renhe Jiang","Xuan Song","Yinqiang Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.05536v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11046v1","updated":"2024-10-14T19:51:32Z","published":"2024-10-14T19:51:32Z","title":"SGUQ: Staged Graph Convolution Neural Network for Alzheimer's Disease\n  Diagnosis using Multi-Omics Data","summary":"  Alzheimer's disease (AD) is a chronic neurodegenerative disorder and the\nleading cause of dementia, significantly impacting cost, mortality, and burden\nworldwide. The advent of high-throughput omics technologies, such as genomics,\ntranscriptomics, proteomics, and epigenomics, has revolutionized the molecular\nunderstanding of AD. Conventional AI approaches typically require the\ncompletion of all omics data at the outset to achieve optimal AD diagnosis,\nwhich are inefficient and may be unnecessary. To reduce the clinical cost and\nimprove the accuracy of AD diagnosis using multi-omics data, we propose a novel\nstaged graph convolutional network with uncertainty quantification (SGUQ). SGUQ\nbegins with mRNA and progressively incorporates DNA methylation and miRNA data\nonly when necessary, reducing overall costs and exposure to harmful tests.\nExperimental results indicate that 46.23% of the samples can be reliably\npredicted using only single-modal omics data (mRNA), while an additional 16.04%\nof the samples can achieve reliable predictions when combining two omics data\ntypes (mRNA + DNA methylation). In addition, the proposed staged SGUQ achieved\nan accuracy of 0.858 on ROSMAP dataset, which outperformed existing methods\nsignificantly. The proposed SGUQ can not only be applied to AD diagnosis using\nmulti-omics data but also has the potential for clinical decision-making using\nmulti-viewed data. Our implementation is publicly available at\nhttps://github.com/chenzhao2023/multiomicsuncertainty.\n","authors":["Liang Tao","Yixin Xie","Jeffrey D Deng","Hui Shen","Hong-Wen Deng","Weihua Zhou","Chen Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.11046v1.pdf","comment":"20 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.10994v1","updated":"2024-10-14T18:20:09Z","published":"2024-10-14T18:20:09Z","title":"GraFPrint: A GNN-Based Approach for Audio Identification","summary":"  This paper introduces GraFPrint, an audio identification framework that\nleverages the structural learning capabilities of Graph Neural Networks (GNNs)\nto create robust audio fingerprints. Our method constructs a k-nearest neighbor\n(k-NN) graph from time-frequency representations and applies max-relative graph\nconvolutions to encode local and global information. The network is trained\nusing a self-supervised contrastive approach, which enhances resilience to\nambient distortions by optimizing feature representation. GraFPrint\ndemonstrates superior performance on large-scale datasets at various levels of\ngranularity, proving to be both lightweight and scalable, making it suitable\nfor real-world applications with extensive reference databases.\n","authors":["Aditya Bhattacharjee","Shubhr Singh","Emmanouil Benetos"],"pdf_url":"https://arxiv.org/pdf/2410.10994v1.pdf","comment":"Submitted to IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP 2025)"},{"id":"http://arxiv.org/abs/2410.10639v1","updated":"2024-10-14T15:50:35Z","published":"2024-10-14T15:50:35Z","title":"Generating Model Parameters for Controlling: Parameter Diffusion for\n  Controllable Multi-Task Recommendation","summary":"  Commercial recommender systems face the challenge that task requirements from\nplatforms or users often change dynamically (e.g., varying preferences for\naccuracy or diversity). Ideally, the model should be re-trained after resetting\na new objective function, adapting to these changes in task requirements.\nHowever, in practice, the high computational costs associated with retraining\nmake this process impractical for models already deployed to online\nenvironments. This raises a new challenging problem: how to efficiently adapt\nthe learning model to different task requirements by controlling model\nparameters after deployment, without the need for retraining. To address this\nissue, we propose a novel controllable learning approach via Parameter\nDiffusion for controllable multi-task Recommendation (PaDiRec), which allows\nthe customization and adaptation of recommendation model parameters to new task\nrequirements without retraining. Specifically, we first obtain the optimized\nmodel parameters through adapter tunning based on the feasible task\nrequirements. Then, we utilize the diffusion model as a parameter generator,\nemploying classifier-free guidance in conditional training to learn the\ndistribution of optimized model parameters under various task requirements.\nFinally, the diffusion model is applied to effectively generate model\nparameters in a test-time adaptation manner given task requirements. As a\nmodel-agnostic approach, PaDiRec can leverage existing recommendation models as\nbackbones to enhance their controllability. Extensive experiments on public\ndatasets and a dataset from a commercial app, indicate that PaDiRec can\neffectively enhance controllability through efficient model parameter\ngeneration. The code is released at\nhttps://anonymous.4open.science/r/PaDiRec-DD13.\n","authors":["Chenglei Shen","Jiahao Zhao","Xiao Zhang","Weijie Yu","Ming He","Jianping Fan"],"pdf_url":"https://arxiv.org/pdf/2410.10639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10594v1","updated":"2024-10-14T15:04:18Z","published":"2024-10-14T15:04:18Z","title":"VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents","summary":"  Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 25--39\\%\nend-to-end performance gain over traditional text-based RAG pipeline. Further\nanalysis reveals that VisRAG is effective in utilizing training data and\ndemonstrates strong generalization capability, positioning it as a promising\nsolution for RAG on multi-modality documents. Our code and data are available\nat https://github.com/openbmb/visrag .\n","authors":["Shi Yu","Chaoyue Tang","Bokai Xu","Junbo Cui","Junhao Ran","Yukun Yan","Zhenghao Liu","Shuo Wang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.10594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10542v1","updated":"2024-10-14T14:22:12Z","published":"2024-10-14T14:22:12Z","title":"Rethinking Legal Judgement Prediction in a Realistic Scenario in the Era\n  of Large Language Models","summary":"  This study investigates judgment prediction in a realistic scenario within\nthe context of Indian judgments, utilizing a range of transformer-based models,\nincluding InLegalBERT, BERT, and XLNet, alongside LLMs such as Llama-2 and\nGPT-3.5 Turbo. In this realistic scenario, we simulate how judgments are\npredicted at the point when a case is presented for a decision in court, using\nonly the information available at that time, such as the facts of the case,\nstatutes, precedents, and arguments. This approach mimics real-world\nconditions, where decisions must be made without the benefit of hindsight,\nunlike retrospective analyses often found in previous studies. For transformer\nmodels, we experiment with hierarchical transformers and the summarization of\njudgment facts to optimize input for these models. Our experiments with LLMs\nreveal that GPT-3.5 Turbo excels in realistic scenarios, demonstrating robust\nperformance in judgment prediction. Furthermore, incorporating additional legal\ninformation, such as statutes and precedents, significantly improves the\noutcome of the prediction task. The LLMs also provide explanations for their\npredictions. To evaluate the quality of these predictions and explanations, we\nintroduce two human evaluation metrics: Clarity and Linking. Our findings from\nboth automatic and human evaluations indicate that, despite advancements in\nLLMs, they are yet to achieve expert-level performance in judgment prediction\nand explanation tasks.\n","authors":["Shubham Kumar Nigam","Aniket Deroy","Subhankar Maity","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2410.10542v1.pdf","comment":"Accepted on NLLP at EMNLP 2024"},{"id":"http://arxiv.org/abs/2309.00976v4","updated":"2024-10-14T14:11:22Z","published":"2023-09-02T16:20:41Z","title":"Pure Message Passing Can Estimate Common Neighbor for Link Prediction","summary":"  Message Passing Neural Networks (MPNNs) have emerged as the {\\em de facto}\nstandard in graph representation learning. However, when it comes to link\nprediction, they often struggle, surpassed by simple heuristics such as Common\nNeighbor (CN). This discrepancy stems from a fundamental limitation: while\nMPNNs excel in node-level representation, they stumble with encoding the joint\nstructural features essential to link prediction, like CN. To bridge this gap,\nwe posit that, by harnessing the orthogonality of input vectors, pure\nmessage-passing can indeed capture joint structural features. Specifically, we\nstudy the proficiency of MPNNs in approximating CN heuristics. Based on our\nfindings, we introduce the Message Passing Link Predictor (MPLP), a novel link\nprediction model. MPLP taps into quasi-orthogonal vectors to estimate\nlink-level structural features, all while preserving the node-level\ncomplexities. Moreover, our approach demonstrates that leveraging\nmessage-passing to capture structural features could offset MPNNs'\nexpressiveness limitations at the expense of estimation variance. We conduct\nexperiments on benchmark datasets from various domains, where our method\nconsistently outperforms the baseline methods.\n","authors":["Kaiwen Dong","Zhichun Guo","Nitesh V. Chawla"],"pdf_url":"https://arxiv.org/pdf/2309.00976v4.pdf","comment":"Accepted to Neurips'24"},{"id":"http://arxiv.org/abs/2410.10455v1","updated":"2024-10-14T12:49:13Z","published":"2024-10-14T12:49:13Z","title":"Advancing Academic Knowledge Retrieval via LLM-enhanced Representation\n  Similarity Fusion","summary":"  In an era marked by robust technological growth and swift information\nrenewal, furnishing researchers and the populace with top-tier, avant-garde\nacademic insights spanning various domains has become an urgent necessity. The\nKDD Cup 2024 AQA Challenge is geared towards advancing retrieval models to\nidentify pertinent academic terminologies from suitable papers for scientific\ninquiries. This paper introduces the LLM-KnowSimFuser proposed by Robo Space,\nwhich wins the 2nd place in the competition. With inspirations drawed from the\nsuperior performance of LLMs on multiple tasks, after careful analysis of the\nprovided datasets, we firstly perform fine-tuning and inference using\nLLM-enhanced pre-trained retrieval models to introduce the tremendous language\nunderstanding and open-domain knowledge of LLMs into this task, followed by a\nweighted fusion based on the similarity matrix derived from the inference\nresults. Finally, experiments conducted on the competition datasets show the\nsuperiority of our proposal, which achieved a score of 0.20726 on the final\nleaderboard.\n","authors":["Wei Dai","Peng Fu","Chunjing Gan"],"pdf_url":"https://arxiv.org/pdf/2410.10455v1.pdf","comment":"The 2nd Place of KDD Cup 2024 OAG-Challenge AQA"},{"id":"http://arxiv.org/abs/2407.19669v2","updated":"2024-10-14T12:19:44Z","published":"2024-07-29T03:12:28Z","title":"mGTE: Generalized Long-Context Text Representation and Reranking Models\n  for Multilingual Text Retrieval","summary":"  We present systematic efforts in building long-context multilingual text\nrepresentation model (TRM) and reranker from scratch for text retrieval. We\nfirst introduce a text encoder (base size) enhanced with RoPE and unpadding,\npre-trained in a native 8192-token context (longer than 512 of previous\nmultilingual encoders). Then we construct a hybrid TRM and a cross-encoder\nreranker by contrastive learning. Evaluations show that our text encoder\noutperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM\nand reranker match the performance of large-sized state-of-the-art BGE-M3\nmodels and achieve better results on long-context retrieval benchmarks. Further\nanalysis demonstrate that our proposed models exhibit higher efficiency during\nboth training and inference. We believe their efficiency and effectiveness\ncould benefit various researches and industrial applications.\n","authors":["Xin Zhang","Yanzhao Zhang","Dingkun Long","Wen Xie","Ziqi Dai","Jialong Tang","Huan Lin","Baosong Yang","Pengjun Xie","Fei Huang","Meishan Zhang","Wenjie Li","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.19669v2.pdf","comment":"Camera-ready version of EMNLP 2024: Industry Track"},{"id":"http://arxiv.org/abs/2410.10408v1","updated":"2024-10-14T12:00:58Z","published":"2024-10-14T12:00:58Z","title":"Medico: Towards Hallucination Detection and Correction with Multi-source\n  Evidence Fusion","summary":"  As we all know, hallucinations prevail in Large Language Models (LLMs), where\nthe generated content is coherent but factually incorrect, which inflicts a\nheavy blow on the widespread application of LLMs. Previous studies have shown\nthat LLMs could confidently state non-existent facts rather than answering ``I\ndon't know''. Therefore, it is necessary to resort to external knowledge to\ndetect and correct the hallucinated content. Since manual detection and\ncorrection of factual errors is labor-intensive, developing an automatic\nend-to-end hallucination-checking approach is indeed a needful thing. To this\nend, we present Medico, a Multi-source evidence fusion enhanced hallucination\ndetection and correction framework. It fuses diverse evidence from multiple\nsources, detects whether the generated content contains factual errors,\nprovides the rationale behind the judgment, and iteratively revises the\nhallucinated content. Experimental results on evidence retrieval (0.964 HR@5,\n0.908 MRR@5), hallucination detection (0.927-0.951 F1), and hallucination\ncorrection (0.973-0.979 approval rate) manifest the great potential of Medico.\nA video demo of Medico can be found at https://youtu.be/RtsO6CSesBI.\n","authors":["Xinping Zhao","Jindi Yu","Zhenyu Liu","Jifang Wang","Dongfang Li","Yibin Chen","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.10408v1.pdf","comment":"12 pages, 3 figures, 6 tables. Accepted by EMNLP 2024's demo track"},{"id":"http://arxiv.org/abs/2410.10381v1","updated":"2024-10-14T11:10:15Z","published":"2024-10-14T11:10:15Z","title":"Collaborative filtering based on nonnegative/binary matrix factorization","summary":"  Collaborative filtering generates recommendations based on user-item\nsimilarities through rating data, which may involve numerous unrated items. To\npredict scores for unrated items, matrix factorization techniques, such as\nnonnegative matrix factorization (NMF), are often employed to predict scores\nfor unrated items. Nonnegative/binary matrix factorization (NBMF), which is an\nextension of NMF, approximates a nonnegative matrix as the product of\nnonnegative and binary matrices. Previous studies have employed NBMF for image\nanalysis where the data were dense. In this paper, we propose a modified NBMF\nalgorithm that can be applied to collaborative filtering where data are sparse.\nIn the modified method, unrated elements in a rating matrix are masked, which\nimproves the collaborative filtering performance. Utilizing a low-latency Ising\nmachine in NBMF is advantageous in terms of the computation time, making the\nproposed method beneficial.\n","authors":["Yukino Terui","Yuka Inoue","Yohei Hamakawa","Kosuke Tatsumura","Kazue Kudo"],"pdf_url":"https://arxiv.org/pdf/2410.10381v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.10372v1","updated":"2024-10-14T10:55:58Z","published":"2024-10-14T10:55:58Z","title":"BookWorm: A Dataset for Character Description and Analysis","summary":"  Characters are at the heart of every story, driving the plot and engaging\nreaders. In this study, we explore the understanding of characters in\nfull-length books, which contain complex narratives and numerous interacting\ncharacters. We define two tasks: character description, which generates a brief\nfactual profile, and character analysis, which offers an in-depth\ninterpretation, including character development, personality, and social\ncontext. We introduce the BookWorm dataset, pairing books from the Gutenberg\nProject with human-written descriptions and analyses. Using this dataset, we\nevaluate state-of-the-art long-context models in zero-shot and fine-tuning\nsettings, utilizing both retrieval-based and hierarchical processing for\nbook-length inputs. Our findings show that retrieval-based approaches\noutperform hierarchical ones in both tasks. Additionally, fine-tuned models\nusing coreference-based retrieval produce the most factual descriptions, as\nmeasured by fact- and entailment-based metrics. We hope our dataset,\nexperiments, and analysis will inspire further research in character-based\nnarrative understanding.\n","authors":["Argyrios Papoudakis","Mirella Lapata","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2410.10372v1.pdf","comment":"30 pages, 2 figures, EMNLP 2024 Findings"},{"id":"http://arxiv.org/abs/2410.10367v1","updated":"2024-10-14T10:46:32Z","published":"2024-10-14T10:46:32Z","title":"A Hybrid Filtering for Micro-video Hashtag Recommendation using\n  Graph-based Deep Neural Network","summary":"  Due to the growing volume of user generated content, hashtags are employed as\ntopic indicators to manage content efficiently on social media platforms.\nHowever, finding these vital topics is challenging in microvideos since they\ncontain substantial information in a short duration. Existing methods that\nrecommend hashtags for microvideos primarily focus on content and\npersonalization while disregarding relatedness among users. Moreover, the cold\nstart user issue prevails in hashtag recommendation systems. Considering the\nabove, we propose a hybrid filtering based MIcro-video haSHtag recommendatiON\nMISHON technique to recommend hashtags for micro-videos. Besides content based\nfiltering, we employ user-based collaborative filtering to enhance\nrecommendations. Since hashtags reflect users topical interests, we find\nsimilar users based on historical tagging behavior to model user relatedness.\nWe employ a graph-based deep neural network to model user to user, modality to\nmodality, and user to modality interactions. We then use refined modality\nspecific and user representations to recommend pertinent hashtags for\nmicrovideos. The empirical results on three real world datasets demonstrate\nthat MISHON attains a comparative enhancement of 3.6, 2.8, and 6.5 reported in\npercentage concerning the F1 score, respectively. Since cold start users exist\nwhose historical tagging information is unavailable, we also propose a content\nand social influence based technique to model the relatedness of cold start\nusers with influential users. The proposed solution shows a relative\nimprovement of 15.8 percent in the F1 score over its content only counterpart.\nThese results show that the proposed framework mitigates the cold start user\nproblem.\n","authors":["Shubhi Bansal","Kushaan Gowda","Mohammad Zia Ur Rehman","Chandravardhan Singh Raghaw","Nagendra Kumar"],"pdf_url":"https://arxiv.org/pdf/2410.10367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10360v1","updated":"2024-10-14T10:26:57Z","published":"2024-10-14T10:26:57Z","title":"Parenting: Optimizing Knowledge Selection of Retrieval-Augmented\n  Language Models with Parameter Decoupling and Tailored Tuning","summary":"  Retrieval-Augmented Generation (RAG) offers an effective solution to the\nissues faced by Large Language Models (LLMs) in hallucination generation and\nknowledge obsolescence by incorporating externally retrieved knowledge.\nHowever, due to potential conflicts between internal and external knowledge, as\nwell as retrieval noise, LLMs often struggle to effectively integrate external\nevidence, leading to a decline in performance. Although existing methods\nattempt to tackle these challenges, they often struggle to strike a balance\nbetween model adherence and robustness, resulting in significant learning\nvariance. Inspired by human cognitive processes, we propose Parenting, a novel\nframework that decouples adherence and robustness within the parameter space of\nLLMs. Specifically, Parenting utilizes a key parameter mining method based on\nforward activation gain to identify and isolate the crucial parameter units\nthat are strongly linked to adherence and robustness. Then, Parenting employs a\ntype-guided tailored tuning strategy, applying specific and appropriate\nfine-tuning methods to parameter units representing different capabilities,\naiming to achieve a balanced enhancement of adherence and robustness. Extensive\nexperiments on various datasets and models validate the effectiveness and\ngeneralizability of our methods.\n","authors":["Yongxin Xu","Ruizhe Zhang","Xinke Jiang","Yujie Feng","Yuzhen Xiao","Xinyu Ma","Runchuan Zhu","Xu Chu","Junfeng Zhao","Yasha Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10296v1","updated":"2024-10-14T08:49:11Z","published":"2024-10-14T08:49:11Z","title":"Enhancing Attributed Graph Networks with Alignment and Uniformity\n  Constraints for Session-based Recommendation","summary":"  Session-based Recommendation (SBR), seeking to predict a user's next action\nbased on an anonymous session, has drawn increasing attention for its\npracticability. Most SBR models only rely on the contextual transitions within\na short session to learn item representations while neglecting additional\nvaluable knowledge. As such, their model capacity is largely limited by the\ndata sparsity issue caused by short sessions. A few studies have exploited the\nModeling of Item Attributes (MIA) to enrich item representations. However, they\nusually involve specific model designs that can hardly transfer to existing\nattribute-agnostic SBR models and thus lack universality. In this paper, we\npropose a model-agnostic framework, named AttrGAU (Attributed Graph Networks\nwith Alignment and Uniformity Constraints), to bring the MIA's superiority into\nexisting attribute-agnostic models, to improve their accuracy and robustness\nfor recommendation. Specifically, we first build a bipartite attributed graph\nand design an attribute-aware graph convolution to exploit the rich attribute\nsemantics hidden in the heterogeneous item-attribute relationship. We then\ndecouple existing attribute-agnostic SBR models into the graph neural network\nand attention readout sub-modules to satisfy the non-intrusive requirement.\nLastly, we design two representation constraints, i.e., alignment and\nuniformity, to optimize distribution discrepancy in representation between the\nattribute semantics and collaborative semantics. Extensive experiments on three\npublic benchmark datasets demonstrate that the proposed AttrGAU framework can\nsignificantly enhance backbone models' recommendation performance and\nrobustness against data sparsity and data noise issues. Our implementation\ncodes will be available at https://github.com/ItsukiFujii/AttrGAU.\n","authors":["Xinping Zhao","Chaochao Chen","Jiajie Su","Yizhao Zhang","Baotian Hu"],"pdf_url":"https://arxiv.org/pdf/2410.10296v1.pdf","comment":"11 pages, 4 figures, 5 tables. Accepted by ICWS 2024"},{"id":"http://arxiv.org/abs/2410.10293v1","updated":"2024-10-14T08:47:21Z","published":"2024-10-14T08:47:21Z","title":"FunnelRAG: A Coarse-to-Fine Progressive Retrieval Paradigm for RAG","summary":"  Retrieval-Augmented Generation (RAG) prevails in Large Language Models. It\nmainly consists of retrieval and generation. The retrieval modules (a.k.a.\nretrievers) aim to find useful information used to facilitate generation\nmodules (a.k.a. generators). As such, generators' performance largely depends\non the effectiveness and efficiency of retrievers. However, the retrieval\nparadigm that we design and use remains flat, which treats the retrieval\nprocedures as a one-off deal with constant granularity. Despite effectiveness,\nwe argue that they suffer from two limitations: (1) flat retrieval exerts a\nsignificant burden on one retriever; (2) constant granularity limits the\nceiling of retrieval performance. In this work, we propose a progressive\nretrieval paradigm with coarse-to-fine granularity for RAG, termed FunnelRAG,\nso as to balance effectiveness and efficiency. Specifically, FunnelRAG\nestablishes a progressive retrieval pipeline by collaborating coarse-to-fine\ngranularity, large-to-small quantity, and low-to-high capacity, which can\nrelieve the burden on one retriever and also promote the ceiling of retrieval\nperformance. Extensive experiments manifest that FunnelRAG achieves comparable\nretrieval performance while the time overhead is reduced by nearly 40 percent.\n","authors":["Xinping Zhao","Yan Zhong","Zetian Sun","Xinshuo Hu","Zhenyu Liu","Dongfang Li","Baotian Hu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.10293v1.pdf","comment":"18 pages, 6 figures, 13 tables"},{"id":"http://arxiv.org/abs/2410.10286v1","updated":"2024-10-14T08:38:29Z","published":"2024-10-14T08:38:29Z","title":"Back-of-the-Book Index Automation for Arabic Documents","summary":"  Back-of-the-book indexes are crucial for book readability. Their manual\ncreation is laborious and error prone. In this paper, we consider automating\nback-of-the-book index extraction for Arabic books to help simplify both the\ncreation and review tasks. Given a back-of-the-book index, we aim to check and\nidentify the accurate occurrences of index terms relative to the associated\npages. To achieve this, we first define a pool of candidates for each term by\nextracting all possible noun phrases from paragraphs appearing on the relevant\nindex pages. These noun phrases, identified through part-of-speech analysis,\nare stored in a vector database for efficient retrieval. We use several\nmetrics, including exact matches, lexical similarity, and semantic similarity,\nto determine the most appropriate occurrence. The candidate with the highest\nscore based on these metrics is chosen as the occurrence of the term. We\nfine-tuned a heuristic method, that considers the above metrics and that\nachieves an F1-score of .966 (precision=.966, recall=.966). These excellent\nresults open the door for future work related to automation of back-of-the-book\nindex generation and checking.\n","authors":["Nawal Haidar","Fadi A. Zaraket"],"pdf_url":"https://arxiv.org/pdf/2410.10286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08478v2","updated":"2024-10-14T07:55:16Z","published":"2024-10-11T03:10:09Z","title":"Personalized Item Representations in Federated Multimodal Recommendation","summary":"  Federated recommendation systems are essential for providing personalized\nrecommendations while protecting user privacy. However, current methods mainly\nrely on ID-based item embeddings, neglecting the rich multimodal information of\nitems. To address this, we propose a Federated Multimodal Recommendation\nSystem, called FedMR. FedMR uses a foundation model on the server to encode\nmultimodal item data, such as images and text. To handle data heterogeneity\ncaused by user preference differences, FedMR introduces a Mixing Feature Fusion\nModule on each client, which adjusts fusion strategy weights based on user\ninteraction history to generate personalized item representations that capture\nusers' fine-grained preferences. FedMR is compatible with existing ID-based\nfederated recommendation systems, improving performance without modifying the\noriginal framework. Experiments on four real-world multimodal datasets\ndemonstrate FedMR's effectiveness. The code is available at\nhttps://anonymous.4open.science/r/FedMR.\n","authors":["Zhiwei Li","Guodong Long","Jing Jiang","Chengqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.08478v2.pdf","comment":"12 pages, 4 figures, 5 tables, conference"},{"id":"http://arxiv.org/abs/2410.10130v1","updated":"2024-10-14T03:37:47Z","published":"2024-10-14T03:37:47Z","title":"DecKG: Decentralized Collaborative Learning with Knowledge Graph\n  Enhancement for POI Recommendation","summary":"  Decentralized collaborative learning for Point-of-Interest (POI)\nrecommendation has gained research interest due to its advantages in privacy\npreservation and efficiency, as it keeps data locally and leverages\ncollaborative learning among clients to train models in a decentralized manner.\nHowever, since local data is often limited and insufficient for training\naccurate models, a common solution is integrating external knowledge as\nauxiliary information to enhance model performance. Nevertheless, this solution\nposes challenges for decentralized collaborative learning. Due to private\nnature of local data, identifying relevant auxiliary information specific to\neach user is non-trivial. Furthermore, resource-constrained local devices\nstruggle to accommodate all auxiliary information, which places heavy burden on\nlocal storage. To fill the gap, we propose a novel decentralized collaborative\nlearning with knowledge graph enhancement framework for POI recommendation\n(DecKG). Instead of directly uploading interacted items, users generate\ndesensitized check-in data by uploading general categories of interacted items\nand sampling similar items from same category. The server then pretrains KG\nwithout sensitive user-item interactions and deploys relevant partitioned\nsub-KGs to individual users. Entities are further refined on the device,\nallowing client to client communication to exchange knowledge learned from\nlocal data and sub-KGs. Evaluations across two real-world datasets demonstrate\nDecKG's effectiveness recommendation performance.\n","authors":["Ruiqi Zheng","Liang Qu","Guanhua Ye","Tong Chen","Yuhui Shi","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2410.10130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10127v1","updated":"2024-10-14T03:26:51Z","published":"2024-10-14T03:26:51Z","title":"MAIR: A Massive Benchmark for Evaluating Instructed Retrieval","summary":"  Recent information retrieval (IR) models are pre-trained and\ninstruction-tuned on massive datasets and tasks, enabling them to perform well\non a wide range of tasks and potentially generalize to unseen tasks with\ninstructions. However, existing IR benchmarks focus on a limited scope of\ntasks, making them insufficient for evaluating the latest IR models. In this\npaper, we propose MAIR (Massive Instructed Retrieval Benchmark), a\nheterogeneous IR benchmark that includes 126 distinct IR tasks across 6\ndomains, collected from existing datasets. We benchmark state-of-the-art\ninstruction-tuned text embedding models and re-ranking models. Our experiments\nreveal that instruction-tuned models generally achieve superior performance\ncompared to non-instruction-tuned models on MAIR. Additionally, our results\nsuggest that current instruction-tuned text embedding models and re-ranking\nmodels still lack effectiveness in specific long-tail tasks. MAIR is publicly\navailable at https://github.com/sunnweiwei/Mair.\n","authors":["Weiwei Sun","Zhengliang Shi","Jiulong Wu","Lingyong Yan","Xinyu Ma","Yiding Liu","Min Cao","Dawei Yin","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2410.10127v1.pdf","comment":"EMNLP 2024"},{"id":"http://arxiv.org/abs/2403.16504v3","updated":"2024-10-14T03:26:10Z","published":"2024-03-25T07:38:40Z","title":"LARA: Linguistic-Adaptive Retrieval-Augmentation for Multi-Turn Intent\n  Classification","summary":"  Multi-turn intent classification is notably challenging due to the complexity\nand evolving nature of conversational contexts. This paper introduces LARA, a\nLinguistic-Adaptive Retrieval-Augmentation framework to enhance accuracy in\nmulti-turn classification tasks across six languages, accommodating a large\nnumber of intents in chatbot interactions. LARA combines a fine-tuned smaller\nmodel with a retrieval-augmented mechanism, integrated within the architecture\nof LLMs. The integration allows LARA to dynamically utilize past dialogues and\nrelevant intents, thereby improving the understanding of the context.\nFurthermore, our adaptive retrieval techniques bolster the cross-lingual\ncapabilities of LLMs without extensive retraining and fine-tuning.\nComprehensive experiments demonstrate that LARA achieves state-of-the-art\nperformance on multi-turn intent classification tasks, enhancing the average\naccuracy by 3.67\\% from state-of-the-art single-turn intent classifiers.\n","authors":["Junhua Liu","Yong Keat Tan","Bin Fu","Kwan Hui Lim"],"pdf_url":"https://arxiv.org/pdf/2403.16504v3.pdf","comment":"Accepted to EMNLP'24"},{"id":"http://arxiv.org/abs/2409.02864v2","updated":"2024-10-14T01:07:54Z","published":"2024-09-04T16:43:14Z","title":"Language Model Powered Digital Biology","summary":"  Recent advancements in Large Language Models (LLMs) are transforming biology,\ncomputer science, and many other research fields, as well as impacting everyday\nlife. While transformer-based technologies are currently being deployed in\nbiology, no available agentic system has been developed to tackle\nbioinformatics workflows. We present a prototype Bioinformatics Retrieval\nAugmented Data (BRAD) digital assistant. BRAD is a chatbot and agentic system\nthat integrates a suite of tools to handle bioinformatics tasks, from code\nexecution to online search. We demonstrate its capabilities through (1)\nimproved question-and-answering with retrieval augmented generation (RAG), (2)\nthe ability to run complex software pipelines, and (3) the ability to organize\nand distribute tasks in agentic workflows. We use BRAD for automation,\nperforming tasks ranging from gene enrichment and searching the archive to\nautomatic code generation for running biomarker identification pipelines. BRAD\nis a step toward autonomous, self-driving labs for digital biology.\n","authors":["Joshua Pickard","Marc Andrew Choi","Natalie Oliven","Cooper Stansbury","Jillian Cwycyshyn","Nicholas Galioto","Alex Gorodetsky","Alvaro Velasquez","Indika Rajapakse"],"pdf_url":"https://arxiv.org/pdf/2409.02864v2.pdf","comment":"49 pages, 3 tables, 12 figures"},{"id":"http://arxiv.org/abs/2310.16605v4","updated":"2024-10-14T00:45:35Z","published":"2023-10-25T12:50:34Z","title":"Enhancing Dense Retrievers' Robustness with Group-level Reweighting","summary":"  The anchor-document data derived from web graphs offers a wealth of paired\ninformation for training dense retrieval models in an unsupervised manner.\nHowever, unsupervised data contains diverse patterns across the web graph and\noften exhibits significant imbalance, leading to suboptimal performance in\nunderrepresented or difficult groups. In this paper, we introduce WebDRO, an\nefficient approach for clustering the web graph data and optimizing group\nweights to enhance the robustness of dense retrieval models. Initially, we\nbuild an embedding model for clustering anchor-document pairs. Specifically, we\ncontrastively train the embedding model for link prediction, which guides the\nembedding model in capturing the document features behind the web graph links.\nSubsequently, we employ the group distributional robust optimization to\nrecalibrate the weights across different clusters of anchor-document pairs\nduring training retrieval models. During training, we direct the model to\nassign higher weights to clusters with higher loss and focus more on worst-case\nscenarios. This approach ensures that the model has strong generalization\nability on all data patterns. Our experiments on MS MARCO and BEIR demonstrate\nthat our method can effectively improve retrieval performance in unsupervised\ntraining and finetuning settings. Further analysis confirms the stability and\nvalidity of group weights learned by WebDRO. The code of this paper can be\nobtained from https://github.com/Hanpx20/GroupDRO_Dense_Retrieval.\n","authors":["Peixuan Han","Zhenghao Liu","Zhiyuan Liu","Chenyan Xiong"],"pdf_url":"https://arxiv.org/pdf/2310.16605v4.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.06753v3","updated":"2024-10-14T16:06:54Z","published":"2024-08-13T09:19:59Z","title":"Detecting Audio-Visual Deepfakes with Fine-Grained Inconsistencies","summary":"  Existing methods on audio-visual deepfake detection mainly focus on\nhigh-level features for modeling inconsistencies between audio and visual data.\nAs a result, these approaches usually overlook finer audio-visual artifacts,\nwhich are inherent to deepfakes. Herein, we propose the introduction of\nfine-grained mechanisms for detecting subtle artifacts in both spatial and\ntemporal domains. First, we introduce a local audio-visual model capable of\ncapturing small spatial regions that are prone to inconsistencies with audio.\nFor that purpose, a fine-grained mechanism based on a spatially-local distance\ncoupled with an attention module is adopted. Second, we introduce a\ntemporally-local pseudo-fake augmentation to include samples incorporating\nsubtle temporal inconsistencies in our training set. Experiments on the DFDC\nand the FakeAVCeleb datasets demonstrate the superiority of the proposed method\nin terms of generalization as compared to the state-of-the-art under both\nin-dataset and cross-dataset settings.\n","authors":["Marcella Astrid","Enjie Ghorbel","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2408.06753v3.pdf","comment":"Accepted in BMVC 2024"},{"id":"http://arxiv.org/abs/2409.02845v2","updated":"2024-10-14T14:55:13Z","published":"2024-09-04T16:17:41Z","title":"Multi-Track MusicLDM: Towards Versatile Music Generation with Latent\n  Diffusion Model","summary":"  Diffusion models have shown promising results in cross-modal generation tasks\ninvolving audio and music, such as text-to-sound and text-to-music generation.\nThese text-controlled music generation models typically focus on generating\nmusic by capturing global musical attributes like genre and mood. However,\nmusic composition is a complex, multilayered task that often involves musical\narrangement as an integral part of the process. This process involves composing\neach instrument to align with existing ones in terms of beat, dynamics,\nharmony, and melody, requiring greater precision and control over tracks than\ntext prompts usually provide. In this work, we address these challenges by\nextending the MusicLDM, a latent diffusion model for music, into a multi-track\ngenerative model. By learning the joint probability of tracks sharing a\ncontext, our model is capable of generating music across several tracks that\ncorrespond well to each other, either conditionally or unconditionally.\nAdditionally, our model is capable of arrangement generation, where the model\ncan generate any subset of tracks given the others (e.g., generating a piano\ntrack complementing given bass and drum tracks). We compared our model with an\nexisting multi-track generative model and demonstrated that our model achieves\nconsiderable improvements across objective metrics for both total and\narrangement generation tasks.\n","authors":["Tornike Karchkhadze","Mohammad Rasool Izadi","Ke Chen","Gerard Assayag","Shlomo Dubnov"],"pdf_url":"https://arxiv.org/pdf/2409.02845v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14485v7","updated":"2024-10-14T14:26:04Z","published":"2024-06-20T16:48:14Z","title":"Proceedings of The second international workshop on eXplainable AI for\n  the Arts (XAIxArts)","summary":"  This second international workshop on explainable AI for the Arts (XAIxArts)\nbrought together a community of researchers in HCI, Interaction Design, AI,\nexplainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\nWorkshop held at the 16th ACM Conference on Creativity and Cognition (C&C\n2024), Chicago, USA.\n","authors":["Nick Bryan-Kinns","Corey Ford","Shuoyang Zheng","Helen Kennedy","Alan Chamberlain","Makayla Lewis","Drew Hemment","Zijin Li","Qiong Wu","Lanxi Xiao","Gus Xia","Jeba Rezwana","Michael Clemens","Gabriel Vigliensoni"],"pdf_url":"https://arxiv.org/pdf/2406.14485v7.pdf","comment":"Proceedings of The second international workshop on eXplainable AI\n  for the Arts (XAIxArts)"},{"id":"http://arxiv.org/abs/2410.10319v1","updated":"2024-10-14T09:25:09Z","published":"2024-10-14T09:25:09Z","title":"Spatial-Aware Efficient Projector for MLLMs via Multi-Layer Feature\n  Aggregation","summary":"  The projector plays a crucial role in multi-modal language models (MLLMs).\nThe number of visual tokens it outputs affects the efficiency of the MLLM,\nwhile the quality of the visual tokens influences the visual understanding\ncapabilities of the MLLM. Current explorations on the projector focus on\nreducing the number of visual tokens to improve efficiency, often overlooking\nthe inherent spatial discrepancy between the serialized 2-dimensional visual\ntoken sequences and natural language token sequences. A Spatial-Aware Efficient\nProjector (SAEP) is proposed to address this issue. In detail, our SAEP method\nemploys an modified separable depthwise convolution module on multi-layer\nvisual features to enhance the spatial information of visual tokens. As a\nresult, our SAEP method can not only largely reduce the number of visual tokens\nby 75\\%, but also significantly improve the multimodal spatial understanding\ncapability of MLLMs. Moreover, compared to existing projectors, our SAEP gets\nbest performances on massive multimodal evaluation benchmarks, which denotes\nits effectiveness on bridging the modality gap.\n","authors":["Shun Qian","Bingquan Liu","Chengjie Sun","Zhen Xu","Baoxun Wang"],"pdf_url":"https://arxiv.org/pdf/2410.10319v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2405.07930v2","updated":"2024-10-14T08:19:13Z","published":"2024-05-13T17:01:28Z","title":"Improving Multimodal Learning with Multi-Loss Gradient Modulation","summary":"  Learning from multiple modalities, such as audio and video, offers\nopportunities for leveraging complementary information, enhancing robustness,\nand improving contextual understanding and performance. However, combining such\nmodalities presents challenges, especially when modalities differ in data\nstructure, predictive contribution, and the complexity of their learning\nprocesses. It has been observed that one modality can potentially dominate the\nlearning process, hindering the effective utilization of information from other\nmodalities and leading to sub-optimal model performance. To address this issue\nthe vast majority of previous works suggest to assess the unimodal\ncontributions and dynamically adjust the training to equalize them. We improve\nupon previous work by introducing a multi-loss objective and further refining\nthe balancing process, allowing it to dynamically adjust the learning pace of\neach modality in both directions, acceleration and deceleration, with the\nability to phase out balancing effects upon convergence. We achieve superior\nresults across three audio-video datasets: on CREMA-D, models with ResNet\nbackbone encoders surpass the previous best by 1.9% to 12.4%, and Conformer\nbackbone models deliver improvements ranging from 2.8% to 14.1% across\ndifferent fusion methods. On AVE, improvements range from 2.7% to 7.7%, while\non UCF101, gains reach up to 6.1%.\n","authors":["Konstantinos Kontras","Christos Chatzichristos","Matthew Blaschko","Maarten De Vos"],"pdf_url":"https://arxiv.org/pdf/2405.07930v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10178v1","updated":"2024-10-14T05:51:53Z","published":"2024-10-14T05:51:53Z","title":"GUISE: Graph GaUssIan Shading watErmark","summary":"  In the expanding field of generative artificial intelligence, integrating\nrobust watermarking technologies is essential to protect intellectual property\nand maintain content authenticity. Traditionally, watermarking techniques have\nbeen developed primarily for rich information media such as images and audio.\nHowever, these methods have not been adequately adapted for graph-based data,\nparticularly molecular graphs. Latent 3D graph diffusion(LDM-3DG) is an\nascendant approach in the molecular graph generation field. This model\neffectively manages the complexities of molecular structures, preserving\nessential symmetries and topological features. We adapt the Gaussian Shading, a\nproven performance lossless watermarking technique, to the latent graph\ndiffusion domain to protect this sophisticated new technology. Our adaptation\nsimplifies the watermark diffusion process through duplication and padding,\nmaking it adaptable and suitable for various message types. We conduct several\nexperiments using the LDM-3DG model on publicly available datasets QM9 and\nDrugs, to assess the robustness and effectiveness of our technique. Our results\ndemonstrate that the watermarked molecules maintain statistical parity in 9 out\nof 10 performance metrics compared to the original. Moreover, they exhibit a\n100% detection rate and a 99% extraction rate in a 2D decoded pipeline, while\nalso showing robustness against post-editing attacks.\n","authors":["Renyi Yang"],"pdf_url":"https://arxiv.org/pdf/2410.10178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13711v2","updated":"2024-10-14T01:43:38Z","published":"2024-08-25T02:56:26Z","title":"SceneDreamer360: Text-Driven 3D-Consistent Scene Generation with\n  Panoramic Gaussian Splatting","summary":"  Text-driven 3D scene generation has seen significant advancements recently.\nHowever, most existing methods generate single-view images using generative\nmodels and then stitch them together in 3D space. This independent generation\nfor each view often results in spatial inconsistency and implausibility in the\n3D scenes. To address this challenge, we proposed a novel text-driven\n3D-consistent scene generation model: SceneDreamer360. Our proposed method\nleverages a text-driven panoramic image generation model as a prior for 3D\nscene generation and employs 3D Gaussian Splatting (3DGS) to ensure consistency\nacross multi-view panoramic images. Specifically, SceneDreamer360 enhances the\nfine-tuned Panfusion generator with a three-stage panoramic enhancement,\nenabling the generation of high-resolution, detail-rich panoramic images.\nDuring the 3D scene construction, a novel point cloud fusion initialization\nmethod is used, producing higher quality and spatially consistent point clouds.\nOur extensive experiments demonstrate that compared to other methods,\nSceneDreamer360 with its panoramic image generation and 3DGS can produce higher\nquality, spatially consistent, and visually appealing 3D scenes from any text\nprompt. Our codes are available at\n\\url{https://github.com/liwrui/SceneDreamer360}.\n","authors":["Wenrui Li","Fucheng Cai","Yapeng Mi","Zhe Yang","Wangmeng Zuo","Xingtao Wang","Xiaopeng Fan"],"pdf_url":"https://arxiv.org/pdf/2408.13711v2.pdf","comment":null}]},"2024-10-13T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.09999v1","updated":"2024-10-13T20:45:00Z","published":"2024-10-13T20:45:00Z","title":"Leveraging Customer Feedback for Multi-modal Insight Extraction","summary":"  Businesses can benefit from customer feedback in different modalities, such\nas text and images, to enhance their products and services. However, it is\ndifficult to extract actionable and relevant pairs of text segments and images\nfrom customer feedback in a single pass. In this paper, we propose a novel\nmulti-modal method that fuses image and text information in a latent space and\ndecodes it to extract the relevant feedback segments using an image-text\ngrounded text decoder. We also introduce a weakly-supervised data generation\ntechnique that produces training data for this task. We evaluate our model on\nunseen data and demonstrate that it can effectively mine actionable insights\nfrom multi-modal customer feedback, outperforming the existing baselines by\n$14$ points in F1 score.\n","authors":["Sandeep Sricharan Mukku","Abinesh Kanagarajan","Pushpendu Ghosh","Chetan Aggarwal"],"pdf_url":"https://arxiv.org/pdf/2410.09999v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2410.09942v1","updated":"2024-10-13T17:53:50Z","published":"2024-10-13T17:53:50Z","title":"Learning to Rank for Multiple Retrieval-Augmented Models through\n  Iterative Utility Maximization","summary":"  This paper investigates the design of a unified search engine to serve\nmultiple retrieval-augmented generation (RAG) agents, each with a distinct\ntask, backbone large language model (LLM), and retrieval-augmentation strategy.\nWe introduce an iterative approach where the search engine generates retrieval\nresults for these RAG agents and gathers feedback on the quality of the\nretrieved documents during an offline phase. This feedback is then used to\niteratively optimize the search engine using a novel expectation-maximization\nalgorithm, with the goal of maximizing each agent's utility function.\nAdditionally, we adapt this approach to an online setting, allowing the search\nengine to refine its behavior based on real-time individual agents feedback to\nbetter serve the results for each of them. Experiments on diverse datasets from\nthe Knowledge-Intensive Language Tasks (KILT) benchmark demonstrates that our\napproach significantly on average outperforms competitive baselines across 18\nRAG models. We also demonstrate that our method effectively ``personalizes''\nthe retrieval process for each RAG agent based on the collected feedback.\nFinally, we provide a comprehensive ablation study to explore various aspects\nof our method.\n","authors":["Alireza Salemi","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2410.09942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09936v1","updated":"2024-10-13T17:44:04Z","published":"2024-10-13T17:44:04Z","title":"The Role of Fake Users in Sequential Recommender Systems","summary":"  Sequential Recommender Systems (SRSs) are widely used to model user behavior\nover time, yet their robustness remains an under-explored area of research. In\nthis paper, we conduct an empirical study to assess how the presence of fake\nusers, who engage in random interactions, follow popular or unpopular items, or\nfocus on a single genre, impacts the performance of SRSs in real-world\nscenarios. We evaluate two SRS models across multiple datasets, using\nestablished metrics such as Normalized Discounted Cumulative Gain (NDCG) and\nRank Sensitivity List (RLS) to measure performance. While traditional metrics\nlike NDCG remain relatively stable, our findings reveal that the presence of\nfake users severely degrades RLS metrics, often reducing them to near-zero\nvalues. These results highlight the need for further investigation into the\neffects of fake users on training data and emphasize the importance of\ndeveloping more resilient SRSs that can withstand different types of\nadversarial attacks.\n","authors":["Filippo Betello"],"pdf_url":"https://arxiv.org/pdf/2410.09936v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2410.09923v1","updated":"2024-10-13T17:08:16Z","published":"2024-10-13T17:08:16Z","title":"Analysis and Design of a Personalized Recommendation System Based on a\n  Dynamic User Interest Model","summary":"  With the rapid development of the internet and the explosion of information,\nproviding users with accurate personalized recommendations has become an\nimportant research topic. This paper designs and analyzes a personalized\nrecommendation system based on a dynamic user interest model. The system\ncaptures user behavior data, constructs a dynamic user interest model, and\ncombines multiple recommendation algorithms to provide personalized content to\nusers. The research results show that this system significantly improves\nrecommendation accuracy and user satisfaction. This paper discusses the\nsystem's architecture design, algorithm implementation, and experimental\nresults in detail and explores future research directions.\n","authors":["Chunyan Mao","Shuaishuai Huang","Mingxiu Sui","Haowei Yang","Xueshe Wang"],"pdf_url":"https://arxiv.org/pdf/2410.09923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09875v1","updated":"2024-10-13T15:34:11Z","published":"2024-10-13T15:34:11Z","title":"ViFi-ReID: A Two-Stream Vision-WiFi Multimodal Approach for Person\n  Re-identification","summary":"  Person re-identification(ReID), as a crucial technology in the field of\nsecurity, plays a vital role in safety inspections, personnel counting, and\nmore. Most current ReID approaches primarily extract features from images,\nwhich are easily affected by objective conditions such as clothing changes and\nocclusions. In addition to cameras, we leverage widely available routers as\nsensing devices by capturing gait information from pedestrians through the\nChannel State Information (CSI) in WiFi signals and contribute a multimodal\ndataset. We employ a two-stream network to separately process video\nunderstanding and signal analysis tasks, and conduct multi-modal fusion and\ncontrastive learning on pedestrian video and WiFi data. Extensive experiments\nin real-world scenarios demonstrate that our method effectively uncovers the\ncorrelations between heterogeneous data, bridges the gap between visual and\nsignal modalities, significantly expands the sensing range, and improves ReID\naccuracy across multiple sensors.\n","authors":["Chen Mao","Chong Tan","Jingqi Hu","Min Zheng"],"pdf_url":"https://arxiv.org/pdf/2410.09875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10808v2","updated":"2024-10-13T15:33:52Z","published":"2024-08-20T12:58:16Z","title":"ColBERT Retrieval and Ensemble Response Scoring for Language Model\n  Question Answering","summary":"  Domain-specific question answering remains challenging for language models,\ngiven the deep technical knowledge required to answer questions correctly. This\ndifficulty is amplified for smaller language models that cannot encode as much\ninformation in their parameters as larger models. The \"Specializing Large\nLanguage Models for Telecom Networks\" challenge aimed to enhance the\nperformance of two small language models, Phi-2 and Falcon-7B in\ntelecommunication question answering. In this paper, we present our question\nanswering systems for this challenge. Our solutions achieved leading marks of\n81.9% accuracy for Phi-2 and 57.3% for Falcon-7B. We have publicly released our\ncode and fine-tuned models.\n","authors":["Alex Gichamba","Tewodros Kederalah Idris","Brian Ebiyau","Eric Nyberg","Teruko Mitamura"],"pdf_url":"https://arxiv.org/pdf/2408.10808v2.pdf","comment":"7 pages, 2 figures, and 8 tables. This paper has been accepted at the\n  2024 IEEE Global Communications (GLOBECOM) Workshops"},{"id":"http://arxiv.org/abs/2406.16048v2","updated":"2024-10-13T15:30:32Z","published":"2024-06-23T08:24:08Z","title":"Evaluating D-MERIT of Partial-annotation on Information Retrieval","summary":"  Retrieval models are often evaluated on partially-annotated datasets. Each\nquery is mapped to a few relevant texts and the remaining corpus is assumed to\nbe irrelevant. As a result, models that successfully retrieve false negatives\nare punished in evaluation. Unfortunately, completely annotating all texts for\nevery query is not resource efficient. In this work, we show that using\npartially-annotated datasets in evaluation can paint a distorted picture. We\ncurate D-MERIT, a passage retrieval evaluation set from Wikipedia, aspiring to\ncontain all relevant passages for each query. Queries describe a group (e.g.,\n\"journals about linguistics\") and relevant passages are evidence that entities\nbelong to the group (e.g., a passage indicating that \"Language\" is a journal\nabout linguistics). We show that evaluating on a dataset containing annotations\nfor only a subset of the relevant passages might result in misleading ranking\nof the retrieval systems and that as more relevant texts are included in the\nevaluation set, the rankings converge. We propose our dataset as a resource for\nevaluation and our study as a recommendation for balance between\nresource-efficiency and reliable evaluation when annotating evaluation sets for\ntext retrieval.\n","authors":["Royi Rassin","Yaron Fairstein","Oren Kalinsky","Guy Kushilevitz","Nachshon Cohen","Alexander Libov","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2406.16048v2.pdf","comment":"Accepted to EMNLP 2024 main track. Our dataset can be downloaded from\n  https://D-MERIT.github.io"},{"id":"http://arxiv.org/abs/2410.09871v1","updated":"2024-10-13T15:11:31Z","published":"2024-10-13T15:11:31Z","title":"A Comparative Study of PDF Parsing Tools Across Diverse Document\n  Categories","summary":"  PDF is one of the most prominent data formats, making PDF parsing crucial for\ninformation extraction and retrieval, particularly with the rise of RAG\nsystems. While various PDF parsing tools exist, their effectiveness across\ndifferent document types remains understudied, especially beyond academic\npapers. Our research aims to address this gap by comparing 10 popular PDF\nparsing tools across 6 document categories using the DocLayNet dataset. These\ntools include PyPDF, pdfminer.six, PyMuPDF, pdfplumber, pypdfium2,\nUnstructured, Tabula, Camelot, as well as the deep learning-based tools Nougat\nand Table Transformer(TATR). We evaluated both text extraction and table\ndetection capabilities. For text extraction, PyMuPDF and pypdfium generally\noutperformed others, but all parsers struggled with Scientific and Patent\ndocuments. For these challenging categories, learning-based tools like Nougat\ndemonstrated superior performance. In table detection, TATR excelled in the\nFinancial, Patent, Law & Regulations, and Scientific categories. Table\ndetection tool Camelot performed best for tender documents, while PyMuPDF\nperformed superior in the Manual category. Our findings highlight the\nimportance of selecting appropriate parsing tools based on document type and\nspecific tasks, providing valuable insights for researchers and practitioners\nworking with diverse document sources.\n","authors":["Narayan S. Adhikari","Shradha Agarwal"],"pdf_url":"https://arxiv.org/pdf/2410.09871v1.pdf","comment":"17 pages,11 figures, 5 tables"},{"id":"http://arxiv.org/abs/2410.09829v1","updated":"2024-10-13T13:07:31Z","published":"2024-10-13T13:07:31Z","title":"Generating Driving Simulations via Conversation","summary":"  Cyber-physical systems like autonomous vehicles are tested in simulation\nbefore deployment, using domain-specific programs for scenario specification.\nTo aid the testing of autonomous vehicles in simulation, we design a natural\nlanguage interface, using an instruction-following large language model, to\nassist a non-coding domain expert in synthesising the desired scenarios and\nvehicle behaviours. We show that using it to convert utterances to the symbolic\nprogram is feasible, despite the very small training dataset. Human experiments\nshow that dialogue is critical to successful simulation generation, leading to\na 4.5 times higher success rate than a generation without engaging in extended\nconversation.\n","authors":["Rimvydas Rubavicius","Antonio Valerio Miceli-Barone","Alex Lascarides","Subramanian Ramamoorthy"],"pdf_url":"https://arxiv.org/pdf/2410.09829v1.pdf","comment":"6 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2410.09781v1","updated":"2024-10-13T08:58:54Z","published":"2024-10-13T08:58:54Z","title":"ContextWIN: Whittle Index Based Mixture-of-Experts Neural Model For\n  Restless Bandits Via Deep RL","summary":"  This study introduces ContextWIN, a novel architecture that extends the\nNeural Whittle Index Network (NeurWIN) model to address Restless Multi-Armed\nBandit (RMAB) problems with a context-aware approach. By integrating a mixture\nof experts within a reinforcement learning framework, ContextWIN adeptly\nutilizes contextual information to inform decision-making in dynamic\nenvironments, particularly in recommendation systems. A key innovation is the\nmodel's ability to assign context-specific weights to a subset of NeurWIN\nnetworks, thus enhancing the efficiency and accuracy of the Whittle index\ncomputation for each arm. The paper presents a thorough exploration of\nContextWIN, from its conceptual foundation to its implementation and potential\napplications. We delve into the complexities of RMABs and the significance of\nincorporating context, highlighting how ContextWIN effectively harnesses these\nelements. The convergence of both the NeurWIN and ContextWIN models is\nrigorously proven, ensuring theoretical robustness. This work lays the\ngroundwork for future advancements in applying contextual information to\ncomplex decision-making scenarios, recognizing the need for comprehensive\ndataset exploration and environment development for full potential realization.\n","authors":["Zhanqiu Guo","Wayne Wang"],"pdf_url":"https://arxiv.org/pdf/2410.09781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09761v1","updated":"2024-10-13T07:38:44Z","published":"2024-10-13T07:38:44Z","title":"ChartKG: A Knowledge-Graph-Based Representation for Chart Images","summary":"  Chart images, such as bar charts, pie charts, and line charts, are\nexplosively produced due to the wide usage of data visualizations. Accordingly,\nknowledge mining from chart images is becoming increasingly important, which\ncan benefit downstream tasks like chart retrieval and knowledge graph\ncompletion. However, existing methods for chart knowledge mining mainly focus\non converting chart images into raw data and often ignore their visual\nencodings and semantic meanings, which can result in information loss for many\ndownstream tasks. In this paper, we propose ChartKG, a novel knowledge graph\n(KG) based representation for chart images, which can model the visual elements\nin a chart image and semantic relations among them including visual encodings\nand visual insights in a unified manner. Further, we develop a general\nframework to convert chart images to the proposed KG-based representation. It\nintegrates a series of image processing techniques to identify visual elements\nand relations, e.g., CNNs to classify charts, yolov5 and optical character\nrecognition to parse charts, and rule-based methods to construct graphs. We\npresent four cases to illustrate how our knowledge-graph-based representation\ncan model the detailed visual elements and semantic relations in charts, and\nfurther demonstrate how our approach can benefit downstream applications such\nas semantic-aware chart retrieval and chart question answering. We also conduct\nquantitative evaluations to assess the two fundamental building blocks of our\nchart-to-KG framework, i.e., object recognition and optical character\nrecognition. The results provide support for the usefulness and effectiveness\nof ChartKG.\n","authors":["Zhiguang Zhou","Haoxuan Wang","Zhengqing Zhao","Fengling Zheng","Yongheng Wang","Wei Chen","Yong Wang"],"pdf_url":"https://arxiv.org/pdf/2410.09761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11890v1","updated":"2024-10-13T07:20:47Z","published":"2024-10-13T07:20:47Z","title":"Online Digital Investigative Journalism using SociaLens","summary":"  Media companies witnessed a significant transformation with the rise of the\ninternet, bigdata, machine learning (ML) and AI. Recent emergence of large\nlanguage models (LLM) have added another aspect to this transformation.\nResearchers believe that with the help of these technologies, investigative\ndigital journalism will enter a new era. Using a smart set of data gathering\nand analysis tools, journalists will be able to create data driven contents and\ninsights in unprecedented ways. In this paper, we introduce a versatile and\nautonomous investigative journalism tool, called {\\em SociaLens}, for\nidentifying and extracting query specific data from online sources, responding\nto probing queries and drawing conclusions entailed by large volumes of data\nusing ML analytics fully autonomously. We envision its use in investigative\njournalism, law enforcement and social policy planning. The proposed system\ncapitalizes on the integration of ML technology with LLMs and advanced bigdata\nsearch techniques. We illustrate the functionality of SociaLens using a focused\ncase study on rape incidents in a developing country and demonstrate that\njournalists can gain nuanced insights without requiring coding expertise they\nmight lack. SociaLens is designed as a ChatBot that is capable of contextual\nconversation, find and collect data relevant to queries, initiate ML tasks to\nrespond to queries, generate textual and visual reports, all fully autonomously\nwithin the ChatBot environment.\n","authors":["Hasan M. Jamil","Sajratul Y. Rubaiat"],"pdf_url":"https://arxiv.org/pdf/2410.11890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04408v2","updated":"2024-10-13T07:05:51Z","published":"2024-01-09T08:04:11Z","title":"Fine-Grained Embedding Dimension Optimization During Training for\n  Recommender Systems","summary":"  Huge embedding tables in modern deep learning recommender models (DLRM)\nrequire prohibitively large memory during training and inference. This paper\nproposes FIITED, a system to automatically reduce the memory footprint via\nFIne-grained In-Training Embedding Dimension pruning. By leveraging the key\ninsight that embedding vectors are not equally important, FIITED adaptively\nadjusts the dimension of each individual embedding vector during model\ntraining, assigning larger dimensions to more important embeddings while\nadapting to dynamic changes in data. We prioritize embedding dimensions with\nhigher frequencies and gradients as more important. To enable efficient pruning\nof embeddings and their dimensions during model training, we propose an\nembedding storage system based on virtually-hashed physically-indexed hash\ntables. Experiments on two industry models and months of realistic datasets\nshow that FIITED can reduce DLRM embedding size by more than 65% while\npreserving model quality, outperforming state-of-the-art in-training embedding\npruning methods. On public datasets, FIITED can reduce the size of embedding\ntables by 2.1x to 800x with negligible accuracy drop, while improving model\nthroughput.\n","authors":["Qinyi Luo","Penghan Wang","Wei Zhang","Fan Lai","Jiachen Mao","Xiaohan Wei","Jun Song","Wei-Yu Tsai","Shuai Yang","Yuxi Hu","Xuehai Qian"],"pdf_url":"https://arxiv.org/pdf/2401.04408v2.pdf","comment":"12 pages, 15 figures"},{"id":"http://arxiv.org/abs/2310.08891v2","updated":"2024-10-13T04:49:45Z","published":"2023-10-13T06:53:02Z","title":"EHI: End-to-end Learning of Hierarchical Index for Efficient Dense\n  Retrieval","summary":"  Dense embedding-based retrieval is widely used for semantic search and\nranking. However, conventional two-stage approaches, involving contrastive\nembedding learning followed by approximate nearest neighbor search (ANNS), can\nsuffer from misalignment between these stages. This mismatch degrades retrieval\nperformance. We propose End-to-end Hierarchical Indexing (EHI), a novel method\nthat directly addresses this issue by jointly optimizing embedding generation\nand ANNS structure. EHI leverages a dual encoder for embedding queries and\ndocuments while simultaneously learning an inverted file index (IVF)-style tree\nstructure. To facilitate the effective learning of this discrete structure, EHI\nintroduces dense path embeddings that encodes the path traversed by queries and\ndocuments within the tree. Extensive evaluations on standard benchmarks,\nincluding MS MARCO (Dev set) and TREC DL19, demonstrate EHI's superiority over\ntraditional ANNS index. Under the same computational constraints, EHI\noutperforms existing state-of-the-art methods by +1.45% in MRR@10 on MS MARCO\n(Dev) and +8.2% in nDCG@10 on TREC DL19, highlighting the benefits of our\nend-to-end approach.\n","authors":["Ramnath Kumar","Anshul Mittal","Nilesh Gupta","Aditya Kusupati","Inderjit Dhillon","Prateek Jain"],"pdf_url":"https://arxiv.org/pdf/2310.08891v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09713v1","updated":"2024-10-13T03:45:24Z","published":"2024-10-13T03:45:24Z","title":"Agentic Information Retrieval","summary":"  What will information entry look like in the next generation of digital\nproducts? Since the 1970s, user access to relevant information has relied on\ndomain-specific architectures of information retrieval (IR). Over the past two\ndecades, the advent of modern IR systems, including web search engines and\npersonalized recommender systems, has greatly improved the efficiency of\nretrieving relevant information from vast data corpora. However, the core\nparadigm of these IR systems remains largely unchanged, relying on filtering a\npredefined set of candidate items. Since 2022, breakthroughs in large language\nmodels (LLMs) have begun transforming how information is accessed, establishing\na new technical paradigm. In this position paper, we introduce Agentic\nInformation Retrieval (Agentic IR), a novel IR paradigm shaped by the\ncapabilities of LLM agents. Agentic IR expands the scope of accessible tasks\nand leverages a suite of new techniques to redefine information retrieval. We\ndiscuss three types of cutting-edge applications of agentic IR and the\nchallenges faced. We propose that agentic IR holds promise for generating\ninnovative applications, potentially becoming a central information entry point\nin future digital ecosystems.\n","authors":["Weinan Zhang","Junwei Liao","Ning Li","Kounianhua Du"],"pdf_url":"https://arxiv.org/pdf/2410.09713v1.pdf","comment":"11 pages, position paper"}],"Multimedia":[{"id":"http://arxiv.org/abs/2406.04321v2","updated":"2024-10-13T17:59:22Z","published":"2024-06-06T17:58:11Z","title":"VidMuse: A Simple Video-to-Music Generation Framework with\n  Long-Short-Term Modeling","summary":"  In this work, we systematically study music generation conditioned solely on\nthe video. First, we present a large-scale dataset comprising 360K video-music\npairs, including various genres such as movie trailers, advertisements, and\ndocumentaries. Furthermore, we propose VidMuse, a simple framework for\ngenerating music aligned with video inputs. VidMuse stands out by producing\nhigh-fidelity music that is both acoustically and semantically aligned with the\nvideo. By incorporating local and global visual cues, VidMuse enables the\ncreation of musically coherent audio tracks that consistently match the video\ncontent through Long-Short-Term modeling. Through extensive experiments,\nVidMuse outperforms existing models in terms of audio quality, diversity, and\naudio-visual alignment. The code and datasets will be available at\nhttps://github.com/ZeyueT/VidMuse/.\n","authors":["Zeyue Tian","Zhaoyang Liu","Ruibin Yuan","Jiahao Pan","Qifeng Liu","Xu Tan","Qifeng Chen","Wei Xue","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2406.04321v2.pdf","comment":"The code and datasets will be available at\n  https://github.com/ZeyueT/VidMuse/"},{"id":"http://arxiv.org/abs/2410.09872v1","updated":"2024-10-13T15:13:00Z","published":"2024-10-13T15:13:00Z","title":"Towards Reproducible Learning-based Compression","summary":"  A deep learning system typically suffers from a lack of reproducibility that\nis partially rooted in hardware or software implementation details. The\nirreproducibility leads to skepticism in deep learning technologies and it can\nhinder them from being deployed in many applications. In this work, the\nirreproducibility issue is analyzed where deep learning is employed in\ncompression systems while the encoding and decoding may be run on devices from\ndifferent manufacturers. The decoding process can even crash due to a single\nbit difference, e.g., in a learning-based entropy coder. For a given deep\nlearning-based module with limited resources for protection, we first suggest\nthat reproducibility can only be assured when the mismatches are bounded. Then\na safeguarding mechanism is proposed to tackle the challenges. The proposed\nmethod may be applied for different levels of protection either at the\nreconstruction level or at a selected decoding level. Furthermore, the overhead\nintroduced for the protection can be scaled down accordingly when the error\nbound is being suppressed. Experiments demonstrate the effectiveness of the\nproposed approach for learning-based compression systems, e.g., in image\ncompression and point cloud compression.\n","authors":["Jiahao Pang","Muhammad Asad Lodhi","Junghyun Ahn","Yuning Huang","Dong Tian"],"pdf_url":"https://arxiv.org/pdf/2410.09872v1.pdf","comment":"Accepted at MMSP 2024"},{"id":"http://arxiv.org/abs/2405.13984v2","updated":"2024-10-13T14:24:49Z","published":"2024-05-22T20:40:53Z","title":"Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption\n  Generation and Fine-Grained NLI Evaluation","summary":"  Scientific language models drive research innovation but require extensive\nfine-tuning on large datasets. This work enhances such models by improving\ntheir inference and evaluation capabilities with minimal or no additional\ntraining. Focusing on molecule caption generation, we explore synergies between\nalignment fine-tuning and model merging in a cross-modal setup. We reveal\nintriguing insights into the behaviour and suitability of such methods while\nsignificantly surpassing state-of-the-art models. Moreover, we propose a novel\natomic-level evaluation method leveraging off-the-shelf Natural Language\nInference (NLI) models for use in the unseen chemical domain. Our experiments\ndemonstrate that our evaluation operates at the right level of granularity,\neffectively handling multiple content units and subsentence reasoning, while\nwidely adopted NLI methods consistently misalign with assessment criteria.\n","authors":["Dimitris Gkoumas","Maria Liakata"],"pdf_url":"https://arxiv.org/pdf/2405.13984v2.pdf","comment":null}]}}